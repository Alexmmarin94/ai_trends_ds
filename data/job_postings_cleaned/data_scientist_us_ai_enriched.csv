job_id,ai_mentions,ai_details,data_mentions,data_details
nFB7ISC3OFcsdxK7AAAAAA==,[],,"['Supervised Learning', 'Reinforcement Learning', 'Python', 'XGBoost', 'scikit-learn', 'statsmodels', 'Relational Databases', 'Machine Learning Model Lifecycle', 'AWS', 'Kubernetes', 'H2O', 'Spark']","Supervised Learning: Used to build predictive models that forecast customer needs and recommend optimal financial solutions.; Reinforcement Learning: Applied to develop models that optimize recommendations for customers facing financial hardship.; Python: Primary programming language used to write custom libraries and develop data science solutions.; XGBoost: Open-source machine learning library utilized for building predictive models in production and analytical environments.; scikit-learn: Open-source ML library used for developing machine learning models as part of the data science workflow.; statsmodels: Library used for statistical modeling and analysis within the data science team.; Relational Databases: Used for managing and querying large volumes of structured data to support analytics and modeling.; Machine Learning Model Lifecycle: Involves all phases from design, training, evaluation, validation, to implementation of models.; AWS: Cloud computing platform leveraged to handle large-scale data processing and model deployment.; Kubernetes: Used to orchestrate containerized applications and support scalable deployment of data science solutions.; H2O: Technology used as part of the machine learning stack to build scalable models.; Spark: Big data processing framework employed to analyze large volumes of numeric and textual data."
XdrEhkRRuZAndmnAAAAAAA==,[],,"['Python', 'SQL', 'ESRI', 'Alteryx', 'Tableau', 'Scripting and ETL languages', 'Machine learning models', 'Mathematical concepts', 'Data architecture and warehousing', 'Big Data and streaming services', 'Hadoop ecosystem', 'R programming', 'SAS and SPSS', 'Text analysis and text mining']","Python: Used as a primary programming language for data manipulation, analysis, and scripting in the role.; SQL: Utilized for querying and managing data within databases, essential for data extraction and manipulation.; ESRI: Applied as a geographic information system tool for spatial data analysis and visualization.; Alteryx: Used for data preparation, blending, and advanced analytics workflows, including ETL processes.; Tableau: Employed for creating data visualizations and dashboards to communicate insights effectively.; Scripting and ETL languages: Advanced skills required to write scripts and develop ETL pipelines for data processing and transformation.; Machine learning models: Experience with major families of models such as regression, decision trees, clustering, SVMs, and neural networks for predictive analytics and model scoring.; Mathematical concepts: Command of advanced mathematics including calculus, partial differential equations, probability, and statistics to support modeling and analysis.; Data architecture and warehousing: Basic understanding of data architecture, data warehouses, and data marts to support data storage and retrieval.; Big Data and streaming services: Experience working with large datasets and tools to obtain, transform, and store data in big data and streaming environments.; Hadoop ecosystem: Familiarity with Hadoop and related tools such as Hive and Spark for distributed data processing and analytics.; R programming: Experience with R and its open-source ecosystem (CRAN) for statistical analysis and data science tasks.; SAS and SPSS: Knowledge of statistical software packages used for data analysis and modeling.; Text analysis and text mining: Applied techniques for extracting insights from unstructured text data."
vikyPvnmOOVrMK3UAAAAAA==,"['Large Language Models', 'Deep Learning Frameworks', 'Retrieval-Augmented Generation']","Large Language Models: Fine-tuning and applying large language models (LLMs) such as GPT, LLaMA, or Claude for natural language processing and generative AI tasks.; Deep Learning Frameworks: Using deep learning frameworks like PyTorch and TensorFlow specifically for neural network model development and fine-tuning of AI models.; Retrieval-Augmented Generation: Utilizing retrieval-augmented generation (RAG) pipelines and vector databases to enhance generative AI capabilities by combining retrieval of relevant information with language model generation.","['Machine Learning', 'Feature Engineering', 'Data Analytics', 'ML Frameworks', 'Python Programming', 'ML Pipelines', 'Cloud Integration']","Machine Learning: Developing and deploying machine learning models in enterprise or government environments to extract insights and solve complex problems from structured and unstructured data.; Feature Engineering: Applying feature engineering techniques to prepare and transform data for building effective machine learning models and end-to-end ML pipelines.; Data Analytics: Performing data analytics to understand data-rich environments and extract meaningful information to support client decision-making.; ML Frameworks: Using machine learning frameworks such as Scikit-learn for traditional ML model development and deployment.; Python Programming: Programming in Python to develop, fine-tune, and deploy machine learning and data analytics solutions.; ML Pipelines: Building end-to-end machine learning pipelines to automate data processing, model training, and deployment workflows.; Cloud Integration: Integrating AI and machine learning models into secure or cloud-hosted environments such as AWS or Azure to support scalable and compliant deployments."
KOEcsjTne8KUqX0sAAAAAA==,[],,"['Data Modeling and Visualization', 'ETL (Extract, Transform, Load)', 'Python', 'SQL', 'NiFi', 'AWS or Azure Cloud', 'CI/CD Pipelines', 'Data Quality and Integrity Assessment', 'Git/GitHub']","Data Modeling and Visualization: This role involves creating models and visual representations to extract insights from large, complex data sets.; ETL (Extract, Transform, Load): The job requires experience in ETL processes to manage and prepare data for analysis.; Python: Python is used as a primary programming language for data manipulation and analysis tasks.; SQL: SQL is utilized for managing and querying various types of databases.; NiFi: NiFi is employed for data flow automation and management within data pipelines.; AWS or Azure Cloud: Experience with cloud platforms like AWS or Azure is necessary for deploying and managing data solutions.; CI/CD Pipelines: Building and maintaining continuous integration and continuous deployment pipelines is part of the role to support data workflows and software development.; Data Quality and Integrity Assessment: The role includes assessing and ensuring the quality and integrity of data used for analysis.; Git/GitHub: Version control tools like Git and GitHub are used for managing code and collaboration."
0vkoqsWaaHsrLgRiAAAAAA==,[],,"['Supervised Learning', 'Reinforcement Learning', 'Python', 'XGBoost', 'Scikit-learn', 'Statsmodels', 'Relational Databases', 'Machine Learning', 'Python Libraries', 'Kubernetes', 'AWS', 'H2O', 'Spark']","Supervised Learning: Used to build models that predict customer needs and recommend optimal solutions in the financial domain.; Reinforcement Learning: Applied to develop models that help predict customer needs and recommend personalized financial solutions.; Python: Used for writing custom libraries and developing data science solutions within both analytical and production environments.; XGBoost: An open-source machine learning library utilized for building predictive models impacting millions of customers daily.; Scikit-learn: An open-source machine learning library used to develop and implement models for customer treatment and financial decision-making.; Statsmodels: Used as part of the open-source ML libraries to support statistical modeling in data science solutions.; Relational Databases: Utilized for managing and querying large-scale structured data essential for analytics and model development.; Machine Learning: Employed throughout all phases of model development including design, training, evaluation, validation, and implementation to drive business value.; Python Libraries: Custom libraries developed to support machine learning and data analysis workflows in production and analytical settings.; Kubernetes: Part of the technology stack leveraged to manage scalable computing environments for data science workloads.; AWS: Cloud computing platform used to support large-scale data analysis and machine learning model deployment.; H2O: Technology used within the data science stack to build and deploy machine learning models at scale.; Spark: Utilized to process and analyze huge volumes of numeric and textual data efficiently."
g2BI_dwhDGqBZXgcAAAAAA==,[],,"['Statistical Modeling', 'Machine Learning', 'Clustering', 'Classification', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'Data Retrieval and Integration', 'Data Monitoring and Alerting', 'Python', 'Conda', 'AWS', 'H2O', 'Spark', 'PySpark', 'SQL', 'Model Validation and Backtesting', 'Confusion Matrix and ROC Curve Interpretation']","Statistical Modeling: Used to personalize credit card offers and generate insights from credit bureau data to support underwriting decisions.; Machine Learning: Building models through all phases including design, training, evaluation, validation, and implementation to extract value from large datasets.; Clustering: Applied as part of data science techniques to analyze and segment data for better insights.; Classification: Used to categorize data and support predictive modeling in underwriting and other business decisions.; Sentiment Analysis: Employed to analyze textual data as part of extracting insights from large volumes of numeric and textual data.; Time Series Analysis: Used to analyze data over time, supporting modeling and forecasting tasks relevant to the business.; Deep Learning: Experience with deep learning methods is valued for advanced modeling tasks within the data science team.; Data Retrieval and Integration: Skills to retrieve, combine, and analyze data from various sources and structures to enable effective data science solutions.; Data Monitoring and Alerting: Owning monitoring solutions to promptly alert users to potential production errors in data features.; Python: Used as a primary programming language for data science and machine learning model development.; Conda: Utilized as an environment and package management system to support data science workflows.; AWS: Cloud computing platform leveraged to handle large-scale data processing and model deployment.; H2O: Machine learning platform used to build and deploy models efficiently on large datasets.; Spark: Big data processing framework used to handle and analyze huge volumes of numeric and textual data.; PySpark: Python API for Spark used to process large-scale data and build machine learning models.; SQL: Used for querying and managing relational databases to support data analytics and model building.; Model Validation and Backtesting: Practices to ensure model accuracy and reliability by validating and backtesting predictive models.; Confusion Matrix and ROC Curve Interpretation: Techniques used to evaluate classification model performance and interpret results."
18oCPxMqXOGheocuAAAAAA==,"['Deep Learning Frameworks', 'Artificial Intelligence and Machine Learning Research']",Deep Learning Frameworks: Use of TensorFlow and PyTorch for developing AI/ML capabilities applicable to defense and intelligence missions.; Artificial Intelligence and Machine Learning Research: Participate in research and development of AI/ML capabilities to support defense and intelligence objectives.,"['Data Modeling', 'Machine Learning Pipelines', 'Data Analysis', 'Data Visualization and Dashboards', 'Data Quality Assessment', 'Statistical Modeling', 'Predictive Analytics', 'Data Mining', 'Programming Languages', 'Databases and Data Processing Frameworks', 'Machine Learning Libraries', 'Cloud Environments']","Data Modeling: Design, develop, and implement data models to solve complex defense-related problems.; Machine Learning Pipelines: Design, develop, and implement machine learning pipelines to address complex defense-related challenges.; Data Analysis: Analyze large, structured and unstructured datasets from multiple sources, including sensor data, signals intelligence, satellite imagery, and operational reports.; Data Visualization and Dashboards: Build dashboards, data visualizations, and interactive tools to communicate analytical findings to non-technical audiences and senior leadership.; Data Quality Assessment: Conduct data quality assessments and recommend enhancements to existing data systems and processes.; Statistical Modeling: Apply strong understanding of statistical modeling techniques in data science projects.; Predictive Analytics: Utilize predictive analytics methods to support defense and intelligence missions.; Data Mining: Employ data mining techniques to extract insights from complex datasets.; Programming Languages: Use Python, R, or similar programming languages for data science tasks.; Databases and Data Processing Frameworks: Work with SQL and NoSQL databases, and data processing frameworks such as Spark and Hadoop.; Machine Learning Libraries: Familiarity with machine learning libraries including Scikit-learn, TensorFlow, and PyTorch.; Cloud Environments: Experience with DoD data platforms and cloud environments like AWS GovCloud and Azure Government."
Gbav3WbCOvLIce_kAAAAAA==,[],,"['Operational Research & Statistical Analysis', 'Data Visualization Tools', 'Logistics Data Analysis']",Operational Research & Statistical Analysis: Used to analyze and interpret complex logistics data to support U.S. Marine Corps logistics systems and optimize supply chain and logistics operations.; Data Visualization Tools: Developed to create effective metrics and visual representations of logistics data to support equipment readiness and decision-making.; Logistics Data Analysis: Focuses on analyzing and interpreting complex logistics data to identify strategies for optimizing supply chain and logistics operations.
j0D1iMNi9cjZUJ6cAAAAAA==,['Artificial Intelligence Tools Automation'],"Artificial Intelligence Tools Automation: Builds AI tools that automate certain intelligence analytic processes, enhancing efficiency and decision-making.","['Data Analytics', 'Data Engineering', 'Data Mining', 'Exploratory Data Analysis', 'Predictive Analysis', 'Statistical Analysis', 'Machine Learning', 'Data Visualization', 'Programming Languages (R, Python, SQL)', 'Multi-INT Analytics', 'Distributed Analytics and Scalable Algorithms', 'Algorithm Development', 'Data Science Tradecraft Compliance', 'Technical Documentation and Knowledge Modeling', 'Testing, Evaluation, Validation, and Verification (TEVV)', 'Performance Metrics and Data-Driven Decision Making', 'Version Control and Code Management']","Data Analytics: Conducts data analytics to extract insights and support informed analytic decisions in intelligence analysis contexts.; Data Engineering: Performs data engineering tasks including managing and merging disparate data sources using R, Python, or SQL to support large-scale intelligence data workflows.; Data Mining: Applies data mining techniques to discover patterns and build high-quality predictive systems for intelligence analysis.; Exploratory Data Analysis: Conducts exploratory analysis to better understand data sets and inform subsequent modeling and decision-making processes.; Predictive Analysis: Builds automated predictive analytics and ML-based tools such as recommendation engines and lead scoring systems to enhance intelligence assessments.; Statistical Analysis: Performs statistical analysis and develops statistical models to evaluate system performance, robustness, and fairness in all-source analysis contexts.; Machine Learning: Creates and applies machine learning models and automated predictive analytics to support multi-INT analytics and intelligence tradecraft.; Data Visualization: Utilizes data visualization tools including Microsoft Power BI and Tableau to create graphical and visual narrative products for analytic decisions.; Programming Languages (R, Python, SQL): Writes scripts in R and Python and uses SQL for data science workflows, managing data sources, and implementing data mining algorithms.; Multi-INT Analytics: Leverages multi-intelligence (multi-INT) data sources and spatial data to perform complex intelligence analysis and predictive modeling.; Distributed Analytics and Scalable Algorithms: Develops and implements distributed algorithms to scale analytics on large and complex datasets exceeding RAM capacity.; Algorithm Development: Designs, develops, and evaluates advanced algorithmic intelligence concepts and analytic techniques for all-source intelligence analysis.; Data Science Tradecraft Compliance: Ensures compliance with analytic standards and tradecraft in the application of data science techniques within intelligence workflows.; Technical Documentation and Knowledge Modeling: Reviews, translates, and refines technical documentation into tradecraft-compliant language and provides guidance on governance and knowledge modeling.; Testing, Evaluation, Validation, and Verification (TEVV): Designs and executes TEVV protocols to assess system performance, robustness, fairness, and compliance in intelligence analysis systems.; Performance Metrics and Data-Driven Decision Making: Develops and analyzes performance metrics to identify trends and inform corrective actions and improvements in analytic systems.; Version Control and Code Management: Contributes to capability development by editing and storing code in government-owned or controlled source version control repositories."
e3bHDqC3T7JHo0tUAAAAAA==,"['Large Language Models (LLMs)', 'Retrieval-Augmented Generation (RAG)', 'Computer Vision']","Large Language Models (LLMs): Developed, deployed, and optimized within pipelines to support AI business solutions involving natural language understanding and generation.; Retrieval-Augmented Generation (RAG): Implemented and maintained pipelines that combine retrieval of relevant information with generative AI models to enhance performance and accuracy in AI applications.; Computer Vision: Applied as a potential AI application area involving image and video data analysis using modern AI techniques.","['Time Series Forecasting', 'Regression Models', 'Classification Models', 'Root Cause Analysis (RCA)', 'Simulation and Optimization', 'Machine Learning Models', 'Applied Analytics', 'Predictive Analytics', 'Prescriptive Analytics', 'Retrieval-Augmented Generation (RAG) Pipelines', 'Semantic and Ontology Technologies', 'Exploratory and Targeted Data Analysis', 'Data Quality Assessment and Cleansing', 'Model Productionization', 'Cloud Platforms and Machine Learning Services', 'Python Programming', 'CI/CD Pipelines']","Time Series Forecasting: Used as a potential application area for predictive analytics to analyze and forecast data trends over time in business problems.; Regression Models: Applied in machine learning regression tasks to develop predictive models addressing customer needs and opportunities.; Classification Models: Used in machine learning classification tasks to categorize data and support business solutions.; Root Cause Analysis (RCA): Employed to identify underlying causes of issues within data and business processes to improve solutions.; Simulation and Optimization: Applied to model complex systems and optimize outcomes as part of advanced analytics and prescriptive analytics.; Machine Learning Models: Designed, developed, and deployed in production environments to solve business problems and drive AI business solutions.; Applied Analytics: Involved in the development, deployment, and application of analytics techniques to address real-world business challenges.; Predictive Analytics: Used to forecast future outcomes based on historical data to support decision-making.; Prescriptive Analytics: Applied to recommend actions based on data insights and analytic prototyping to optimize business processes.; Retrieval-Augmented Generation (RAG) Pipelines: Developed and maintained to enhance data retrieval and generation capabilities in conjunction with large language models.; Semantic and Ontology Technologies: Utilized to enrich data semantically, improving data integration, retrieval, and supporting advanced analytics and machine learning models.; Exploratory and Targeted Data Analysis: Performed using descriptive statistics and other methods to understand data characteristics and inform model development.; Data Quality Assessment and Cleansing: Collaborated with data engineers to ensure data integrity and prepare data for analytics and model productionization.; Model Productionization: Involved in deploying machine learning models into production environments for operational use.; Cloud Platforms and Machine Learning Services: Interacted with and deployed models within cloud environments such as AWS, Azure, Google Cloud, and Databricks to leverage scalable infrastructure and services.; Python Programming: Used as the primary programming language for developing machine learning models, analytics, and data processing pipelines.; CI/CD Pipelines: Deployed and managed continuous integration and continuous deployment pipelines to automate model and analytics delivery."
Ovm-G7mZRL8v1phBAAAAAA==,[],,"['Data Strategy Development', 'Data Governance', 'Cross-Functional Collaboration', 'Advanced Analytics Tools', 'Data Science and Machine Learning', 'Communication of Insights']","Data Strategy Development: Develop and implement data strategies that align with business objectives to ensure effective utilization of data assets across the organization.; Data Governance: Oversee data collection, storage, and maintenance to ensure data integrity and security, and enforce data governance policies.; Cross-Functional Collaboration: Collaborate with engineering, product development, and business stakeholders to understand their needs and deliver data-driven solutions.; Advanced Analytics Tools: Utilize advanced analytics tools and technologies to enhance data processing and analysis capabilities, ensuring scalability and efficiency of data solutions.; Data Science and Machine Learning: Stay current with advancements in data science and machine learning to drive innovation and continuous improvement within the team.; Communication of Insights: Communicate findings and recommendations to senior management and other stakeholders by translating complex technical concepts into actionable business insights."
xR4raTaIOb_Gja9mAAAAAA==,"['Artificial Intelligence', 'Natural Language Processing with AI', 'Deep Learning for AI', 'Computer Vision for AI', 'Reinforcement Learning for AI', 'Machine Learning Frameworks for AI']","Artificial Intelligence: Understands AI concepts, algorithms, and platforms to design architectures that support intelligent systems and enable AI-driven applications.; Natural Language Processing with AI: Utilizes NLP techniques specifically in the context of AI to analyze and extract insights from unstructured text data.; Deep Learning for AI: Applies deep learning neural network methods as part of AI-driven model development and deployment.; Computer Vision for AI: Employs computer vision techniques within AI frameworks to create innovative solutions.; Reinforcement Learning for AI: Uses reinforcement learning methods in AI to develop autonomous and adaptive models and solutions.; Machine Learning Frameworks for AI: Uses AI-relevant ML frameworks such as TensorFlow and PyTorch to develop and deploy neural network-based AI models.","['Predictive Modeling', 'Business Intelligence Tools', 'Data Pipelines', 'Exploratory Data Analysis', 'Machine Learning Algorithms', 'Deep Learning', 'Natural Language Processing', 'Computer Vision', 'Reinforcement Learning', 'Python', 'ML Frameworks', 'Statistical Analysis', 'Data Mining', 'Web Mining', 'Data Monetization', 'Data Ethics', 'Reporting and Dashboarding']","Predictive Modeling: Develops predictive models and integrates them with Business Intelligence tools to support business goals and decision-making.; Business Intelligence Tools: Uses BI tools to integrate predictive models and provide actionable insights for strategic decisions.; Data Pipelines: Develops and maintains data pipelines for efficient data retrieval, processing, and ensuring data quality and consistency.; Exploratory Data Analysis: Performs exploratory data analysis to identify patterns, insights, and communicate findings to stakeholders.; Machine Learning Algorithms: Utilizes advanced machine learning techniques including supervised and unsupervised learning to solve complex business challenges.; Deep Learning: Applies deep learning methods as part of advanced machine learning techniques to create innovative solutions.; Natural Language Processing: Proficient in analyzing and extracting insights from unstructured text data, including sentiment analysis, topic modeling, and language understanding.; Computer Vision: Uses computer vision techniques as part of advanced machine learning to develop innovative AI-driven solutions.; Reinforcement Learning: Employs reinforcement learning methods to develop innovative machine learning models and solutions.; Python: Expertise in Python programming language for developing machine learning models and data science solutions.; ML Frameworks: Proficiency in machine learning frameworks such as TensorFlow, PyTorch, and scikit-learn for model development and deployment.; Statistical Analysis: Applies statistical analysis principles including data collection, sampling, probability distributions, hypothesis testing, and statistical process control to support decision-making.; Data Mining: Uses data mining techniques to identify patterns and relationships in data, including association, sequence/path analysis, classification, clustering, and forecasting.; Web Mining: Applies web mining techniques to analyze large volumes of web data for customer relationship management and user behavior pattern identification.; Data Monetization: Familiarity with strategies and techniques for data commercialization, data marketplaces, and realizing data value.; Data Ethics: Drives ethical considerations in model deployment and data utilization, including strategies to mitigate biases in data-driven decision-making.; Reporting and Dashboarding: Prepares reports and dashboards by accessing information from databases and other sources to meet business requirements."
hiPwBaWcFXanqlFtAAAAAA==,"['Generative AI Large Language Models', 'Deep Learning Frameworks']",Generative AI Large Language Models: Crafting prompts for generative AI large language models (LLMs) to leverage advanced AI capabilities in solving real-world challenges and enhancing intelligence analysis.; Deep Learning Frameworks: Using deep learning frameworks such as TensorFlow and PyTorch specifically for neural network development and deployment in AI model building.,"['Machine Learning', 'Python', 'Data Visualization', 'Streaming and Batch Data Pipelines', 'Machine Learning Frameworks', 'Cloud Platforms and Containerization', 'Streaming Data Technologies and Distributed Systems', 'Open-Source Databases', 'Agile Methodologies and DevOps Practices']","Machine Learning: Designing and operationalizing machine learning models and predictive analytics to transform data into actionable insights and support mission-critical intelligence work.; Python: Using Python for data wrangling, modeling, and visualization, including libraries such as Pandas, NumPy, Matplotlib, and Seaborn to process and analyze data.; Data Visualization: Building interactive visual interfaces that translate the results of advanced models into meaningful decisions, supporting intelligence analysis.; Streaming and Batch Data Pipelines: Developing and utilizing both streaming and batch data pipelines to handle large volumes of structured and unstructured data for analytics and model deployment.; Machine Learning Frameworks: Employing frameworks such as TensorFlow, PyTorch, and scikit-learn to develop and deploy machine learning models.; Cloud Platforms and Containerization: Using cloud platforms like AWS, Azure, or GCP for model hosting and leveraging containerization technologies such as Docker and Kubernetes to deploy production-grade models and analytics tools.; Streaming Data Technologies and Distributed Systems: Experience with Apache Kafka, Flink, Hadoop, and Spark to manage and process streaming data and distributed computing environments.; Open-Source Databases: Working with databases such as MySQL, PostgreSQL, or SQLite to store and query data for analytics and model support.; Agile Methodologies and DevOps Practices: Applying Agile methodologies, automated testing, and DevOps practices to ensure rapid prototyping, deployment, and operationalization of analytics solutions."
jWxFAP0iymu5r3AzAAAAAA==,"['Large Language Models', 'Generative AI', 'PyTorch', 'Hugging Face', 'LangChain', 'Lightning', 'Vector Databases', 'Training Optimization', 'Self-Supervised Learning', 'Explainability', 'Reinforcement Learning from Human Feedback']","Large Language Models: Harness the power of Large Language Models (LLMs) by adapting and finetuning them for customer-facing applications and features, enabling advanced AI-powered products.; Generative AI: Experiment, innovate, and create next generation experiences powered by the latest emerging generative AI technologies to deliver dynamic and personalized customer experiences.; PyTorch: Use PyTorch as a deep learning framework to build, train, and deploy neural network models, including language models.; Hugging Face: Utilize Hugging Face tools and libraries to access, fine-tune, and deploy transformer-based models and other state-of-the-art AI models.; LangChain: Leverage LangChain framework to build applications powered by language models, enabling integration of LLMs with external data and tools.; Lightning: Use Lightning (PyTorch Lightning) to simplify and accelerate deep learning model development and training workflows.; Vector Databases: Employ vector databases to efficiently store and retrieve high-dimensional embeddings generated by AI models for search and recommendation applications.; Training Optimization: Apply techniques to optimize the training process of large language and computer vision models to improve efficiency and performance.; Self-Supervised Learning: Use self-supervised learning methods to train models on unlabeled data, enhancing model capabilities without requiring extensive labeled datasets.; Explainability: Incorporate explainability techniques to interpret and understand AI model decisions, improving transparency and trust.; Reinforcement Learning from Human Feedback: Implement reinforcement learning from human feedback (RLHF) to fine-tune language models based on human preferences and improve alignment with user needs.","['Machine Learning', 'Natural Language Processing', 'Data Analytics', 'SQL', 'Python', 'Scala', 'R', 'AWS Cloud Computing']","Machine Learning: Build machine learning models through all phases of development, from design through training, evaluation, and validation; partnering with engineering teams to operationalize them in scalable and resilient production systems that serve 80+ million customers.; Natural Language Processing: Be the expert in Natural Language Processing (NLP) to harness the power of Large Language Models, adapt and finetune them for customer facing applications and features.; Data Analytics: Perform data analytics using quantitative skills in statistics, economics, operations research, analytics, mathematics, or computer science, supported by at least 2 years of experience.; SQL: Use SQL for data querying and manipulation as part of the data science and analytics workflow.; Python: Use Python programming language for data science and machine learning tasks, with at least 2 years of experience preferred.; Scala: Use Scala programming language for data science and machine learning tasks, with at least 2 years of experience preferred.; R: Use R programming language for data science and machine learning tasks, with at least 2 years of experience preferred.; AWS Cloud Computing: Leverage AWS cloud computing platforms, including AWS Ultraclusters, to handle large-scale data processing and model training workloads."
M_MpaS5ifiBn0kjBAAAAAA==,"['Deep Learning', 'Natural Language Processing']",Deep Learning: Apply neural network-based advanced analytics techniques to extract insights and enhance data modeling capabilities.; Natural Language Processing: Use AI-driven methods to analyze and interpret human language data as part of advanced analytics initiatives.,"['Data Strategies and Frameworks', 'Data Governance', 'Data Collection, Storage, and Retrieval', 'Advanced Statistical Techniques', 'Data Mining', 'Machine Learning Algorithms', 'Data Analysis Tools and Languages', 'Data Visualization Tools', 'Data Warehousing and Data Lakes', 'Big Data Technologies', 'Statistical Modeling', 'Data Quality and Integrity', 'Analytical and Problem-Solving Skills', 'Data Storytelling and Presentation Skills', 'Advanced Analytics Techniques', 'Cloud-Based Data Platforms']","Data Strategies and Frameworks: Develop and implement organizational approaches to manage and utilize data effectively, ensuring alignment with business goals and data governance policies.; Data Governance: Establish comprehensive policies and practices to ensure data quality, integrity, and compliance across the organization.; Data Collection, Storage, and Retrieval: Design and implement processes to optimize how data is gathered, stored, and accessed to support efficient data management and analysis.; Advanced Statistical Techniques: Apply sophisticated statistical methods to analyze data, uncover patterns, and support decision-making.; Data Mining: Use techniques to explore large datasets to identify patterns, trends, and correlations relevant to program management and strategic initiatives.; Machine Learning Algorithms: Utilize algorithms to build predictive models and uncover insights from data, supporting data-driven decision-making.; Data Analysis Tools and Languages: Proficiency in tools and programming languages such as Python (with pandas and scikit-learn), R, and SQL to perform data manipulation, analysis, and modeling.; Data Visualization Tools: Leverage tools like Tableau and Power BI to create reports, dashboards, and visualizations that communicate complex data insights clearly to stakeholders.; Data Warehousing and Data Lakes: Knowledge of concepts and architectures for storing and managing large volumes of structured and unstructured data to support analytics.; Big Data Technologies: Experience with platforms such as Hadoop and Spark to process and analyze large-scale datasets efficiently.; Statistical Modeling: Develop and apply models to represent data relationships and support predictive analytics.; Data Quality and Integrity: Ensure accuracy, consistency, and reliability of data throughout its lifecycle to support trustworthy analysis and reporting.; Analytical and Problem-Solving Skills: Ability to extract meaningful insights from complex datasets and translate them into actionable recommendations.; Data Storytelling and Presentation Skills: Communicate data-driven insights effectively through narratives and visualizations to influence decision-making.; Advanced Analytics Techniques: Apply sophisticated methods including deep learning and natural language processing to enhance data analysis capabilities.; Cloud-Based Data Platforms: Utilize cloud services such as AWS, Azure, and GCP to support scalable data storage, processing, and analytics."
OLbn6rBegU2ASEx_AAAAAA==,['Advanced AI Technologies'],"Advanced AI Technologies: Developing and demonstrating innovative solutions to complex AI challenges, spearheading exploration of new AI technologies, and pushing boundaries in the AI field.",['Data Science Research'],Data Science Research: Leading research initiatives to explore and implement innovative data-driven solutions using massive datasets and open-ended exploration.
bU7z1i-Xp3rNeGmeAAAAAA==,"['Large Language Models', 'Natural Language Processing', 'AI Agent Architectures', 'Model Context Protocols', 'Retrieval-Augmented Generation', 'Vector Databases', 'Voice AI and Automated Care Navigation']","Large Language Models: The job involves deep expertise in LLMs including development and deployment of LLM-based agents in production healthcare systems.; Natural Language Processing: Experience with NLP models such as medical BERT and BioGPT is required, focusing on AI agent architectures and contextual understanding.; AI Agent Architectures: The role focuses on building interoperable, context-aware, and self-improving AI agents that operate across clinical, administrative, and benefits platforms.; Model Context Protocols: Hands-on development experience with Model Context Protocols (MCP) and AXA Protocols is essential for managing AI agent interactions.; Retrieval-Augmented Generation: Expertise in retrieval-augmented generation (RAG) techniques is required to enhance AI agent memory and retrieval capabilities.; Vector Databases: Deep understanding of VectorDB integration is needed for dynamic agent memory and retrieval in AI systems.; Voice AI and Automated Care Navigation: Experience with voice AI, automated care navigation, and AI triage tools is preferred to support healthcare AI applications.","['Python', 'Healthcare Data Standards', 'Cloud Platforms and DevOps Tools']","Python: The role requires coding experience in Python with proficiency in machine learning and NLP libraries.; Healthcare Data Standards: Experience with healthcare data standards such as FHIR, HLX, ICD/CPT, and X12 EDI formats is required.; Cloud Platforms and DevOps Tools: Proficiency in AWS, Azure, or Google Cloud Platform including Kubernetes, Docker, and CI/CD pipelines is necessary."
54AX23B3l6L8myBvAAAAAA==,"['Azure AI Services', 'Artificial Intelligence', 'Machine Learning on Azure']","Azure AI Services: The role requires developing and maintaining technical proficiency in Azure AI cloud services to build and deliver AI/ML solutions for customers.; Artificial Intelligence: The position focuses on building advanced AI solutions leveraging Azure cloud capabilities to help customers achieve digital transformation and solve organizational challenges.; Machine Learning on Azure: The job involves consulting on machine learning solutions using Azure cloud services, including educating customers and formulating AI/ML strategies tailored to their needs.","['Machine Learning', 'Azure Data Factory', 'SQL', 'Power BI', 'Data & AI']","Machine Learning: The role involves delivering consulting engagements that leverage machine learning capabilities on the Azure cloud to address customer challenges and drive digital transformation.; Azure Data Factory: Experience with Azure Data Factory is preferred, indicating involvement in building and managing data pipelines within the Azure cloud environment.; SQL: SQL Server knowledge is preferred, highlighting the importance of querying and managing relational databases as part of data-related tasks.; Power BI: Power BI is mentioned as a preferred skill, suggesting responsibilities related to creating business intelligence dashboards and visualizations to support data-driven decision making.; Data & AI: The job requires proficiency in data and AI technologies on Azure, including SQL, Azure SQL, Azure Data Factory, and Power BI, to build and deploy data science and analytics solutions."
7rVkGrihy04aQlXdAAAAAA==,['Deep Learning'],"Deep Learning: Applying deep learning and neural network techniques for advanced modeling in fraud detection, scams, and social engineering prevention within financial services.","['Machine Learning', 'Statistical Modeling', 'Data Analysis', 'Risk Models', 'Python', 'R']","Machine Learning: Developing and validating machine learning models to support risk and compliance functions, including predictive modeling and optimization of existing models.; Statistical Modeling: Applying advanced statistical models to analyze data and support decision-making in risk prevention and compliance.; Data Analysis: Analyzing large datasets to identify trends, patterns, and actionable insights that inform business decisions and strategic direction.; Risk Models: Understanding and enhancing current risk models to build flexible solutions that maintain high standards for risk prevention and compliance.; Python: Using Python programming language for implementing machine learning and statistical models as part of data science projects.; R: Utilizing R programming language for statistical analysis and model development in risk and compliance contexts."
FhX3tdkq7XTgImwEAAAAAA==,[],,"['Predictive Modeling', 'Supervised and Unsupervised Learning', 'Data Cleaning and Transformation', 'Algorithm Design and Optimization', 'Data Science Model Development and Deployment', 'Statistical Analysis and Simulations', 'Collaborative Software Development']","Predictive Modeling: Develop and manage state-of-the-art predictive algorithms that use data at scale to automate and optimize decisions in merchandising and other business areas.; Supervised and Unsupervised Learning: Apply supervised and unsupervised machine learning methods using Python to solve retail challenges and improve merchandising assortment products.; Data Cleaning and Transformation: Clean, transform, and analyze large datasets to generate insights that lead to business improvements in merchandising and retail operations.; Algorithm Design and Optimization: Utilize mathematical and statistical concepts, algorithm design, and optimization techniques to address business problems and improve model performance.; Data Science Model Development and Deployment: Design, develop, deploy, and maintain data science models and tools for enterprise merchandising solutions with large-scale business impact.; Statistical Analysis and Simulations: Apply statistical concepts, data analysis, simulations, and visualizations to support decision-making and optimize merchandising strategies.; Collaborative Software Development: Develop, test, and maintain large codebases in a collaborative environment following industry best practices to support data science and machine learning projects."
msyDjsdH77JYpdO3AAAAAA==,[],,"['Statistical Modeling', 'Machine Learning Models', 'Model Validation and Backtesting', 'Clustering', 'Classification', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'Python', 'Conda', 'AWS (Amazon Web Services)', 'Spark', 'SQL', 'Model Governance']","Statistical Modeling: Used to build and validate models that assess and quantify risks associated with financial products, supporting decision-making in risk management.; Machine Learning Models: Built and validated to challenge existing production models and contribute to next-generation model governance within the Model Risk Office.; Model Validation and Backtesting: Performed to ensure the accuracy and reliability of models, including interpreting confusion matrices and ROC curves to assess model performance.; Clustering: Applied as a statistical method to group data points for analysis and model development in risk assessment.; Classification: Used as a supervised learning technique to categorize data for predictive modeling and risk evaluation.; Sentiment Analysis: Employed as part of data science solutions to analyze textual data and extract insights relevant to model risk and decision-making.; Time Series Analysis: Utilized to analyze sequential data over time for forecasting and risk modeling purposes.; Deep Learning: Experience with deep learning techniques is noted, likely applied to complex data modeling tasks within the risk office.; Python: Used as a primary programming language for developing data science solutions, building models, and data analysis.; Conda: Utilized as a package and environment management system to support Python-based data science workflows.; AWS (Amazon Web Services): Leveraged as a cloud computing platform to handle big data processing, storage, and model deployment.; Spark: Used for big data processing and analytics to manage and analyze large-scale datasets efficiently.; SQL: Applied to retrieve, combine, and analyze data from various structured sources as part of data preparation and analysis.; Model Governance: Involved in overseeing and managing the lifecycle and compliance of machine learning models to ensure their reliability and regulatory adherence."
L5wOvziyLx6DO_acAAAAAA==,['Large Language Models'],"Large Language Models: Support data format and process development to integrate with LLMs, indicating involvement with AI platform tools and evolving program analytics to connect with unstructured data sources.","['Data Visualization', 'Business Analytics', 'Data Management', 'SQL', 'Tableau', 'Statistical Software Tools', 'Data Mining', 'Python', 'MySQL', 'D3.js', 'Visual Basic', 'R Programming']","Data Visualization: Develop and maintain analytic visualizations and dashboards using tools like Tableau to monitor operations and communicate program performance to leadership.; Business Analytics: Support business analytics processes to measure program performance, inform decision-making, and enhance analytic capacity.; Data Management: Manage data sources including Jira, Excel, and SQL databases, create relationships between disparate data sources, and maintain data documentation such as SOPs, data dictionaries, and process flows.; SQL: Utilize SQL databases as a data source for developing visualizations and conducting data analysis.; Tableau: Develop and maintain dashboards tailored to specific audience groups, focusing on user interface design and storytelling to present complex data effectively.; Statistical Software Tools: Use commercial off-the-shelf statistical software such as SPSS, SAS, and MatLab for data visualization and analysis.; Data Mining: Develop, manipulate, and maintain databases to extract and summarize statistical data for reporting and presentations.; Python: Employ Python programming for data summarization, analysis, and visualization tasks.; MySQL: Use MySQL databases as part of data management and analysis workflows.; D3.js: Utilize D3.js for creating dynamic and interactive data visualizations.; Visual Basic: Apply Visual Basic programming to support data summarization and reporting.; R Programming: Use R for statistical data analysis and creating reports and presentations."
bmNB0Pq_1UeBMrgkAAAAAA==,['Artificial Intelligence'],"Artificial Intelligence: Design, development, and maturation of AI solutions to address complex engineering and operational challenges within defense systems; implementation and deployment of AI models to production environments to enhance automation and decision-making processes.","['Machine Learning', 'Supervised Learning', 'Unsupervised Learning', 'Generative AI', 'Advanced Data Analytics', 'Predictive Analytics', 'Python Programming', 'Cloud-Based Model Development']","Machine Learning: Design and development of machine learning algorithms to solve complex engineering and operational challenges within defense systems; implementation and integration of machine learning models into production environments to enhance predictive analytics, automation, and decision-making.; Supervised Learning: Application of supervised learning models as part of the development and training of machine learning models for defense system challenges.; Unsupervised Learning: Application of unsupervised learning models as part of the development and training of machine learning models for defense system challenges.; Generative AI: Development and application of generative AI models as part of implementing advanced data analytics and AI solutions for defense systems.; Advanced Data Analytics: Use of advanced data analytics techniques in conjunction with machine learning and AI models to address operational challenges and improve system capabilities.; Predictive Analytics: Enhancement of system capabilities through predictive analytics enabled by machine learning and AI model integration into production environments.; Python Programming: Proficiency in Python programming used for developing machine learning and AI models in a cloud-based environment.; Cloud-Based Model Development: Experience developing machine learning and AI models within cloud-based environments to support deployment and scalability."
-1hEf6Rm6gqcogwzAAAAAA==,"['Large Language Models (LLMs)', 'Generative AI', 'PyTorch', 'Hugging Face', 'LangChain', 'Lightning', 'Vector Databases', 'Training Optimization', 'Self-Supervised Learning', 'Explainability', 'Reinforcement Learning from Human Feedback (RLHF)']","Large Language Models (LLMs): Central to building customer-facing applications by adapting and fine-tuning these models to deliver personalized and dynamic experiences.; Generative AI: Leveraged to create next-generation customer experiences powered by emerging AI technologies.; PyTorch: Used as a deep learning framework to develop and train neural network models including LLMs.; Hugging Face: Utilized as an open-source platform and library for accessing, fine-tuning, and deploying transformer-based language models.; LangChain: Employed to build AI applications that integrate LLMs with external data sources and workflows.; Lightning: Used as a framework to streamline and scale deep learning model training and deployment.; Vector Databases: Applied to efficiently store and retrieve high-dimensional embeddings for AI-powered search and recommendation features.; Training Optimization: Expertise in improving the efficiency and effectiveness of training large AI models.; Self-Supervised Learning: Used as a technique to train models on unlabeled data to improve performance and generalization.; Explainability: Applied to interpret and understand AI model decisions, enhancing transparency and trust.; Reinforcement Learning from Human Feedback (RLHF): Used to fine-tune AI models by incorporating human feedback to improve alignment with user needs.","['Statistical Modeling', 'Relational Databases', 'Machine Learning', 'Natural Language Processing (NLP)', 'Model Training and Evaluation', 'Data Analytics', 'Python, Scala, R', 'SQL']","Statistical Modeling: Used historically to personalize credit card offers and drive data-driven decision-making at scale.; Relational Databases: Utilized for managing and querying structured data as part of data analytics and machine learning workflows.; Machine Learning: Applied to build predictive models and data-driven solutions that improve customer experiences and financial decision-making.; Natural Language Processing (NLP): Employed to build models that process and understand textual data, enabling features like digital assistants and content search.; Model Training and Evaluation: Involves all phases of machine learning model development including design, training, evaluation, and validation to ensure robust performance.; Data Analytics: Performed using open source programming languages and relational databases to analyze large-scale numeric and textual data.; Python, Scala, R: Programming languages used for data analysis, machine learning model development, and data processing.; SQL: Used for querying and managing data within relational databases."
Dj-wVQK6Iss0u5ovAAAAAA==,['Transformers'],"Transformers: Apply Transformer architectures as part of deep learning techniques to solve complex, data-intensive problems including NLP and computer vision tasks.","['Machine Learning', 'Deep Learning', 'Evaluation Metrics', 'Feature Engineering', 'Data Pipelines', 'MLOps', 'Python', 'ML Libraries', 'Cloud Platforms', 'Containerization and Model Serving', 'Customer Data Enrichment']","Machine Learning: Design, build, and evaluate machine learning models for classification, regression, recommendation, NLP, computer vision, and time-series forecasting; lead development of ML products from prototyping through production deployment and continuous improvement; proven track record of building, evaluating, and deploying ML models at scale in production environments.; Deep Learning: Apply deep learning techniques such as CNNs, RNNs, LSTMs, and Transformers to solve complex, data-intensive problems; hands-on experience with real-world applications including recommendation engines, fraud detection, customer segmentation, document summarization, image recognition, and speech processing; deep understanding of neural networks, model regularization, overfitting/underfitting prevention, and GPU-accelerated training.; Evaluation Metrics: Select appropriate architectures and hyperparameters, optimize model performance, and use evaluation metrics such as AUC, F1, BLEU, IoU, and perplexity based on the use case.; Feature Engineering: Design automated pipelines for data preprocessing and feature engineering as part of training and inference workflows, supporting batch or real-time processing.; Data Pipelines: Design automated pipelines for data preprocessing, feature engineering, training, and inference (batch or real-time); implement retraining pipelines to evaluate model drift and monitor performance post-deployment as part of production MLOps systems.; MLOps: Evaluate model drift, monitor performance post-deployment, and implement retraining pipelines within production MLOps systems; familiarity with MLOps tools such as MLflow, SageMaker, Airflow, and Kubeflow; experience with CI/CD for ML, feature stores, and real-time inference systems.; Python: Proficiency in Python programming language used for developing machine learning and deep learning models.; ML Libraries: Experience with machine learning libraries including scikit-learn and XGBoost for traditional ML, and TensorFlow, Keras, and PyTorch for deep learning model development.; Cloud Platforms: Experience with cloud platforms such as AWS, Google Cloud Platform, and Azure for deploying and serving machine learning models.; Containerization and Model Serving: Experience with containerization technologies and model serving tools to deploy machine learning models in production environments.; Customer Data Enrichment: Experience working with customer data enrichment to improve data quality and model inputs for better targeting and segmentation."
QN_ILKZJ_sCHUaNrAAAAAA==,"['Transformers', 'Language Models']","Transformers: Used specifically as a modern AI architecture for language models to enhance machine learning applications involving natural language processing.; Language Models: Applied as AI models to process and generate human language, supporting advanced recommendation systems and other AI-driven features.","['Statistical Modeling', 'Relational Databases', 'Machine Learning', 'SQL', 'Python', 'PyTorch', 'Transformers', 'Language Models', 'Conda', 'AWS', 'H2O', 'Spark']","Statistical Modeling: Used to personalize credit card offers and analyze large volumes of numeric and textual data to reveal insights and support data-driven decision-making.; Relational Databases: Utilized for storing and querying large-scale structured data, supporting data retrieval and analysis.; Machine Learning: Applied to build, train, evaluate, validate, and implement predictive models and recommendation systems at scale to deliver customer value.; SQL: Used for querying and managing data within relational databases as part of data analysis and model development workflows.; Python: A primary programming language used for data analysis, machine learning model development, and working with various data science tools and libraries.; PyTorch: Employed as a framework for building and training machine learning models, including deep learning architectures.; Transformers: Used as part of language model development and implementation to enhance machine learning applications involving textual data.; Language Models: Leveraged to process and analyze textual data, supporting recommendation systems and other machine learning applications.; Conda: Used as an environment and package management system to support reproducible data science workflows and dependency management.; AWS: Utilized as a cloud platform to support large-scale data storage, processing, and machine learning model deployment.; H2O: Applied as a machine learning platform to build and deploy scalable predictive models.; Spark: Used for distributed data processing and analytics on large datasets to support machine learning and data engineering tasks."
0RyW4ivnA3vAdnu5AAAAAA==,['Artificial Intelligence'],Artificial Intelligence: Building flexible AI solutions that integrate with existing risk tools to enhance risk prevention and compliance while maintaining business flexibility.,"['Machine Learning', 'Statistical Modeling', 'Deep Learning', 'Data Analysis', 'Data Visualization', 'Data Modeling', 'Python', 'R']","Machine Learning: Developing and validating machine learning models to support risk and compliance objectives, including predictive modeling and optimization of existing models.; Statistical Modeling: Applying statistical modeling techniques to analyze data and build risk models for fraud detection and compliance.; Deep Learning: Utilizing deep learning and neural network applications for advanced modeling in risk prevention and compliance.; Data Analysis: Analyzing large datasets to identify trends, patterns, and actionable insights that inform business decisions related to risk and compliance.; Data Visualization: Creating clear and concise visualizations to communicate complex data findings effectively to both technical and non-technical stakeholders.; Data Modeling: Developing clear and standard data models to capture the semantics of data and complex semi-structured content for communication with stakeholders.; Python: Using Python programming language for implementing machine learning models, data analysis, and other data science tasks.; R: Using R programming language for statistical analysis, modeling, and data science workflows."
Rt-lAc6aN-1QgYeeAAAAAA==,[],,"['Machine Learning Models', 'Statistical Methods', 'Supervised and Unsupervised Learning', 'Data Cleaning and Analysis', 'Predictive Modeling', 'SQL', 'Python', 'SAS', 'Cloud Platforms', 'Data Visualization Tools', 'Anomaly Detection', 'Behavioral Modeling', 'Network Analysis']","Machine Learning Models: Develop and deploy machine learning models to detect, predict, and prevent fraudulent transactions and behavior patterns, supporting decision-making and improving business performance.; Statistical Methods: Apply statistical methods to analyze data and develop data-driven strategies for detecting risky behaviors and preventing fraud.; Supervised and Unsupervised Learning: Utilize supervised and unsupervised fraud detection techniques, including anomaly detection, behavioral modeling, and network analysis, to identify fraud trends and root causes.; Data Cleaning and Analysis: Collect, clean, and analyze large, complex datasets from multiple sources to support fraud detection and risk analytics.; Predictive Modeling: Develop predictive models to support decision-making and improve business performance in fraud and risk detection.; SQL: Use SQL to query and manage large datasets relevant to fraud and risk analytics.; Python: Leverage Python programming for data analysis, machine learning model development, and deployment in fraud detection.; SAS: Utilize SAS software for statistical analysis and modeling in fraud and risk analytics.; Cloud Platforms: Work with cloud platforms such as AWS, Google Cloud Platform, and Azure to handle large datasets and deploy fraud detection solutions.; Data Visualization Tools: Use visualization tools like Tableau and Power BI to design and monitor KPIs and create reports that provide actionable insights into fraud cases.; Anomaly Detection: Apply anomaly detection techniques as part of unsupervised learning methods to identify unusual patterns indicative of fraud.; Behavioral Modeling: Use behavioral modeling to understand and detect fraudulent behavior patterns.; Network Analysis: Employ network analysis techniques to detect fraud by analyzing relationships and interactions within data."
UUkFI02qhxFHE3faAAAAAA==,[],,"['Exploratory Data Analysis', 'Statistical Techniques', 'Predictive Modeling', 'Machine Learning Techniques', 'Data Extraction, Cleaning, and Transformation', 'Entity Resolution', 'Data Mining', 'SQL', 'Relational Database Systems', 'Programming Languages for Data Analysis', 'Statistical Modeling', 'Big Data Solutions', 'Data Profiling', 'Signal Detection Theory', 'Agile Development Environment', 'Cloud Computing and Cloud Storage']","Exploratory Data Analysis: Used to identify significant trends, patterns, correlations, and anomalies in data to support business problem solving.; Statistical Techniques: Applied to analyze data for trends, patterns, and anomalies as part of exploratory data analysis and predictive modeling.; Predictive Modeling: Building models based on extracted, cleaned, and transformed data to forecast outcomes relevant to business problems.; Machine Learning Techniques: Leveraged various statistical and machine learning methods to identify patterns and anomalies in data.; Data Extraction, Cleaning, and Transformation: Performed on data within the problem space to prepare it for analysis and predictive modeling.; Entity Resolution: Experience working in fields involving entity resolution, fuzzy matching, and name matching to improve data quality and analytics.; Data Mining: Applied to discover patterns and insights from large datasets relevant to border security and trade efficiency.; SQL: Used for querying and managing data within relational database systems such as Oracle, MySQL, and Postgres.; Relational Database Systems: Hands-on experience with systems like Oracle, MySQL, and Postgres to store and manage data for analysis.; Programming Languages for Data Analysis: Proficiency in Python, R, Scala, or JavaScript and related packages to develop and analyze data solutions.; Statistical Modeling: Applied to analyze data and support business analytics and decision-making processes.; Big Data Solutions: Applied to manage and analyze the large volume and complexity of data related to physical and virtual border crossings.; Data Profiling: Conceptual understanding and experience related to assessing data quality and characteristics to support analytics.; Signal Detection Theory: Applied concepts to identify signals (patterns or anomalies) within complex data sets.; Agile Development Environment: Experience working within Agile frameworks to manage and deliver data analytics projects efficiently.; Cloud Computing and Cloud Storage: Experience with cloud technologies to support data storage and computing needs for analytics projects."
d2tLs81XH-GUTsjhAAAAAA==,['Generative AI'],"Generative AI: Experience with generative AI technologies is preferred, indicating involvement with modern AI methods beyond traditional machine learning.","['Statistical Modeling', 'Python', 'Conda', 'AWS', 'H2O', 'Spark', 'Machine Learning Models', 'Model Validation and Backtesting', 'Clustering', 'Classification', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'SQL', 'Data Retrieval and Integration', 'Confusion Matrix and ROC Curve Interpretation']","Statistical Modeling: Used to personalize credit card offers and drive data-driven decision-making in financial services.; Python: A primary programming language used for data science solutions and large scale data analysis.; Conda: A package and environment management system leveraged to manage dependencies and environments for data science projects.; AWS: Cloud computing platform used for data storage, processing, and analytics, with at least one year of experience preferred.; H2O: An open-source machine learning platform used to build and deploy machine learning models at scale.; Spark: A big data processing framework used to handle huge volumes of numeric and textual data efficiently.; Machine Learning Models: Built through all phases including design, training, evaluation, validation, and implementation to support risk management and financial crime detection.; Model Validation and Backtesting: Processes to ensure model accuracy, reliability, and compliance with regulatory standards.; Clustering: A statistical method used for grouping data points, applied in model development and analysis.; Classification: A supervised learning technique used to categorize data, relevant for detecting financial crimes and risk assessment.; Sentiment Analysis: Analyzing textual data to extract sentiment, used as part of data insights.; Time Series Analysis: Analyzing sequential data over time, applied in model development and risk assessment.; Deep Learning: Applied as part of advanced modeling techniques to improve predictive accuracy and insights.; SQL: Used for querying and managing structured data in relational databases.; Data Retrieval and Integration: Skills to retrieve, combine, and analyze data from various sources and structures to support data science workflows.; Confusion Matrix and ROC Curve Interpretation: Techniques used to evaluate classification model performance and effectiveness."
IP3-YmuaqDpmJhu9AAAAAA==,"['Retrieval-Augmented Generation', 'Large Language Models', 'Transformers', 'Deep Learning Frameworks', 'Natural Language Processing']","Retrieval-Augmented Generation: Hands-on experience with Retrieval-Augmented Generation (RAG) systems is required to enhance language models with external knowledge retrieval capabilities.; Large Language Models: The role involves training, pretraining, and fine-tuning large language models (LLMs) for telecom-related NLP tasks.; Transformers: Experience with transformer architectures is necessary for developing advanced NLP models and AI applications.; Deep Learning Frameworks: Proficiency in deep learning frameworks such as TensorFlow and PyTorch is required for building and deploying neural network models.; Natural Language Processing: Hands-on experience with NLP models is needed to process and analyze telecom text data using AI techniques.","['Machine Learning', 'Anomaly Detection', 'Data Preprocessing', 'Feature Engineering', 'Model Evaluation', 'Scikit-Learn']",Machine Learning: The job requires strong experience in machine learning techniques and applying them to telecom data for actionable insights and anomaly detection.; Anomaly Detection: Expertise in statistical and machine learning-based anomaly detection techniques is needed to identify unusual patterns in telecom data.; Data Preprocessing: A strong background in data preprocessing is required to prepare telecom datasets for modeling and analysis.; Feature Engineering: The role involves feature engineering to create meaningful input variables for machine learning models.; Model Evaluation: Experience in evaluating machine learning models to ensure accuracy and reliability in telecom applications is necessary.; Scikit-Learn: Experience with Scikit-Learn is required as part of the machine learning framework toolkit for building and evaluating models.
BKMzSlD6Rlg6w4-UAAAAAA==,[],,"['Data Analytics', 'Machine Learning', 'Statistical Methods', 'Data Fusion', 'Application Development', 'Data Governance']",Data Analytics: Utilize advanced data analytics to deliver actionable insights for complex organizational challenges.; Machine Learning: Apply machine learning methods to solve complex technical and business problems and drive innovation.; Statistical Methods: Use statistical methods to analyze complex datasets and support decision-making processes.; Data Fusion: Support integrated ISR operations through data fusion capabilities to enhance mission management for defense and intelligence communities.; Application Development: Create and implement new applications based on professional scientific principles and theories to enhance organizational capabilities.; Data Governance: Ensure adherence to ethical data handling practices and compliance with relevant data governance frameworks.
pz9qdlT5XuoksyxlAAAAAA==,"['TensorFlow', 'PyTorch']",TensorFlow: Use TensorFlow framework to develop and apply neural network models as part of machine learning and NLP tasks.; PyTorch: Use PyTorch framework to develop and apply neural network models as part of machine learning and NLP tasks.,"['Natural Language Processing', 'Machine Learning', 'Data Extraction from Unstructured/Semi-structured Sources', 'Python Programming', 'R Programming', 'Data Visualization and Storytelling']","Natural Language Processing: Develop and apply NLP models to derive insights from textual and structured data extracted from unstructured and semi-structured sources such as PDFs and DoDAF artifacts.; Machine Learning: Develop and apply ML models to analyze data and drive strategic insights and operational effectiveness in mission-critical efforts.; Data Extraction from Unstructured/Semi-structured Sources: Extract and transform data from unstructured and semi-structured documents using tools like Apache Tika and PDFMiner to support analysis and insight generation.; Python Programming: Develop and maintain scripts and tools using Python to support data processing, analysis, and model development.; R Programming: Develop and maintain scripts and tools using R to support data processing, analysis, and model development.; Data Visualization and Storytelling: Conduct qualitative analysis and present findings through data visualization and storytelling to communicate insights effectively."
gcu5TL4WgQ8LDCeMAAAAAA==,[],,"['Data Pipelines', 'Python', 'SQL']","Data Pipelines: Responsible for establishing and maintaining data pipelines using Python and SQL to support client mission and information needs.; Python: Used as a modern software tool to develop software and databases, including building and maintaining data pipelines.; SQL: Utilized for database development and managing data pipelines to meet client data strategy requirements."
76N88hiAX0Vi-aMyAAAAAA==,"['AI-Powered Platforms', 'ChatGPT', 'Gemini Pro']","AI-Powered Platforms: Leveraged to enhance existing analytical frameworks, automate data discovery, identify hidden patterns, and generate prescriptive recommendations, thereby accelerating delivery of actionable insights to product and engineering teams.; ChatGPT: Utilized as an AI-powered tool to generate SQL queries and research techniques, enhancing and accelerating data science output.; Gemini Pro: Used as an AI-powered platform for text analysis to support data science tasks and improve analytical capabilities.","['SQL', 'Python', 'R', 'SAS', 'SPSS', 'A/B Testing', 'Multivariate Testing', 'Multi-Armed Bandit', 'Data Visualization', 'Looker', 'Google BigQuery (GBQ)', 'Tableau', 'PowerBI', 'Machine Learning', 'Advanced Analytical Techniques']","SQL: Used for querying and managing large and complex datasets to support data analysis and insights generation.; Python: Employed for developing machine learning proof-of-concepts, training models, and visualizing outputs to demonstrate possibilities to stakeholders.; R: Mentioned as a statistical programming language option for conducting quantitative analyses and statistical modeling.; SAS: Listed as a statistical programming language option for quantitative analysis and statistical modeling.; SPSS: Included as a statistical programming language option for conducting quantitative analyses and statistical modeling.; A/B Testing: Used as an experimental test design technique to drive business decision making through controlled experiments.; Multivariate Testing: Applied as an experimental design method to evaluate multiple variables simultaneously for business insights.; Multi-Armed Bandit: Utilized as an experimental test design approach to optimize decision making by balancing exploration and exploitation.; Data Visualization: Involves creating reports, dashboards, and analyses using tools like Looker, Google BigQuery (GBQ), Tableau, and PowerBI to distribute data insights effectively.; Looker: Used as a data visualization and reporting tool to create dashboards and distribute insights in an accessible format.; Google BigQuery (GBQ): Employed as a data warehousing and querying platform to support data visualization and analysis workflows.; Tableau: Mentioned as a data visualization software used to create interactive dashboards and reports for business stakeholders.; PowerBI: Used as a business intelligence tool for data visualization and reporting to support decision making.; Machine Learning: Applied to build proof-of-concept models using Python to solve complex business problems and demonstrate potential solutions.; Advanced Analytical Techniques: Used to perform deep-dive analyses and develop data science models that address critical and complex business challenges."
_gZKL81fh2RZAVeSAAAAAA==,['TensorFlow'],"TensorFlow: TensorFlow is preferred for machine learning and deep learning projects, indicating use of neural network frameworks.","['Statistics', 'SAS', 'Python', 'Data Visualization Tools', 'Machine Learning', 'Computer Vision', 'Java', 'Core Java', 'JavaScript', 'C++', 'Spring Boot', 'Microservices', 'Docker', 'Jenkins', 'REST APIs', 'NLP', 'Project Work']","Statistics: Knowledge of statistics is required for data science and machine learning positions to analyze and interpret data effectively.; SAS: Experience with SAS is preferred for data analysis and statistical modeling tasks.; Python: Python programming skills are needed for data science and machine learning projects, including data manipulation and analysis.; Data Visualization Tools: Familiarity with data visualization tools such as Tableau and PowerBI is preferred to create dashboards and visual reports.; Machine Learning: Machine learning knowledge is required for data science roles, including project work on relevant technologies.; Computer Vision: Experience or knowledge in computer vision is mentioned as part of the data science skill set.; Java: Java programming experience is required, including understanding of the software development life cycle, relevant for both data science and software development roles.; Core Java: Knowledge of Core Java is required for software programming tasks.; JavaScript: JavaScript knowledge is required for software programming and full stack development.; C++: C++ programming knowledge is required for software programming.; Spring Boot: Experience with Spring Boot framework is required for building microservices and backend applications.; Microservices: Experience with microservices architecture is required for software development roles.; Docker: Docker experience is required for containerization and deployment of applications.; Jenkins: Jenkins experience is required for continuous integration and continuous deployment (CI/CD) pipelines.; REST APIs: Experience with REST APIs is required for building and consuming web services.; NLP: Natural Language Processing is a preferred skill for text mining and data analysis tasks.; Project Work: Hands-on project experience is emphasized as important for demonstrating skills in data science, machine learning, and software development."
Jb1RDQ_HhaFRXyACAAAAAA==,[],,"['Big Data Analytics', 'Statistical Modeling', 'Data Visualization', 'Database Management', 'Data Integration and ETL', 'Scripting and Programming Languages', 'Geospatial Intelligence (GEOINT) Analysis', 'Data Preparation and Normalization']","Big Data Analytics: Responsible for querying, visualizing, aggregating, correlating, and analyzing large datasets across intelligence disciplines to extract meaningful insights and improve operational response times.; Statistical Modeling: Writing scripts in Visual Basic, R, and Python to develop statistical models that identify patterns, relationships, and anticipatory behaviors in large datasets, supporting hypothesis testing and knowledge capture.; Data Visualization: Creating visualizations that effectively communicate analytical discoveries and are easily integrated into daily operations, using tools such as Tableau and Excel.; Database Management: Maintaining, accessing, moving, and optimizing data stored in various database systems including SQL databases, NoSQL databases, ArcServer, Oracle, and Access to improve query performance and data handling.; Data Integration and ETL: Developing processes and custom tools to automate data ingestion, cleaning, migration, and normalization from legacy and new datasets to ensure repeatable and explainable data workflows.; Scripting and Programming Languages: Utilizing programming languages such as Visual Basic, R, and Python to support data manipulation, statistical analysis, and automation of analytic processes.; Geospatial Intelligence (GEOINT) Analysis: Applying GEOINT standards and quality measures to solve complex geospatial analysis problems using structured data and relational databases, often involving ArcGIS and ArcServer.; Data Preparation and Normalization: Developing tradecraft techniques and training solutions for the discovery, preparation, manipulation, and normalization of big data to ensure consistent and repeatable analytic methods."
8YiP-FtLhRFLwhfuAAAAAA==,"['Large Language Models', 'Prompt Engineering', 'Copilot Memory and Personalization AI']","Large Language Models: Stay current on the latest research related to large language models (LLMs), particularly focusing on evaluation and prompting techniques.; Prompt Engineering: Engage with the latest prompting methods for LLMs to improve model interaction and output quality.; Copilot Memory and Personalization AI: Develop AI systems that remember, evolve, and personalize user experiences by deepening memory with every interaction to create trusted partner-like interactions.","['Evaluation Methodologies', 'Data Pipelines', 'Statistical Techniques', 'Advanced Algorithms', 'Product Analytics and Metric Tracking', 'Experimentation and Instrumentation', 'Data Storytelling and Communication']","Evaluation Methodologies: Develop and improve methodologies to assess model output quality using both machine evaluation and human evaluation metrics and coverage.; Data Pipelines: Design, implement, and maintain scalable data pipelines to extract, transform, and structure product logs and large-scale telemetry data for evaluation and analysis purposes.; Statistical Techniques: Apply statistical techniques to manage structured and unstructured data and report results as part of data science responsibilities.; Advanced Algorithms: Use advanced algorithms and tools to conduct hands-on analysis of large-scale telemetry data to derive actionable insights.; Product Analytics and Metric Tracking: Drive actionable product insights, opportunity analyses, and metric tracking to guide product direction and success.; Experimentation and Instrumentation: Develop new instrumentation and measurement approaches to evaluate new feature performance through experimentation.; Data Storytelling and Communication: Articulate insights and storyboard with data to communicate findings effectively and influence leadership and key decision makers."
K24fatQuZpsBE5vTAAAAAA==,['AI Opportunities Investigation'],"AI Opportunities Investigation: The role involves investigating AI opportunities to enhance customer experience, reduce manual work, and automate remedial tasks, indicating an interest in applying modern AI techniques to business processes.","['Statistics', 'Machine Learning', 'Data Analysis Tools', 'Data Management and Modeling', 'Business Intelligence (BI) Tools', 'Relational Databases', 'Data Exploration with Structured and Unstructured Data', 'Prototyping and Performance Evaluation']","Statistics: The role requires advanced knowledge of statistics to apply statistical methods for business operations and data analysis.; Machine Learning: The job involves applying machine learning techniques to business operations and conducting research and prototyping of new analyses using machine learning algorithms.; Data Analysis Tools: Experience with various data analysis tools such as R, Python, Scala, and Java is required to research, implement algorithms, and analyze data.; Data Management and Modeling: The position includes responsibilities for managing and modeling data, including driving the collection of new data and refining existing data sources.; Business Intelligence (BI) Tools: Developing dashboards and reports using industry-grade BI tools like PowerBI, Tableau, and Domo is a key responsibility to support business metrics and visualization.; Relational Databases: Strong skills in relational databases are necessary to manipulate and analyze complex, high-volume, and high-dimensional data from various sources.; Data Exploration with Structured and Unstructured Data: The role involves exploring undefined business problems using both structured and unstructured data to identify insights and opportunities.; Prototyping and Performance Evaluation: The job includes prototyping new analyses, investigating data, evaluating performance, and summarizing findings for leadership to drive business improvements."
VfSdEFPB8XE0uiclAAAAAA==,[],,"['Cybersecurity Principles', 'Data Science Techniques', 'Machine Learning']","Cybersecurity Principles: The job requires a deep understanding of cybersecurity principles to identify, detect, prevent, and respond to cyber threats protecting critical production environments and operational technology systems.; Data Science Techniques: The role involves applying data science techniques to analyze security incidents and vulnerabilities within the cybersecurity domain.; Machine Learning: Proficiency in machine learning is required to support cybersecurity operations, including monitoring, detecting, and responding to cyber threats."
u9HcyQcPlAli2Oy1AAAAAA==,['Artificial Intelligence'],"Artificial Intelligence: Leveraging artificial intelligence techniques to develop automated approaches that streamline program, process, and report inputs for XM30.","['Data Science Principles', 'Data Analytics', 'Machine Learning', 'Natural Language Processing', 'Data Visualization', 'Statistical Analysis', 'Programming Languages', 'Reporting and Documentation', 'Problem Solving with Data', 'Big Data Technologies', 'Business Intelligence Tools']","Data Science Principles: Applying data science principles, concepts, and practices to analyze systems, processes, and operational challenges using scientific methods and techniques.; Data Analytics: Performing analytics on complex datasets to identify patterns, trends, and insights that support organizational decision-making.; Machine Learning: Developing automated approaches leveraging machine learning to streamline the input of programs, processes, and reports for XM30.; Natural Language Processing: Using natural language processing techniques to automate and improve the input of programs, processes, and reports for XM30.; Data Visualization: Developing data visualization solutions to address operational problems and support decision-making.; Statistical Analysis: Applying knowledge of mathematics and statistical analysis to analyze and interpret data for recommending practical solutions.; Programming Languages: Using programming languages such as Python, R, or SQL to perform data analysis and develop data science solutions.; Reporting and Documentation: Preparing comprehensive reports, documentation, and correspondence that clearly communicate factual and procedural information.; Problem Solving with Data: Analyzing problems to identify significant factors, gather pertinent data, and develop practical, evidence-based solutions.; Big Data Technologies: Having background knowledge in big data technologies and cloud computing platforms to support data science tasks.; Business Intelligence Tools: Experience with data visualization tools such as Tableau and Power BI to create dashboards and visual analytics."
WIxMPf-oM0kHz_ZkAAAAAA==,[],,"['Predictive Modeling', 'SQL', 'Machine Learning Algorithms', 'Data Visualization Tools', 'Advanced Analytics', 'Data Quality and Governance', 'Statistical Modeling Techniques', 'Programming Languages', 'Digital Marketing Metrics and KPIs']","Predictive Modeling: Develop and implement predictive models to generate insights and recommendations for optimizing keyword bids based on volume and profit.; SQL: Utilize complex SQL logic to extract, transform, and load (ETL) data from disparate tables and sources.; Machine Learning Algorithms: Apply machine learning algorithms and statistical techniques to uncover hidden relationships and correlations within large first-party datasets.; Data Visualization Tools: Use tools like Tableau or Power BI to create visualizations and interactive dashboards for presenting findings to non-technical audiences and senior leadership.; Advanced Analytics: Leverage advanced analytics to drive insights and optimize digital commerce and retail media strategies, including strategic recommendations and business outcome improvements.; Data Quality and Governance: Evaluate data quality and completeness, identify gaps, improve data utility, and partner with data stewards to maintain and enhance data quality through governance policies, validation, and quality checks.; Statistical Modeling Techniques: Employ statistical modeling techniques alongside machine learning to analyze data and support predictive insights.; Programming Languages: Proficiency in Python or R for data analysis, modeling, and advanced analytics tasks.; Digital Marketing Metrics and KPIs: Familiarity with marketing metrics such as Customer Acquisition Cost (CAC), Impressions, Click-Through Rate (CTR), Conversion Rate, Return on Ad Spend (ROAS), Customer Lifetime Value (CLV), and Attribution Modeling to inform analytics and recommendations."
_n08oRCBfg_0HZYmAAAAAA==,"['Large Language Models', 'Artificial Intelligence', 'Machine Learning']",Large Language Models: Have functional knowledge of large learning models as part of potential artificial intelligence solutions to meet Division data and automation requirements.; Artificial Intelligence: Identify and evaluate potential AI solutions to enhance data and automation capabilities within the Operation Assessment Division.; Machine Learning: Consider and apply machine learning solutions to improve data analytics and automation processes in support of command assessments and decision-making.,"['Data Science', 'Operations Research', 'R Programming', 'Python Programming', 'SQL/PostgreSQL', 'Data Visualization', 'Data Stream Interfaces', 'Analytic Methods', 'MLOps', 'Git', 'Knowledge Management', 'Business Intelligence Tools']","Data Science: Provide data science capabilities on-site for a combatant command Operation Assessment Division, including designing, developing, and applying data collection and decision analytics processes using mathematical, statistical, and other analytic methods.; Operations Research: Apply operations research techniques to support assessments and decision analytics within a combatant command environment, including the use of mathematical and statistical methods.; R Programming: Use R and R-Shiny to build digital solutions that automate data collection, analysis, and staff processes, and to develop real-time or near real-time data visualization and analysis methodologies.; Python Programming: Utilize Python and Python-Shiny to develop automated applications and digital solutions that enhance data collection, analysis, and operational efficiency.; SQL/PostgreSQL: Employ SQL and PostgreSQL for managing and interfacing with authoritative data sources to support assessments and risk analysis.; Data Visualization: Develop data visualizations and information displays to facilitate senior leadership decisions and support analytic products.; Data Stream Interfaces: Identify and develop interfaces for authoritative data streams to support assessments and risk analysis within the command environment.; Analytic Methods: Design and apply a variety of mathematical, statistical, and analytic methods to support decision analytics and operational assessments.; MLOps: Possess functional knowledge of machine learning operations to support potential AI and ML solutions within the Division's data and automation requirements.; Git: Use Git for version control and collaboration in developing data science and analytic solutions.; Knowledge Management: Support knowledge management and information process requirements to enhance operational planning and analytic capabilities.; Business Intelligence Tools: Develop analytic tools and dashboards integrated into systems like MAVEN Smart Systems, C2IE, and Advana to support command and control functions."
FtYf8CMHxr7yuuEPAAAAAA==,[],,"['Data Wrangling', 'Analytics Methods for Big Data', 'Feature Detection', 'Statistical Analysis', 'Data Mining', 'Predictive Modeling', 'Machine Learning', 'Natural Language Processing', 'Business Intelligence']","Data Wrangling: Involves performing pre-analytics tasks such as data collection, cleansing, integration, storage, and retrieval to prepare data for analysis.; Analytics Methods for Big Data: Applying analytical techniques suitable for large-scale data to derive actionable insights and support decision-making.; Feature Detection: Using techniques to identify relevant features in data that contribute to predictive modeling and analysis outcomes.; Statistical Analysis: Employing statistical methods to interpret data, validate results, and support scientific approaches in deriving value from data.; Data Mining: Utilizing data mining techniques to discover patterns and relationships within data to inform predictive modeling and business intelligence.; Predictive Modeling: Developing models that use historical data to predict future outcomes, supporting mission needs and analytical objectives.; Machine Learning: Applying machine learning techniques to analyze data and build models that improve decision-making and predictive accuracy.; Natural Language Processing: Using NLP techniques to analyze and interpret textual data as part of the analytical approach to achieve results.; Business Intelligence: Leveraging BI tools and methods to visualize data and communicate insights effectively to stakeholders."
rTGWz-jOqmLvvZ1EAAAAAA==,[],,"['Experimentation Management', 'Statistical Analysis', 'SQL', 'Python', 'Online A/B Testing', 'Telemetry and Instrumentation']","Experimentation Management: Managing and analyzing experiments by collaborating with feature teams to design, implement, and debug experiments that provide actionable insights for product and user experience improvements.; Statistical Analysis: Performing power analysis and experimental inference to ensure the validity and reliability of experiments.; SQL: Using SQL to pull and analyze data quickly to gain insights into user behavior and experiment outcomes.; Python: Utilizing Python for data analysis to extract insights from experiment data and user behavior.; Online A/B Testing: Experience conducting online A/B testing to evaluate the impact of product features and changes on user behavior and metrics.; Telemetry and Instrumentation: Working closely with partners to ensure robust telemetry and instrumentation frameworks are in place to support data collection and experiment design."
YqXeVDPZnyloL4yhAAAAAA==,[],,"['SQL', 'DBT', 'Tableau', 'Healthcare Claims Data', 'Data Quality and Validation', 'Data Science Collaboration', 'Python', 'Benefit Expansion Analysis', 'Git and CI/CD']","SQL: Used to write performant queries for transforming and analyzing large datasets primarily in Snowflake cloud data warehouse.; DBT: Used for designing, maintaining, modular data modeling, testing, and documentation within a medallion data architecture.; Tableau: Used to build visually compelling, user-friendly dashboards for executive and operational stakeholders focused on dental claims, utilization trends, and benefit impact.; Healthcare Claims Data: Involves interpreting Medicaid and Medicare dental data, including CDT codes, utilization patterns, benefit design, and dental domain terminology for care progression, cost modeling, and policy evaluation.; Data Quality and Validation: Ensuring data quality, validation, and documentation to maintain auditability of data and models.; Data Science Collaboration: Collaborating across data science, actuarial, and operations teams to deliver insights supporting utilization-based scoring, claims optimization, and policy evaluation.; Python: Used for data transformation and supporting machine learning tasks.; Benefit Expansion Analysis: Exposure to analyzing benefit expansion or dental claims optimization to support policy and operational decisions.; Git and CI/CD: Familiarity with version control and continuous integration/continuous deployment workflows to support development and automation."
cD4EfZ4utwJ23pWwAAAAAA==,[],,"['SQL', 'Python', 'Data Visualization Tools', 'Data Analysis and Trend Recognition', 'Data Accuracy and Assessment', 'Investigative Data Support', 'Quality Control and Reporting', 'Data Entry and Processing Systems', 'Unstructured Data Manipulation']","SQL: Used for querying and manipulating data as part of the data analysis tasks in support of law enforcement investigations.; Python: Utilized for data analysis and manipulation, supporting investigative data expertise and analytical skills required for the role.; Data Visualization Tools: Includes MS Excel, Power BI, and similar tools used to design charts, create formulas, build pivot tables, and manipulate unstructured data from various platforms to support reporting and trend recognition.; Data Analysis and Trend Recognition: Involves compiling and analyzing data from multiple sources to identify trends across various data sets, locations, and targets relevant to criminal and civil investigations.; Data Accuracy and Assessment: Monitoring data for accuracy, integrity, authenticity, and relevancy, identifying intelligence gaps, and assessing data viability for investigative purposes.; Investigative Data Support: Collaborating with teams to examine data gathered from investigations and developing methodologies to exploit investigative information effectively.; Quality Control and Reporting: Participating in the development of work products, conducting progress reviews, ensuring quality control of findings, and regularly reporting results to lead personnel.; Data Entry and Processing Systems: Performing data entry from hard and soft copies into large-scale data processing systems using tools like optical character readers, scanners, and digital cameras.; Unstructured Data Manipulation: Handling and analyzing unstructured data from different platforms as part of investigative and analytical responsibilities."
vKkHH38qQ79zzWC2AAAAAA==,[],,"['Exploratory Data Analysis', 'Statistical Modeling', 'Machine Learning', 'Python', 'R', 'Pandas', 'SQL', 'scikit-learn', 'TensorFlow', 'PyTorch', 'Google Cloud Platform']","Exploratory Data Analysis: Used to analyze and summarize data from real-world sources to understand patterns and inform model development.; Statistical Modeling: Applied for developing predictive models as part of the model development process.; Machine Learning: Involved in model development and deployment, including working with teams to deploy models into production.; Python: Used as a major programming language for data science tasks including data manipulation and model development.; R: Used as a major programming language for data science tasks including data manipulation and model development.; Pandas: Utilized for data manipulation and analysis as part of the data processing workflow.; SQL: Used for querying and managing data from databases to support data analysis and model development.; scikit-learn: Used as a machine learning framework for model development.; TensorFlow: Used as a machine learning framework for model development.; PyTorch: Used as a machine learning framework for model development.; Google Cloud Platform: Used as a cloud computing platform to support data science workflows and potentially model deployment."
u17_iIJaFDOi1TxNAAAAAA==,[],,"['Data Mining', 'Data Collection', 'Data Analysis', 'Algorithm Design', 'Programming Languages', 'Data Solutions Development', 'Data Quality Management', 'Collaboration with Intelligence Teams']","Data Mining: Used to extract complex, voluminous, and diverse data from various sources and platforms to support analysis and reporting needs.; Data Collection: Identifying new sources and methods to gather data effectively to meet customer requirements.; Data Analysis: Analyzing collected data to compile actionable information that supports intelligence products and customer needs.; Algorithm Design: Designing algorithms and data manipulation capabilities using programming languages such as R, Python, C++, JavaScript, and Go to process and analyze data.; Programming Languages: Utilizing languages including R, Python, C++, JavaScript, and Go to develop data solutions and tools.; Data Solutions Development: Building tools and capabilities that enable self-service frameworks for data consumers to monitor and report on data.; Data Quality Management: Improving data usability by ensuring adherence to principles such as metadata management, data lineage, and business definitions.; Collaboration with Intelligence Teams: Working jointly with intelligence and data analysis teams to produce qualitative and quantitative data supporting intelligence products."
ujHBqekAy1MAOLjvAAAAAA==,"['Generative AI', 'Large Language Models', 'AI Model Deployment and Management']","Generative AI: The role includes designing and implementing AI solutions using the latest Generative AI technologies to build advanced AI-driven applications.; Large Language Models: Experience with foundation models and large language models (LLMs) is required for developing AI solutions and understanding basic LLM concepts.; AI Model Deployment and Management: Responsibilities include deploying AI models in production, managing AI model performance post-deployment, and supporting AI tool development and best practices.","['Data Pipelines', 'Machine Learning Models', 'Feature Engineering', 'Model Selection and Training', 'Supervised Learning', 'Unsupervised Learning', 'Reinforcement Learning', 'Python Programming', 'Cloud Platforms', 'Containerization Tools', 'ML Frameworks and Tools', 'MLOps Pipelines']","Data Pipelines: The role involves hands-on experience with foundational code for data pipelines and database migrations to support AI and machine learning projects.; Machine Learning Models: Responsibilities include designing, developing, deploying, and optimizing machine learning models to support high-impact projects.; Feature Engineering: The job requires conducting feature engineering as part of data preparation to ensure optimal model performance.; Model Selection and Training: The candidate will perform model selection, training, and optimization to achieve the best performance from AI models.; Supervised Learning: The role requires strong knowledge and application of supervised learning techniques in AI/ML development.; Unsupervised Learning: The candidate must have expertise in unsupervised learning methods as part of AI/ML techniques used in the role.; Reinforcement Learning: Experience with reinforcement learning is required as part of the AI/ML techniques applied in the job.; Python Programming: Expertise in Python and AI/ML key libraries is essential for developing and deploying AI and machine learning solutions.; Cloud Platforms: Experience with cloud platforms such as AWS, GCP, or Azure is necessary for deploying and managing machine learning models and data workflows.; Containerization Tools: The job involves using containerization tools like Docker and Kubernetes to support deployment and scalability of AI/ML applications.; ML Frameworks and Tools: Experience with machine learning frameworks and tools in the cloud, including Amazon SageMaker, is required for model development and deployment.; MLOps Pipelines: Developing automation scripts for MLOps pipelines using Infrastructure as Code (IaC) is part of the responsibilities to support ML model deployment and inferencing workflows."
5uF80EeGMYCqCMh2AAAAAA==,[],,"['Statistical Modeling', 'Credit Bureau Data Analysis', 'Python', 'Conda', 'AWS', 'H2O', 'Spark', 'Machine Learning Models', 'Clustering', 'Classification', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'Confusion Matrix and ROC Curve Interpretation', 'SQL', 'PySpark', 'Scala', 'R']","Statistical Modeling: Used to personalize credit card offers and generate insights from credit bureau data to support underwriting decisions.; Credit Bureau Data Analysis: Extracting value and generating insights from raw credit bureau data to shape underwriting processes and monitor production errors.; Python: Utilized as part of the technology stack to analyze large volumes of numeric and textual data and develop data science solutions.; Conda: Used as part of the technology stack to manage environments and dependencies for data science workflows.; AWS: Cloud computing platform leveraged to process and analyze large-scale data sets and support data science solutions.; H2O: Employed as a machine learning platform to build and deploy models on large datasets.; Spark: Used for big data processing and analytics, including working with PySpark for distributed data processing.; Machine Learning Models: Built through all phases including design, training, evaluation, validation, backtesting, and implementation to support business goals.; Clustering: Applied as a statistical method for grouping data points to uncover patterns within credit bureau data.; Classification: Used to categorize data points, likely for credit risk or underwriting decisions.; Sentiment Analysis: Performed as part of data analysis to interpret textual data.; Time Series Analysis: Used to analyze data points collected or recorded at specific time intervals, relevant for modeling and forecasting.; Deep Learning: Applied as part of advanced modeling techniques to improve predictive performance.; Confusion Matrix and ROC Curve Interpretation: Used to evaluate and validate classification models' performance.; SQL: Used to retrieve and combine data from various sources and structures for analysis.; PySpark: Experience with PySpark for distributed data processing and analytics on big data.; Scala: Used as a programming language for data processing and analytics.; R: Used for statistical analysis and data science tasks."
KfyGieO3W0E2DuJeAAAAAA==,[],,"['Data Mining', 'Statistical Analysis', 'Data Cleaning and Interpretation', 'Big Data Analytics', 'Open-Source Intelligence (OSINT)', 'Python Programming']","Data Mining: Used to extract and analyze data from multiple sources to identify trends, anomalies, and hidden opportunities relevant to military and intelligence operations.; Statistical Analysis: Applied mathematical and statistical expertise to identify statistical trends and anomalies in data to support operational decision-making.; Data Cleaning and Interpretation: Involves mining, interpreting, and cleaning data to ensure accuracy and usability for intelligence and planning products.; Big Data Analytics: Utilized various big data analytic methods to transform raw data into finished planning, intelligence, and assessment products.; Open-Source Intelligence (OSINT): Understanding and operational application of OSINT to support intelligence activities and decision-making.; Python Programming: Basic understanding of Python programming language to support data analysis tasks."
IkHCXIwwAbWdvnuvAAAAAA==,[],,"['Python', 'R', 'SQL', 'MATLAB', 'Statistical Modeling', 'Data Visualization', 'Time-Series Forecasting', 'Data Cleaning and Preprocessing', 'ETL (Extract, Transform, Load)', 'Pandas', 'NumPy', 'Excel', 'Multi-Intelligence Spatial Temporal Tool (MIST)', 'APIs', 'Geospatial Analysis', 'Machine Learning']","Python: Used as a primary programming language to analyze large structured and unstructured datasets, automate workflows, and develop scripts for efficient data handling.; R: Employed for statistical modeling, data visualization, and analyzing large datasets as part of data science tasks.; SQL: Utilized to query and manage large datasets, supporting data extraction and analysis workflows.; MATLAB: Applied for data analysis, statistical modeling, and processing large datasets within the analytical support role.; Statistical Modeling: Used to analyze data trends and patterns, supporting long-term trending assessments and forecasting.; Data Visualization: Creating visual representations of data to inform operational decisions and capability assessments.; Time-Series Forecasting: Applied techniques to predict trends over time, particularly for scenario-based campaign planning and execution.; Data Cleaning and Preprocessing: Collecting, cleaning, preprocessing, and consolidating unfiltered, unstructured datasets from various data feeds.; ETL (Extract, Transform, Load): Developing scripts and lightweight applications to streamline data extraction, transformation, and loading processes.; Pandas: Used as a data manipulation library to automate workflows and handle data efficiently.; NumPy: Employed for numerical data processing and automation of data workflows.; Excel: Leveraged to create custom analytics solutions and automate workflows.; Multi-Intelligence Spatial Temporal Tool (MIST): Integrated and automated workflows using this IC tool to support data analysis and operational decision-making.; APIs: Experience with Application Programming Interfaces to extract and process data programmatically.; Geospatial Analysis: Used to analyze spatial data relevant to Unmanned Aerial Systems (UAS) activity and campaign planning.; Machine Learning: Familiarity with AI/ML models to process and analyze large datasets, supporting analytic solution development."
B20-wXs7f5F0FLLRAAAAAA==,[],,"['Predictive Models', 'Data Extraction, Cleaning, and Transformation', 'Statistical and Machine Learning Techniques', 'Exploratory Data Analysis (EDA)', 'Entity Resolution', 'Unsupervised Machine Learning Methods', 'AutoML Tools and Platforms', 'Big Data Technologies', 'Programming Languages for Data Science', 'Machine Learning Model Lifecycle Management', 'Automated Text/Data Classification and Categorization']","Predictive Models: Used to develop analytics solutions that support law enforcement mission critical activities by predicting outcomes based on CBP transactional and associated data.; Data Extraction, Cleaning, and Transformation: Involves extracting, cleaning, and transforming CBP transactional and associated data sets within an identified problem space to prepare data for predictive modeling.; Statistical and Machine Learning Techniques: Applied to develop, evaluate, and deploy new predictive analytical models that inform mission decisions, including supervised and unsupervised learning methods.; Exploratory Data Analysis (EDA): Constructing and executing queries to extract data in support of exploratory data analysis and model development.; Entity Resolution: Techniques such as record linking, named entity matching, deduplication, and disambiguation used to identify and resolve entities within datasets.; Unsupervised Machine Learning Methods: Includes cluster analysis methods like K-means, K-nearest Neighbor, Hierarchical clustering, Deep Belief Networks, and Principal Component Analysis used for segmentation and pattern identification.; AutoML Tools and Platforms: Utilization of automated machine learning platforms such as AWS Sagemaker, DataRobot, and DataBricks to streamline model development and deployment.; Big Data Technologies: Experience with technologies like Hadoop, HIVE, HDFS, HBase, MapReduce, Spark, Kafka, and Sqoop to handle large-scale data processing and analytics.; Programming Languages for Data Science: Use of R, Python, Scala, Java, SQL, and Spark for data extraction, analysis, model development, and deployment.; Machine Learning Model Lifecycle Management: Full-lifecycle development, deployment, and monitoring of machine learning models across multiple platforms including on-premises and cloud environments.; Automated Text/Data Classification and Categorization: Execution of projects involving automated classification, categorization, entity recognition, resolution, and extraction to identify patterns and anomalies in large datasets."
GUK-1LrCOzXGi2q7AAAAAA==,['Deep Learning'],"Deep Learning: Research and development of deep learning models and algorithms, including determining requirements to train and evolve these models.","['Machine Learning', 'Statistical Methods', 'Predictive Modeling', 'Data Pipelines', 'SQL and NoSQL', 'R and RStudio', 'Python', 'SAS', 'Cloud Machine Learning Technologies', 'Machine Learning Frameworks', 'Data Analytics Tools and Techniques', 'Big Data Analysis Tools']","Machine Learning: Used to build predictive models and solve complex business problems by leveraging both structured and unstructured data sources.; Statistical Methods: Includes Bayesian Networks Inference, linear and non-linear regression, hierarchical and mixed/multi-level modeling applied to modeling complex business problems and extracting insights from data.; Predictive Modeling: Developing and deploying models to answer business questions and support decision-making processes.; Data Pipelines: Building scalable data pipelines to process large-scale data for analysis and model development.; SQL and NoSQL: Used for data sourcing and managing structured and unstructured data.; R and RStudio: Utilized for statistical analysis, data manipulation, and model development.; Python: Used for scripting, data analysis, and building machine learning models.; SAS: Applied for advanced statistical analysis and modeling.; Cloud Machine Learning Technologies: Experience with cloud platforms like AWS SageMaker to build, train, and deploy machine learning models.; Machine Learning Frameworks: Experience with TensorFlow, scikit-learn, and caret for developing and deploying machine learning models.; Data Analytics Tools and Techniques: Used to perform advanced analytics, segmentation, statistical inference, and visualization to extract value from business data.; Big Data Analysis Tools: Applied for handling and analyzing large datasets to support data-driven decision making."
PVL34lmAQd7hYR1QAAAAAA==,[],,"['Statistical Analysis', 'Operations Research Methods', 'Data Visualization with Power BI', 'Data Manipulation and Cleaning', 'Programming with R and Python', 'SQL (Structured Query Language)', 'Quality Control Methods', 'Pattern Analysis', 'Business Intelligence Tools']","Statistical Analysis: Used to perform complex data mining, trend analysis, and causal analysis to support operational decision-making in a federal government context.; Operations Research Methods: Applied to integrate multiple disciplines and translate analytical methods into actionable insights for operational managers.; Data Visualization with Power BI: Required high proficiency in Power BI to create reports and dashboards for monitoring and delivering monthly contract reports and analytics products.; Data Manipulation and Cleaning: Involves preparing structured and unstructured data into cohesive analytical products and ensuring data quality through cleaning and preparation techniques.; Programming with R and Python: Used for statistical programming and data analysis tasks to support complex analytics and problem-solving.; SQL (Structured Query Language): Utilized for querying and managing data within various government and commercial business process automation systems.; Quality Control Methods: Applied to review and ensure the accuracy and reliability of analytical products and reports.; Pattern Analysis: Used to identify and exploit patterns in past, current, and anticipated operational environments to formulate recommendations for operational managers.; Business Intelligence Tools: Experience with SAP Business Objects/Web Intelligence Reports and other BI tools to support data reporting and decision-making."
yI7QAjplPR6bzEDNAAAAAA==,[],,"['Data Analysis', 'Data Methodologies', 'Data Visualization', 'Quantitative and Qualitative Metrics', 'Scripting Languages', 'Databases', 'Microsoft Office Suite']","Data Analysis: Used to understand customer questions and needs by exploring data-rich environments to extract meaningful information that supports decision-making and capability acquisition.; Data Methodologies: Researching, developing, and testing various data methodologies to generate cross-functional solutions through collection and analysis of data sets.; Data Visualization: Applying different formats of data visualization to present findings and recommendations effectively to clients and stakeholders.; Quantitative and Qualitative Metrics: Establishing key performance indicators and metrics to drive technical outcomes and measure impact.; Scripting Languages: Utilizing scripting languages to support data analysis and presentation of data findings.; Databases: Using knowledge of databases to access, manage, and analyze data relevant to client needs.; Microsoft Office Suite: Employing Microsoft Office tools to present data findings and support communication with clients and stakeholders."
yoVv58rWQmBHm6H6AAAAAA==,[],,"['SQL', 'Data Visualization', 'Data Quality Assessment', 'Data Process Mapping', 'Business Analysis', 'Data Models and Relational Databases', 'Dashboards and Reporting', 'Change Management', 'Agile Methodologies']",SQL: Used for performing data analytics and creating reports and dashboards to communicate meaningful insights.; Data Visualization: Constructing reports and visualizations to support business processes and decision-making.; Data Quality Assessment: Conducting rigorous analysis of data quality and root cause analysis of data issues to improve data reliability and safety.; Data Process Mapping: Developing detailed data process maps and documentation for current and future state workflows and projects.; Business Analysis: Conducting structured business analysis and generating well-defined requirements to translate business needs into data solutions.; Data Models and Relational Databases: Practical experience with data models and relational databases to support data-driven decision making.; Dashboards and Reporting: Creating reports and dashboards to communicate insights and support business teams and leadership.; Change Management: Assisting with change management and product rollout plans including training material development and communications to stakeholders.; Agile Methodologies: Knowledge of digital product implementation fundamentals and Agile ways of working to support digital transformation initiatives.
l0WHl7tPrmnuo4TaAAAAAA==,[],,"['Data Quality Assessment', 'Data Augmentation', 'Data Migration Planning', 'Python', 'SQL', 'Data Validation', 'Data Documentation']","Data Quality Assessment: Used to evaluate the integrity, completeness, and readiness of data for advanced analytics and system integration by implementing data quality and completeness checks and identifying discrepancies, duplications, and anomalies through pre-validation checks.; Data Augmentation: Supporting data collection and augmentation efforts to prepare datasets for downstream analysis and reporting, enhancing the dataset for better analytics outcomes.; Data Migration Planning: Assisting in forecasting and planning data migration efforts, including estimating data readiness and transformation needs to ensure smooth data transfer and integration.; Python: Utilized as a primary programming language for data analysis tasks, including working with large and complex datasets.; SQL: Used for querying and managing large and complex datasets to support data analysis and reporting.; Data Validation: Performing thorough data validation and documentation to ensure data accuracy and reliability for analytics and reporting.; Data Documentation: Documenting known data integrity and quality issues and collaborating with relevant teams to support remediation planning."
6cU9Z12IdFtIzkGiAAAAAA==,['Artificial Intelligence (AI)'],"Artificial Intelligence (AI): Knowledge and experience with AI techniques, including machine learning and natural language processing, to develop intelligent systems and models that support client needs.","['Data Exploration and Cleaning', 'Statistical and General-Purpose Programming Languages', 'Predictive Data Modeling', 'Data Visualization', 'Text Mining and Natural Language Processing (NLP)', 'Distributed Data and Computing Tools', 'Algorithm Development']","Data Exploration and Cleaning: Experience with exploring, cleaning, and analyzing both structured and unstructured data sources to prepare data for further analysis and modeling.; Statistical and General-Purpose Programming Languages: Use of programming languages such as R, Python, and SQL/NoSQL for data analysis, algorithm development, and predictive modeling.; Predictive Data Modeling: Development of predictive models and quantitative analyses to extract insights and support decision-making based on targeted data sources.; Data Visualization: Creation of visual representations of data using tools like Plotly, Seaborn, ggplot2, and Palantir Foundry Envision to communicate findings effectively to clients.; Text Mining and Natural Language Processing (NLP): Application of text mining and NLP techniques to analyze unstructured text data as part of data science projects.; Distributed Data and Computing Tools: Experience with distributed computing frameworks and tools such as MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, and MySQL to handle large-scale data processing and analysis.; Algorithm Development: Design and implementation of algorithms leveraging programming languages and data science techniques to solve client problems and extract meaningful insights."
fT5VGjF4Q3RHr25lAAAAAA==,['Retrieval-Augmented Generation'],Retrieval-Augmented Generation: Develop machine learning models including building Retrieval-Augmented Generation (RAG) models for AWS Bedrock to enhance analytic capabilities.,"['Machine Learning', 'Multi-Cloud Environments', 'SQL and NoSQL Databases', 'Big Data Platforms', 'Data Visualization Tools', 'Programming Languages']","Machine Learning: Develop machine learning models for data analysis and automation; expertise in machine learning techniques and statistical analysis; strong understanding of AI/ML applications in systems and software; integrate AI/ML with applications and systems.; Multi-Cloud Environments: Design and implement data science frameworks across multi-cloud environments; cloud computing expertise across AWS, Azure, and other platforms.; SQL and NoSQL Databases: Support data webforms using SQL databases and processes for API connections; proficiency in SQL and NoSQL databases.; Big Data Platforms: Experience with big data platforms such as Hadoop, Spark, and Kafka; develop real-time data processing solutions and data solutions at scale.; Data Visualization Tools: Experience with data visualization tools such as Tableau and PowerBI to communicate findings and recommendations to stakeholders.; Programming Languages: Strong programming skills in Python, SCALA, and/or UNIX shell scripting used to develop data science solutions and automation."
lREO_Gvf-mqPGqOtAAAAAA==,[],,"['Data Normalization', 'Data Scripting', 'ETL (Extract, Transform, Load)', 'Data Integration', 'ServiceNow Discovery', 'Service Mapping', 'Configuration Management Database (CMDB)', 'Event Management', 'Data Dictionary and Metadata Management', 'Data Architecture Design', 'Data Validation and Integrity', 'Data Pipelines Monitoring', 'Data Modeling and Database Design', 'Scripting Languages', 'IT Infrastructure and Systems Management']","Data Normalization: Responsible for overseeing data normalization processes including building tables and moving data to ensure consistent and clean data structures.; Data Scripting: Writing data scripts to support data integration, transformation, and automation within enterprise environments.; ETL (Extract, Transform, Load): Hands-on experience with ETL processes and tools to extract, transform, and load data efficiently in enterprise settings.; Data Integration: Managing data integration tasks including installation, configuration, and ensuring seamless data flow across systems.; ServiceNow Discovery: Utilizing ServiceNow Discovery tools to identify and map IT infrastructure components and services.; Service Mapping: Implementing service mapping to visualize and manage dependencies between IT services and infrastructure.; Configuration Management Database (CMDB): Designing and maintaining the CMDB to support configuration management and health monitoring of IT assets.; Event Management: Using event management practices to monitor and respond to IT events and incidents effectively.; Data Dictionary and Metadata Management: Maintaining and updating the data dictionary and metadata to ensure data governance and compliance with security standards.; Data Architecture Design: Collaborating with platform teams to design scalable and efficient data architectures that support dashboarding and KPI reporting.; Data Validation and Integrity: Participating in data validation, disaster recovery (DR) and continuity of operations (COOP) testing, and ensuring data integrity across development, testing, and production environments.; Data Pipelines Monitoring: Monitoring data pipelines for errors or anomalies and proposing continuous improvements to enhance data reliability.; Data Modeling and Database Design: Proven experience in data modeling, normalization, and designing databases to support enterprise data needs.; Scripting Languages: Familiarity with scripting languages such as JavaScript, Python, and PowerShell to automate and support data-related tasks.; IT Infrastructure and Systems Management: Knowledge of IT infrastructure, systems management, and service dependency mapping to support data and service management."
jP9DTA35CxtBFLhCAAAAAA==,[],,"['Big Data Analytics', 'Statistical Modeling', 'Mathematical and Statistical Techniques', 'Data Visualization', 'Data Engineering and ETL', 'Relational Databases', 'Programming Languages for Data Science', 'Advanced Statistical Software', 'Geospatial Intelligence (GEOINT) Analysis', 'Government Off-The-Shelf (GOTS) Analytics Tools', 'Data Visualization Techniques', 'Analytic Tradecraft Tools']","Big Data Analytics: Develop methods for querying, visualizing, aggregating, correlating, and analyzing large datasets across intelligence disciplines to extract patterns and anticipatory behaviors.; Statistical Modeling: Write scripts in Visual Basic, R, Python, and other languages to build statistical models that are repeatable, efficient, support hypothesis testing, and enable knowledge capture from large datasets.; Mathematical and Statistical Techniques: Apply multidisciplinary mathematical and statistical models and concepts to solve complex GEOINT analysis problems and extract meaningful insights from structured data and relational databases.; Data Visualization: Use tools such as ArcGIS, Excel, SPSS, SAS, Matlab, and R to visualize operational data both temporally and spatially, assisting in data integrity checks and analytical assessments.; Data Engineering and ETL: Maintain, move, and manipulate data between applications using software and Extract-Transform-Load (ETL) procedures involving Microsoft Excel, Access, Oracle, PostgreSQL, and SQL Server, including importing and cleaning analyst-provided datasets.; Relational Databases: Understand and explain the relationship between real-world data collection and the required structure of relational databases to support analysis and visualization.; Programming Languages for Data Science: Use programming languages such as Visual Basic, R, Python, Java, JavaScript, and C++ to develop custom analytic tools, automate processes, and support modeling efforts.; Advanced Statistical Software: Utilize statistical software packages including SPSS, SAS, Matlab, and R for advanced statistical analysis of operational data.; Geospatial Intelligence (GEOINT) Analysis: Apply knowledge of intelligence operations and GEOINT phenomenology to analyze geospatial data and support mission objectives.; Government Off-The-Shelf (GOTS) Analytics Tools: Use specialized GOTS data and analytics capabilities such as MIST, INTELBOOK, LINX, and WATCHBOX to support intelligence analysis.; Data Visualization Techniques: Employ matrix analytics, network analytics, and graphing data techniques to enhance data visualization and interpretation.; Analytic Tradecraft Tools: Leverage ABI tools and tradecraft including MIST, SOM tools and SOM-C, GOWK, CEDALLION, ATLAS, and OBP tools to support analytic processes and intelligence workflows."
jC4mf5O0NpszvA_FAAAAAA==,[],,"['Machine Learning', 'Big Data Systems', 'Structured Query Language (SQL)', 'Python Statistical Programming', 'R Statistical Programming', 'Data Visualization', 'Microsoft Excel', 'Power BI', 'Tableau', 'Advana Data Science']","Machine Learning: The job requires skills and expertise in machine learning tools to select features, create, and optimize classifiers for production-level data models.; Big Data Systems: Experience working with big-data systems within the Department of Defense is required for developing production-level data models.; Structured Query Language (SQL): Practical experience with SQL is necessary to support data analytic efforts and database access.; Python Statistical Programming: Experience with Python for statistical programming is required to develop and execute data analytic efforts.; R Statistical Programming: Experience with R for statistical programming is required to develop and execute data analytic efforts.; Data Visualization: The role involves data visualization skills to support analytic efforts and presentation of data insights.; Microsoft Excel: Proficiency in Microsoft Excel is required to support data analysis and presentation tasks.; Power BI: Prior knowledge of Power BI is preferred for creating business intelligence dashboards and visualizations.; Tableau: Prior knowledge of Tableau is preferred for creating business intelligence dashboards and visualizations.; Advana Data Science: Familiarity with Advana data science platform is preferred to support analytic efforts within the Army National Guard."
q6-tTQUn2Sr0Hg9vAAAAAA==,[],,"['Machine Learning', 'Feature Selection', 'Classification Models', 'SQL', 'Python Statistical Programming', 'R Statistical Programming', 'Data Visualization', 'Power BI', 'Tableau', 'Big Data Systems', 'Data Models']","Machine Learning: The candidate is expected to have skills and expertise in machine learning tools to select features, create, and optimize classifiers for analytic efforts supporting the client.; Feature Selection: The role involves using machine learning tools specifically to select features as part of building and optimizing classifiers.; Classification Models: The job requires creating and optimizing classifiers, indicating experience with classification models in machine learning.; SQL: The candidate should demonstrate practical experience with Structured Query Language for data querying and manipulation.; Python Statistical Programming: Experience with Python for statistical programming is required to support data analysis and model development.; R Statistical Programming: Experience with R for statistical programming is required to support data analysis and model development.; Data Visualization: The candidate should have skills in data visualization, including practical experience with tools like Power BI and Tableau, to present analytic results effectively.; Power BI: Preferred working knowledge of Power BI is mentioned for data visualization and presentation of analytic findings.; Tableau: Preferred working knowledge of Tableau is mentioned for data visualization and presentation of analytic findings.; Big Data Systems: The candidate must have experience working with big-data systems and developing production-level data models, indicating handling large-scale data environments.; Data Models: Developing production-level data models is a key responsibility, supporting analytic efforts and data-driven decision making."
dLAcy4VGzii_2vfPAAAAAA==,"['AI Platforms', 'AI-Based Applications', 'AI Agent Frameworks']","AI Platforms: The job involves developing AI-based applications using the customer's AI platform for operational cloud and secure lab deployments.; AI-Based Applications: Responsibilities include developing, testing, troubleshooting, and enhancing AI-based applications based on feedback.; AI Agent Frameworks: Familiarity with AI agent frameworks is preferred, indicating involvement with autonomous or intelligent agent systems within AI applications.","['Time Series Models', 'MLOps', 'Machine Learning Pipelines', 'Python', 'Numpy and Pandas', 'Jupyter Notebook', 'Statistical Data Analysis', 'Open-Source Machine Learning Frameworks']","Time Series Models: The role involves training on time series data and developing models that analyze temporal data patterns.; MLOps: Responsibilities include managing machine learning pipelines, training, validating, deploying models, and optimizing application performance.; Machine Learning Pipelines: The job requires training, validating, and deploying machine learning pipelines to develop candidate models that meet performance thresholds.; Python: The candidate must have 5+ years of experience in data science development using Python, including proficiency with numpy and pandas libraries.; Numpy and Pandas: Strong proficiency in numpy and pandas is required for data manipulation and analysis tasks.; Jupyter Notebook: Experience with Jupyter Notebook or comparable environments is necessary for developing and documenting data science workflows.; Statistical Data Analysis: The role requires proficiency in statistical data analysis, model evaluation, and feature evaluation to interpret and validate data insights.; Open-Source Machine Learning Frameworks: Experience with frameworks such as PyTorch, TensorFlow, and scikit-learn is preferred for model training and analysis."
vmRyubn28ldCTWSXAAAAAA==,"['Generative AI', 'AI-Enhanced Decision Systems', 'AI Engineering']",Generative AI: Leading or contributing to teams developing and implementing generative AI solutions to address challenging problems.; AI-Enhanced Decision Systems: Proposing and executing novel research in AI-enhanced decision support systems to improve mission outcomes.; AI Engineering: Engaging in AI engineering practices as part of the multidisciplinary data science team to develop AI-driven solutions.,"['Exploratory Data Analysis', 'Machine-learned Predictive Modeling', 'Anomaly Detection', 'Mathematical and Statistical Methods', 'ML Pipelines and Workflows', 'Data Architecture and Cloud Engineering', 'Continuous Integration and Delivery for ML', 'Data Integration and ETL (Extract, Transform, Load)', 'Analytic Modeling and Programming', 'Data Visualization', 'Advanced Mathematics and Computer Science', 'Data Mining and Advanced Analytical Algorithms', 'Data Science Lifecycle Collaboration', 'High-Performance Computing (HPC)']","Exploratory Data Analysis: Used to reveal data features of interest through initial investigation and model-fitting.; Machine-learned Predictive Modeling: Developing predictive models using machine learning techniques to solve complex data problems.; Anomaly Detection: Identifying and analyzing anomalous data, including metadata, to detect irregularities.; Mathematical and Statistical Methods: Applying mathematical science, statistics, and quantitative analysis to analyze data and develop solutions.; ML Pipelines and Workflows: Implementing machine learning pipelines and workflows to operationalize models and data processes.; Data Architecture and Cloud Engineering: Leveraging modern data architecture and cloud platforms such as AWS and Azure for data transformation and management of structured and unstructured data.; Continuous Integration and Delivery for ML: Building CI/CD pipelines specifically for machine learning applications to enable automated deployment and updates.; Data Integration and ETL (Extract, Transform, Load): Constructing usable datasets from multiple sources and developing ELT functions to meet customer needs.; Analytic Modeling and Programming: Performing analytic modeling, scripting, and programming to develop data-driven solutions.; Data Visualization: Creating interpretable visualizations to communicate data insights effectively to stakeholders.; Advanced Mathematics and Computer Science: Utilizing advanced coursework in calculus, differential equations, linear algebra, algorithms, programming, data structures, and data mining as foundational knowledge for data science tasks.; Data Mining and Advanced Analytical Algorithms: Designing and implementing data mining techniques and advanced analytical algorithms to extract insights from data.; Data Science Lifecycle Collaboration: Working collaboratively and iteratively throughout the data science lifecycle with cross-functional teams including data scientists, data engineers, and ML-Ops engineers.; High-Performance Computing (HPC): Utilizing HPC infrastructure to support compute-intensive data science and machine learning workloads."
3BV0mxleNgf3IRb0AAAAAA==,"['AI Platforms', 'AI Agent Frameworks', 'Deep Learning Frameworks']","AI Platforms: Developing AI-based applications using a customer-specific AI platform for operational deployment in cloud and secure lab environments.; AI Agent Frameworks: Familiarity with frameworks that support AI agents, indicating involvement with AI-native systems or architectures beyond traditional machine learning.; Deep Learning Frameworks: Experience with open-source deep learning frameworks such as PyTorch and TensorFlow for model training and analysis.","['MLOps', 'Time Series Modeling', 'Machine Learning Model Training', 'Python Data Science Libraries', 'Jupyter Notebook', 'Statistical Data Analysis', 'Machine Learning Pipelines', 'Model Performance Optimization']","MLOps: Managing machine learning pipelines including training, validation, deployment, and operationalization of models on cloud and secure lab environments.; Time Series Modeling: Training and engineering models specifically on time series data as part of the machine learning workflow.; Machine Learning Model Training: Developing candidate models, training them, evaluating performance, and promoting models to active status when performance thresholds are met.; Python Data Science Libraries: Using Python libraries such as numpy and pandas for data manipulation and analysis in data science development.; Jupyter Notebook: Utilizing Jupyter Notebook or comparable interactive environments for developing and documenting data science workflows.; Statistical Data Analysis: Applying statistical methods for data analysis, model evaluation, and feature evaluation within machine learning projects.; Machine Learning Pipelines: Building, training, validating, deploying, and troubleshooting end-to-end machine learning pipelines.; Model Performance Optimization: Analyzing model performance metrics and implementing optimizations to improve application performance and model accuracy."
hjhpBIewfePGk7gFAAAAAA==,[],,"['Causal Inference Methods', 'Python', 'R', 'SQL', 'Machine Learning', 'Data Integration', 'Statistical Modeling', 'AWS Cloud Environment', 'H2O', 'Spark', 'Conda']","Causal Inference Methods: Used to develop and validate models assessing the social impact of community initiatives and public programs, including techniques such as randomized controlled trials (RCTs), regression discontinuity, propensity score matching, and difference-in-differences.; Python: Utilized as a primary programming language for collecting, cleaning, and analyzing large-scale datasets from diverse sources in cloud environments.; R: Used alongside Python for data analysis, statistical modeling, and working with large-scale datasets in cloud environments.; SQL: Applied for querying and managing relational databases to retrieve and combine structured data for analysis.; Machine Learning: Employed to develop predictive models and analyze data, with experience required for applying machine learning techniques to real-world problems.; Data Integration: Involves sourcing and combining alternative datasets such as government reports, survey data, and financial transaction data to enrich analysis and solve data challenges creatively.; Statistical Modeling: Used to interpret results from causal inference methods and generate actionable insights for decision-making.; AWS Cloud Environment: Provides the cloud infrastructure to handle large-scale data storage, processing, and analysis tasks.; H2O: Part of the technology stack leveraged for scalable machine learning and data analysis.; Spark: Used for big data processing and analytics to handle large datasets efficiently.; Conda: Utilized as an environment and package management system to support data science workflows in Python and R."
jnQ35lpZQpqxX-EJAAAAAA==,"['Deep Learning', 'Natural Language Processing', 'Computer Vision', 'Reinforcement Learning']","Deep Learning: Experience with deep learning techniques is desirable, indicating the use of neural networks for complex data modeling tasks.; Natural Language Processing: Knowledge of natural language processing is preferred, suggesting involvement with text data and language understanding models.; Computer Vision: Experience with computer vision is a plus, implying work with image or video data using AI techniques.; Reinforcement Learning: Familiarity with reinforcement learning is mentioned as a nice-to-have skill, indicating potential work with AI models that learn via interaction with environments.","['Machine Learning', 'Python Programming', 'Mathematics for Data Science', 'Scalable Machine Learning', 'Data Analytics Capabilities', 'Cloud Computing for Data Applications']","Machine Learning: The job involves researching, designing, implementing, and deploying machine learning algorithms for enterprise applications, including regression and classification models, supervised and unsupervised learning methods.; Python Programming: Excellent programming skills in Python are required to develop and deploy machine learning and data analytics solutions.; Mathematics for Data Science: A strong mathematical background in linear algebra, calculus, probability, and statistics is essential to support the development and understanding of machine learning models.; Scalable Machine Learning: Experience with scalable machine learning techniques such as MapReduce and streaming is needed to handle large-scale data processing and model deployment.; Data Analytics Capabilities: The role includes defining new analytics capabilities to provide federal customers with actionable information for decision-making and digital transformation.; Cloud Computing for Data Applications: Hands-on experience deploying and operating applications using Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) on major cloud providers like Amazon AWS, Microsoft Azure, or Google Cloud Services is considered a valuable skill."
KoxF5LmD7llmq8RcAAAAAA==,"['Large Language Models', 'Natural Language Processing', 'Generative AI Frameworks', 'Computer Vision Models', 'Audio Processing Models', 'CUDA and GPU-Accelerated AI', 'Kubernetes for AI Deployment']",Large Language Models: Experience deploying and fine-tuning large language models (LLMs) and applying NLP techniques in production environments.; Natural Language Processing: Use of NLP techniques as part of machine learning tasks including audio extraction and classification.; Generative AI Frameworks: Familiarity with OpenAI technologies and frameworks related to generative AI.; Computer Vision Models: Experience with object recognition models such as YOLO for image and video analysis.; Audio Processing Models: Use of audio processing models such as Whisper for extracting and analyzing audio data.; CUDA and GPU-Accelerated AI: Utilize CUDA for GPU-accelerated processing to optimize machine learning and AI model performance.; Kubernetes for AI Deployment: Use Kubernetes for container orchestration to deploy and manage AI and machine learning models in production.,"['Machine Learning Models', 'Data Extraction, Transformation, and Load (ETL)', 'Data Mapping', 'Data Analytics', 'Relational and NoSQL Databases', 'GPU Processing', 'Data Science Frameworks', 'Data Modeling and Data Quality', 'Streaming Data Analytics', 'Source Code Management and CI/CD', 'Distributed Computing and Cloud Infrastructure', 'Data Operations and Maintenance']","Machine Learning Models: Deploy, fine-tune, monitor, and optimize production machine learning models for tasks such as audio extraction, object recognition, NLP, and classification, including applying machine learning methodologies to build high-quality prediction models.; Data Extraction, Transformation, and Load (ETL): Provide support in data extraction, transformation, and load processes to maintain and operate data and associated systems.; Data Mapping: Experience with data mapping to support data quality, testing, and documentation preparation in a dynamic, process-improvement environment.; Data Analytics: Translate data insights into tools or analytic capabilities that inform operational decisions and improve processes, including providing near-real time analytics from streaming data to augment decision making.; Relational and NoSQL Databases: Demonstrated experience with relational databases such as SQL and Oracle, and NoSQL databases including Elasticsearch, Neo4J, and Redis.; GPU Processing: Optimize machine learning services to better utilize GPU capabilities and assist with planning future GPU requirements.; Data Science Frameworks: Familiarity with data science frameworks such as Keras, TensorFlow, and Theano for building and deploying models.; Data Modeling and Data Quality: Apply principles of data science including data modeling, data testing, and ensuring data quality in production environments.; Streaming Data Analytics: Deploy machine learning models against streaming data to provide near-real time analytics for decision support.; Source Code Management and CI/CD: Familiarity with source code management and integration tools such as GitHub/GitLab, Jenkins, and RunDeck, and continuous integration/continuous deployment (CI/CD) pipelines.; Distributed Computing and Cloud Infrastructure: Experience with server operating systems (Windows, Linux), distributed computing environments, blade centers, and cloud infrastructure.; Data Operations and Maintenance: Support operations, maintenance, and management of data and associated systems in a production environment."
qysGugpRKcn6GQU8AAAAAA==,[],,"['Machine Learning', 'Statistical Analysis', 'Data Modeling', 'Data Preprocessing', 'SQL', 'Python', 'R', 'Data Visualization', 'Power BI', 'Tableau', 'Big Data Systems']","Machine Learning: Used for predictive modeling in threat assessment and risk analysis, including feature selection, classifier creation, and optimization.; Statistical Analysis: Conducted to assess combat effectiveness, force readiness, and mission performance.; Data Modeling: Developed production-level data models to analyze military operational data and support strategic decision-making.; Data Preprocessing: Involves preprocessing large datasets related to Army operations, logistics, personnel, and intelligence for analysis.; SQL: Used as a practical skill for data querying and manipulation.; Python: Utilized for statistical programming and data analysis.; R: Used for statistical programming and data analysis.; Data Visualization: Developing dashboards and reports to visualize key performance indicators and metrics for leadership.; Power BI: Preferred tool for creating data visualizations and dashboards.; Tableau: Preferred tool for creating data visualizations and dashboards.; Big Data Systems: Experience working with large-scale data systems within the Department of Defense context."
xeT0sK51fE3pdAz4AAAAAA==,[],,"['Python', 'R', 'SQL', 'Statistical Modeling', 'Machine Learning', 'Data Visualization Tools', 'Transit Data Standards']","Python: Used for data analysis, statistical modeling, and machine learning tasks relevant to the transit and transportation domain.; R: Applied for data analysis, statistical modeling, and machine learning in the context of transit and transportation data.; SQL: Utilized for database querying, data manipulation, and extraction of transit-related data.; Statistical Modeling: Employed to analyze transit and transportation data for insights and predictive purposes.; Machine Learning: Used to develop predictive models and analyze transit data within the public sector environment.; Data Visualization Tools: Experience with Power BI or equivalent tools to create visual representations of transit and transportation data.; Transit Data Standards: Familiarity with GTFS, AVL/CAD, APC (Automated Passenger Counters), and AVA systems to handle and interpret transit-specific datasets."
4WuacWaNPYl9yRqIAAAAAA==,['Deep Learning'],Deep Learning: Use deep learning frameworks like PyTorch and TensorFlow specifically to build and optimize neural network-based computer vision models for customers.,"['Machine Learning', 'Computer Vision', 'Deep Learning Frameworks', 'Large-scale Data Handling', 'Data Querying and Scripting Languages', 'Statistical Modeling and Machine Learning Techniques', 'Model Experimentation and Algorithm Research', 'MLOps', 'Cloud-native Machine Learning Solutions', 'Infrastructure as Code', 'Model Containerization and Deployment', 'Object Detection']","Machine Learning: Deliver end-to-end machine learning projects including data aggregation, exploration, predictive model building, validation, and deployment on AWS Cloud to create business impact for customers.; Computer Vision: Develop and deploy computer vision models applied to various imagery and sensor types such as satellite imagery, medical imaging, aerial video, synthetic aperture radar, and X-Ray to solve real-world challenges.; Deep Learning Frameworks: Use PyTorch and TensorFlow frameworks to build and optimize computer vision models for customers.; Large-scale Data Handling: Work with terabyte-scale datasets to create scalable, robust, and accurate computer vision systems across diverse application fields.; Data Querying and Scripting Languages: Utilize data querying languages like SQL and scripting languages such as Python, as well as statistical/mathematical software like R, SAS, or Matlab for data analysis and modeling.; Statistical Modeling and Machine Learning Techniques: Apply machine learning and statistical modeling tools and techniques, including understanding parameters that affect model performance, to develop predictive models.; Model Experimentation and Algorithm Research: Lead planning, designing, and running experiments and researching new algorithms to improve model performance and solve customer problems.; MLOps: Assist customers with machine learning operations workflows including model deployment, retraining, testing, and performance monitoring to operationalize ML capabilities.; Cloud-native Machine Learning Solutions: Architect and implement secure, robust, and easy-to-deploy machine learning solutions on AWS Cloud, collaborating with cloud architects and engineering teams.; Infrastructure as Code: Manage multiple AWS and machine learning environments using infrastructure as code tools such as CloudFormation, Cloud Development Kit, Terraform, and Pulumi.; Model Containerization and Deployment: Containerize and deploy computer vision models, specifically neural networks, into production environments to ensure scalability and reliability.; Object Detection: Hands-on experience with state-of-the-art object detection approaches applied within computer vision projects."
sddKJVxz0FQ33rHEAAAAAA==,['Deep Learning'],"Deep Learning: Using deep learning frameworks like TensorFlow and PyTorch to develop models specifically applied to geospatial data, including architectures designed for remote sensing.","['Geospatial Data Analysis', 'Remote Sensing Data', 'Machine Learning', 'Computer Vision', 'Data Preparation and Validation', 'Data Integration and Harmonization', 'Geospatial Metadata Standards', 'Programming with Geospatial Libraries', 'Data Visualization', 'Containerization and Workflow Automation', 'Time-Series Analysis', 'Deep Learning for Remote Sensing', 'Cloud-Based Geospatial Computing']","Geospatial Data Analysis: Analyzing and processing complex geospatial datasets from multiple sensing modalities such as Electro-Optical (EO), Infrared (IR), and Synthetic Aperture Radar (SAR) to support machine learning applications.; Remote Sensing Data: Working with raster-level geospatial data and understanding collection metadata and sensing phenomenologies to evaluate dataset quality, coverage, and suitability for ML training and deployment.; Machine Learning: Applying machine learning techniques to geospatial data, including developing data preparation strategies and evaluating datasets for ML applications.; Computer Vision: Using computer vision techniques specifically applied to satellite and aerial imagery to extract meaningful features for analysis and ML.; Data Preparation and Validation: Designing and implementing innovative data preparation strategies and validation methods to ensure geospatial data quality and suitability for machine learning.; Data Integration and Harmonization: Developing automated processes for integrating and harmonizing geospatial data from multiple sources.; Geospatial Metadata Standards: Proficiency with geospatial metadata standards to ensure accurate analysis and documentation of data characteristics, limitations, and biases.; Programming with Geospatial Libraries: Strong programming skills in Python or R using geospatial libraries such as GDAL and GeoPandas to manipulate and analyze geospatial data.; Data Visualization: Visualizing geospatial data effectively for both technical and non-technical audiences to communicate insights and findings.; Containerization and Workflow Automation: Experience with containerization technologies like Docker and workflow automation to streamline geospatial data pipelines.; Time-Series Analysis: Analyzing time-series satellite imagery data to extract temporal patterns and trends relevant to geospatial applications.; Deep Learning for Remote Sensing: Applying deep learning architectures designed for remote sensing data to enhance analysis and predictive modeling.; Cloud-Based Geospatial Computing: Utilizing cloud platforms such as AWS, Google Earth Engine, and Microsoft Planetary Computer for scalable geospatial data processing and analysis."
sAtX1osX8LlDyT1_AAAAAA==,[],,"['Python', 'Jupyter Notebooks', 'Mathematics', 'Statistics', 'Data Management', 'Data Visualization', 'Data Modeling and Assessment', 'Machine Learning', 'Advanced Analytical Algorithms', 'Programming', 'Data Mining', 'Exploratory Data Analysis (EDA)', 'Statistical Inference']","Python: Used to automate workflows, manipulate data, and create visualizations to support data-driven decision-making.; Jupyter Notebooks: Utilized as an environment for data manipulation, workflow automation, and visualization development.; Mathematics: Foundational knowledge applied to extract meaning and value from large datasets and support analytic modeling and statistical analysis.; Statistics: Used for statistical analysis including variability, sampling error, inference, hypothesis testing, exploratory data analysis, and application of linear models to characterize and assess data.; Data Management: Involves data curation, cleaning, transformation, and handling datasets in various states of organization and cleanliness, particularly within DOD data holdings.; Data Visualization: Creating insightful visualizations to communicate complex data-driven conclusions effectively to both technical and non-technical audiences.; Data Modeling and Assessment: Developing and implementing qualitative and quantitative methods for modeling, inference, and prediction tailored to domain-specific considerations.; Machine Learning: Designing and implementing machine learning algorithms and advanced analytical methods to analyze and extract insights from data.; Advanced Analytical Algorithms: Applying sophisticated algorithms for data analysis, modeling, and prediction within the context of large and complex datasets.; Programming: Skill in at least one high-level programming language (e.g., Python) to support data science tasks including automation, analysis, and modeling.; Data Mining: Extracting patterns and knowledge from large datasets to support mission-critical analytic needs.; Exploratory Data Analysis (EDA): Performing initial investigations on data to discover patterns, spot anomalies, and test hypotheses.; Statistical Inference: Drawing principled conclusions from data using statistical methods to support decision-making."
NHWgeUB0JB527teXAAAAAA==,[],,"['Python', 'R', 'Scala', 'Tableau', 'Microsoft Power BI', 'Data manipulation and analysis', 'Enterprise cloud and data architecture', 'Data strategy development', 'Data integration', 'Data compliance and security', 'Data integrity and quality assurance']","Python: Used as a data science tool and programming language for data manipulation and analysis in various data science environments.; R: Used as a data science tool and programming language for data manipulation and analysis in various data science environments.; Scala: Used as a data science tool and programming language for data manipulation and analysis in various data science environments.; Tableau: Employed as a data visualization tool to deliver data analysis solutions and visualizations.; Microsoft Power BI: Used as a data visualization tool to deliver data analysis solutions and visualizations.; Data manipulation and analysis: Core proficiency required to analyze and work with data effectively in support of the Intelligence Community program.; Enterprise cloud and data architecture: Involves contributing to the design and evolution of the client's enterprise cloud and data architecture to ensure interoperability, security, and alignment with business goals.; Data strategy development: Developing data strategies that consider long-term implications, anticipate future needs and trends, and decompose complex problems into analytic approaches.; Data integration: Collaborating with multiple stakeholders to drive data integration initiatives and foster a data-driven culture.; Data compliance and security: Analyzing current practices to ensure compliance with relevant regulations and enhancing data security within the customer's data ecosystem.; Data integrity and quality assurance: Analyzing current data structure, tagging, and quality standards to discover and address anomalies, ensuring data integrity."
-_klp8UWVhrJmWAwAAAAAA==,[],,"['Exploratory Data Analysis', 'Data Cleaning/Wrangling', 'Statistical Analysis', 'Data Visualization', 'Machine Learning Models', 'Python Programming', 'Databases and Data Sources', 'Agile Development']","Exploratory Data Analysis: Used to analyze and summarize data sets to identify patterns and insights relevant to mission operations and intelligence lifecycle.; Data Cleaning/Wrangling: Applied using Python libraries to prepare raw data for analysis by handling inconsistencies and formatting issues.; Statistical Analysis: Performed to evaluate data trends and support intelligence assessments and model evaluation.; Data Visualization: Established advanced methodologies and designed infographics across media platforms to communicate analytic results and intelligence outcomes effectively.; Machine Learning Models: Developed with attention to model accuracy to support analytic products tailored to mission user requirements.; Python Programming: Used extensively for data science methods including data cleaning, exploratory data analysis, statistical analysis, and visualization.; Databases and Data Sources: Leveraged knowledge of databases and methodologies to identify appropriate data sources, determine indicators and relationships, and generate intelligence support packages.; Agile Development: Utilized agile methodologies and related tools such as Jira to support iterative development and delivery of analytic products."
9-w0Jz3GtdfwWvO4AAAAAA==,[],,"['Python', 'Jupyter', 'Machine Learning', 'scikit-learn', 'NumPy', 'PyTorch', 'TensorFlow', 'Network Data Analysis', 'Big Data Pipelines', 'Predictive Modeling', 'Statistical Analysis', 'Natural Language Processing', 'Data Cleansing and Integration', 'Agile Software Development', 'Spark', 'MLflow', 'Docker', 'AWS', 'Linux/CentOS', 'Bash Scripting', 'Git/GitHub']","Python: Used as a primary programming language for data analysis, model development, and building data pipelines.; Jupyter: Utilized as an interactive environment for developing, documenting, and running data science experiments and models.; Machine Learning: Applied for model development, evaluation, optimization, pattern identification, predictive modeling, and statistical analysis across diverse data sources.; scikit-learn: Used as a tool for developing, evaluating, and optimizing machine learning models.; NumPy: Employed for numerical computing and data manipulation as part of the machine learning and data analysis workflow.; PyTorch: Used for machine learning model development and evaluation, including neural network implementations.; TensorFlow: Utilized for machine learning model development and evaluation, including neural network implementations.; Network Data Analysis: Involves analyzing IP traffic data using tools like Bro/Zeek and TShark/PyShark to extract insights from network data.; Big Data Pipelines: Designing and building scalable data pipelines to consolidate and analyze structured and unstructured big data sources.; Predictive Modeling: Developing models to forecast outcomes and identify patterns within diverse datasets.; Statistical Analysis: Applying statistical methods including hypothesis testing to analyze data and validate models.; Natural Language Processing: Evaluating capabilities and analysis techniques related to processing and understanding text data.; Data Cleansing and Integration: Automating processes to clean, integrate, and prepare datasets for analysis and modeling.; Agile Software Development: Working within agile teams using Scrum methodologies to develop data science solutions iteratively.; Spark: Used for distributed data processing and building scalable data pipelines.; MLflow: Employed for managing the machine learning lifecycle including experiment tracking and model deployment.; Docker: Utilized to containerize applications and machine learning models for consistent deployment.; AWS: Used as a cloud platform to host data pipelines, storage, and machine learning workloads.; Linux/CentOS: Operating system environment for development, deployment, and running data science tools and pipelines.; Bash Scripting: Used to automate workflows and manage data processing tasks in the development environment.; Git/GitHub: Version control tools used to manage codebase and collaborate on software development lifecycle."
YdYVExrltznEb7kIAAAAAA==,"['Large Language Models', 'Generative AI', 'PyTorch', 'Hugging Face', 'LangChain', 'Lightning', 'Vector Databases', 'Self-Supervised Learning', 'Reinforcement Learning from Human Feedback', 'Explainability']","Large Language Models: The job requires hands-on experience working with LLMs, including adapting, fine-tuning, and operationalizing them for customer-facing AI-powered products.; Generative AI: The role involves experimenting, innovating, and creating next-generation experiences powered by the latest emerging generative AI technologies.; PyTorch: PyTorch is used as part of the technology stack for building and training deep learning models, including language models.; Hugging Face: Hugging Face tools and libraries are leveraged to work with transformer models and facilitate NLP and generative AI development.; LangChain: LangChain is used to build AI-powered applications, particularly those involving language models and chaining multiple AI components.; Lightning: Lightning is used to streamline and scale deep learning model training and deployment.; Vector Databases: Vector databases are utilized to manage and search large volumes of numeric and textual data, supporting AI applications such as similarity search and embeddings.; Self-Supervised Learning: Expertise in self-supervised learning is required as a key subdomain for training language or vision models.; Reinforcement Learning from Human Feedback: Experience with RLHF is needed for improving model behavior and alignment in AI systems.; Explainability: Knowledge of explainability techniques is important for interpreting and validating AI and machine learning models.","['Machine Learning', 'Natural Language Processing', 'Python', 'Scala', 'R', 'SQL', 'AWS']","Machine Learning: The role involves building machine learning models through all phases of development, including design, training, evaluation, and validation, and operationalizing them in scalable production systems serving millions of customers.; Natural Language Processing: Expertise in NLP is required to harness Large Language Models, adapt and fine-tune them for customer-facing applications and features.; Python: Experience with Python is required for developing machine learning and data science solutions.; Scala: Experience with Scala is required for developing machine learning and data science solutions.; R: Experience with R is required for developing machine learning and data science solutions.; SQL: Experience with SQL is required for data analytics and managing relational databases.; AWS: Experience working with AWS cloud computing platform is required, including leveraging AWS Ultraclusters for scalable computing."
bOjFjfYsAwA58iRxAAAAAA==,[],,"['Python', 'R', 'SQL', 'Machine Learning', 'Data Mining', 'Cluster Analysis', 'Statistical Models', 'Data Visualization', 'Data Integration', 'Exploratory Data Analysis', 'Data Pipelines']","Python: Used as a primary programming language for data science tasks including scripting, data integration, and model development.; R: Used as a primary programming language for statistical analysis and data science tasks.; SQL: Used for querying and managing data within large data environments.; Machine Learning: Applied to train predictive models and power data-driven products and tools.; Data Mining: Used to extract patterns and insights from large datasets to support analytics solutions.; Cluster Analysis: Applied as a statistical technique to group data points for analysis and insight generation.; Statistical Models: Designed and utilized to analyze data and support decision-making processes.; Data Visualization: Developed graphical analyses to interpret data and communicate insights effectively.; Data Integration: Involves scripting and techniques to consolidate and structure data for analytical use.; Exploratory Data Analysis: Conducted to discover patterns and inform the development of predictive models.; Data Pipelines: Designed and developed to process massive data streams and facilitate analysis."
W2DDfF5JErG6iCPKAAAAAA==,[],,"['Predictive Analytics', 'Exploratory Data Analysis', 'Statistical Methods', 'A/B Testing', 'SQL', 'Data Visualization Tools', 'Business Intelligence', 'Data Modeling', 'Machine Learning', 'Programming Languages for Data Science', 'Agile and SCRUM Methodologies']","Predictive Analytics: Used to extract insights from complex datasets and drive data-driven decision-making within the client's user community.; Exploratory Data Analysis: Conducted to gain insights and formulate hypotheses from large, complex datasets.; Statistical Methods: Utilized to analyze data and communicate findings effectively through data visualization techniques.; A/B Testing: Implemented frameworks to evaluate the impact of changes and interventions, analyze experimental results, and provide recommendations for optimization.; SQL: Used to create queries and understand relational database structures for data extraction and analysis.; Data Visualization Tools: Includes Microsoft Power BI, SSRS, Tableau, and AEON Business Process Management Platform used to produce reports and dashboards for various stakeholders.; Business Intelligence: Involves analysis, design, configuration, testing, and maintenance of BI modules to ensure operational performance and capabilities.; Data Modeling: Defining and documenting customer business processes, report/dashboard content needs, business process diagrams, and data maps.; Machine Learning: Developing and deploying predictive models using machine learning algorithms, optimizing model performance, and evaluating model accuracy.; Programming Languages for Data Science: Proficiency in SQL, Python, and R used for data analysis, model development, and querying.; Agile and SCRUM Methodologies: Applied in the software development lifecycle to manage projects and collaborate effectively in a team-oriented environment."
yUD93sAbedo_NTVWAAAAAA==,"['TensorFlow', 'Natural Language Processing (NLP)']","TensorFlow: Used specifically as a deep learning framework to build and deploy neural network models supporting analytic tasks in intelligence workflows.; Natural Language Processing (NLP): Applied in the context of AI to process and analyze textual data for intelligence purposes, enhancing information extraction and understanding.","['Automated Collection Models', 'Dynamic Analytic Models', 'Workflow Automations', 'Visual Programming', 'Geospatial Software Applications', 'Python', 'SQL', 'Git', 'AWS Sagemaker', 'AWS Cloud', 'Statistics (Descriptive, Bayesian)', 'Markov-Chain Modelling', 'TensorFlow', 'Linear Algebra', 'R', 'SAS', 'Natural Language Processing (NLP)']","Automated Collection Models: Used to support NSG strategies by creating automated models for data collection and analysis to streamline intelligence workflows.; Dynamic Analytic Models: Developed to enhance operational performance and support intelligence analysis through adaptable and evolving data models.; Workflow Automations: Implemented to automate repetitive analysis tasks and improve efficiency in data processing and intelligence operations.; Visual Programming: Applied to support and streamline analysis tasks using tools such as JEMA, FADE/MIST, and ECO/ETAS for geospatial and intelligence data processing.; Geospatial Software Applications: Utilized tools like ESRI ArcGIS and GIMS to analyze and visualize geospatial intelligence data relevant to operational needs.; Python: Used for scripting, data processing, and building analytic models within the intelligence and geospatial context.; SQL: Employed for querying and managing structured data to support analytic modeling and data processing tasks.; Git: Used for version control and collaboration in developing data science and automation solutions.; AWS Sagemaker: Leveraged as a cloud-based platform to build, train, and deploy machine learning models supporting intelligence workflows.; AWS Cloud: Used to host and manage data science and analytic solutions in a scalable and secure cloud environment.; Statistics (Descriptive, Bayesian): Applied to analyze data distributions, inferential statistics, and probabilistic modeling to support analytic decision-making.; Markov-Chain Modelling: Used to model stochastic processes and predict sequences relevant to intelligence data analysis.; TensorFlow: Utilized as a framework for building and deploying machine learning models, including neural networks, to support analytic tasks.; Linear Algebra: Applied as a mathematical foundation for modeling and computations in data science and machine learning workflows.; R: Used for statistical analysis, data visualization, and building analytic models in support of intelligence operations.; SAS: Employed for advanced statistical analysis and data management within analytic modeling tasks.; Natural Language Processing (NLP): Applied to analyze and extract insights from textual intelligence data to support information sharing and decision-making."
Z2Vx-hBADY9ZO7y6AAAAAA==,[],,"['Statistical Modeling', 'Relational Databases', 'Machine Learning', 'Model Validation and Backtesting', 'Classification', 'Clustering', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'Open-Source Programming Languages', 'Cloud Computing Platforms', 'Confusion Matrix and ROC Curve Interpretation']","Statistical Modeling: Used to personalize credit card offers and drive data-driven decision-making in financial services.; Relational Databases: Utilized for managing and querying large-scale customer data to support analytics and model development.; Machine Learning: Applied in building, training, evaluating, validating, and implementing predictive models to improve decision-making and manage model risk.; Model Validation and Backtesting: Performed to assess model performance, ensure reliability, and defend models to internal and regulatory stakeholders.; Classification: Used as a modeling technique to categorize data, relevant for tasks such as risk assessment and customer segmentation.; Clustering: Employed to identify natural groupings in data, supporting exploratory data analysis and model development.; Sentiment Analysis: Applied to analyze textual data for understanding customer opinions or behaviors.; Time Series Analysis: Used to model and forecast data points collected or sequenced over time, relevant for financial and risk modeling.; Deep Learning: Implemented as part of advanced modeling techniques to capture complex patterns in data.; Open-Source Programming Languages: Languages such as Python, Scala, and R are used for large-scale data analysis and developing data science solutions.; Cloud Computing Platforms: Leveraged to support scalable data processing and model deployment environments.; Confusion Matrix and ROC Curve Interpretation: Techniques used to evaluate classification model performance and inform decision-making."
vELC56-lhnBwwds7AAAAAA==,[],,"['Longitudinal Data Analysis', 'Mixed-Effects Models', 'Multi-Omic Data Analysis', 'Data Preprocessing', 'Data Integration and Harmonization', 'Statistical Modeling', 'Data Visualization', 'Reproducible Research Practices', 'Programming Languages for Data Science']","Longitudinal Data Analysis: Used to develop and implement statistical models analyzing transcriptomic, metabolomic, and epigenetic data across multiple timepoints in longitudinal studies.; Mixed-Effects Models: Applied as part of statistical modeling techniques to analyze longitudinal multi-omic datasets, accounting for both fixed and random effects.; Multi-Omic Data Analysis: Involves integration, quality control, and statistical analysis of transcriptomic, metabolomic, and epigenetic datasets across timepoints to support study hypotheses.; Data Preprocessing: Includes normalization, batch correction, and outlier detection performed on raw omics datasets to generate usable data for downstream analyses.; Data Integration and Harmonization: Conducted across multiple omic platforms and longitudinal timepoints to ensure alignment with study protocols and cohort structures.; Statistical Modeling: Development and implementation of models to analyze complex biological data, ensuring methodological rigor in hypothesis testing.; Data Visualization: Creation of publication-ready visualizations and summary outputs for internal reporting, scientific publications, and grant progress updates.; Reproducible Research Practices: Documentation of analytical pipelines and code, including workflow documentation and version control (e.g., Git), to ensure reproducibility and adherence to open science best practices.; Programming Languages for Data Science: Proficiency in R and Python used for statistical analysis, data preprocessing, and workflow development in multi-omic research."
F595o4pTxLpOHPEbAAAAAA==,['Artificial Intelligence and Machine Learning'],Artificial Intelligence and Machine Learning: Experience developing and working with AI/ML technologies to enhance data science solutions and support mission requirements within the Intelligence Community.,"['Data Mining', 'Data Modeling', 'Exploratory Data Analysis', 'Optimization Models', 'Big Data Analytics', 'Advanced Statistical Analysis', 'Data Visualization', 'Database Development and Maintenance', 'Programming Languages for Data Science', 'Geospatial Analysis and GIS Tools', 'Applied Mathematics and Analytical Techniques', 'Data Pipelines and Automated Workflows']","Data Mining: Used to extract and analyze patterns from large structured and unstructured datasets to support intelligence and operational decision-making.; Data Modeling: Creating data models and scripting solutions to represent and analyze complex datasets for the DoD and Intelligence Community.; Exploratory Data Analysis: Applying techniques to identify meaningful relationships, patterns, or trends from complex data to inform decision makers.; Optimization Models: Researching and implementing optimization strategies and methods to improve data management and analytical processes.; Big Data Analytics: Applying big data analytic tools to large, diverse datasets to deliver impactful insights and assessments relevant to mission operations.; Advanced Statistical Analysis: Using commercial off-the-shelf (COTS) statistical software such as Map Large, Tableau, and MatLab for advanced analysis and data visualization to interrogate large datasets and identify patterns and behavioral likelihoods.; Data Visualization: Creating visual representations of data including matrix analytics, network analytics, and graphing to support analytical workforce and intelligence assessments.; Database Development and Maintenance: Developing, maintaining, and manipulating databases, including geodatabases related to GEOINT, SIGINT, or OSINT data to support area of interest needs.; Programming Languages for Data Science: Using programming languages such as Python, SQL, C++, Java, HTML5, and JavaScript for data processing, web interface development, and geospatial analysis.; Geospatial Analysis and GIS Tools: Applying GIScience and geospatial analysis techniques using tools like ArcGIS Desktop, ArcGIS Server, Arc SDE, ArcIMS, ArcObjects, and Model Builder to analyze geospatial intelligence data.; Applied Mathematics and Analytical Techniques: Combining applied mathematics with programming and analytical methods to provide impactful insights for decision makers.; Data Pipelines and Automated Workflows: Leading and assisting in the development of automated processes and workflows to streamline data science solutions and analytical methodologies."
JIwW_VAAwcd4PSYNAAAAAA==,[],,"['Data Modeling', 'Data Warehouses and Lakes', 'Data Governance', 'Data Pipelines', 'Data Migration', 'Data Quality Management', 'Data Artifacts', 'Relational SQL and NoSQL Databases', 'Cloud Data Platforms', 'Data Security and Encryption', 'Data Science Pipelines', 'Business Intelligence Tools', 'Agile Methodology']","Data Modeling: Involves architecting and developing data models, including schema design and entity relationship diagrams, to support data warehouses, lakes, and lakehouses.; Data Warehouses and Lakes: Designing and developing large-scale data repositories such as data warehouses, lakes, and lakehouses for structured and unstructured data storage and retrieval.; Data Governance: Implementing data governance practices to ensure data quality, security, and compliance within data platforms and pipelines.; Data Pipelines: Developing and maintaining data pipelines to support data migration, integration, and operationalization of data for analytics and data science teams.; Data Migration: Managing large-scale migration of data environments with a focus on performance and reliability.; Data Quality Management: Collaborating with management and engineers to support data quality efforts, including development of data quality plans and data management plans.; Data Artifacts: Supporting the development of data artifacts such as data dictionaries, data asset catalogs, and data flow diagrams to document and manage data assets.; Relational SQL and NoSQL Databases: Experience working with both relational SQL and NoSQL databases to design storage and retrieval solutions for diverse data types.; Cloud Data Platforms: Designing and developing cloud-based data platforms using managed services from AWS, Azure, or GCP, including services like AWS EC2, EMR, RDS, Redshift, and Google BigQuery.; Data Security and Encryption: Applying FIPS 140-2 compliant encryption standards to protect data at rest and in motion within data platforms.; Data Science Pipelines: Designing storage and retrieval solutions that support data science pipelines and operationalization of information.; Business Intelligence Tools: Experience working with BI tools such as Looker to enable user consumption and visualization of data.; Agile Methodology: Working in an agile environment to collaborate effectively with technical and non-technical stakeholders at all levels."
FSq23doT5Q3_l8m3AAAAAA==,[],,"['Marketing Mix Modeling', 'Bayesian Modeling', 'Frequentist Modeling', 'Python', 'SQL', 'Data Integration', 'Data Visualization']","Marketing Mix Modeling: Build, validate, and interpret Marketing Mix Models to quantify channel effectiveness and integrate internal and external data sources into unified datasets for analysis.; Bayesian Modeling: Apply Bayesian statistical methods in Marketing Mix Modeling to enhance model accuracy and inference.; Frequentist Modeling: Use Frequentist statistical approaches in Marketing Mix Modeling for model development and validation.; Python: Utilize Python programming language, including libraries such as pandas, statsmodels, and PyMC3, for data manipulation, statistical modeling, and analysis.; SQL: Employ SQL skills for querying, extracting, and transforming large datasets to support modeling and analysis.; Data Integration: Combine internal and external data sources into unified datasets to support comprehensive Marketing Mix Modeling.; Data Visualization: Create visualizations to communicate technical insights and optimization recommendations effectively to business stakeholders."
qpj3zaGyt6CCKKexAAAAAA==,"['Large Language Models', 'Embeddings']",Large Language Models: Experience with LLMs and embeddings is required to enhance ML/AI capabilities and support advanced AI-driven features in the Apollo product.; Embeddings: Used in conjunction with LLMs to improve entity resolution and information retrieval within the AI/ML framework.,"['Statistical Modeling', 'Relational Databases', 'Machine Learning', 'Graph-based Machine Learning', 'Deep Learning', 'Algorithm Development', 'Data Analytics', 'Open Source Programming Languages', 'Entity Resolution', 'Information Retrieval', 'Data Science Solutions', 'Data Pipelines']","Statistical Modeling: Used to personalize credit card offers and drive data-driven decision-making at scale across billions of customer records.; Relational Databases: Utilized for managing and querying structured business data critical to the Apollo platform.; Machine Learning: Applied to resolve identity, predict valuable features, and build business data products that provide competitive advantages.; Graph-based Machine Learning: A core technical area leveraged for entity resolution and information retrieval within the Apollo product.; Deep Learning: Used alongside graph-based ML and algorithm development to advance ML/AI capabilities for business data products.; Algorithm Development: Involved in creating advanced algorithms to support ML/AI solutions and product features.; Data Analytics: Performed extensively by the candidate, including large scale data analysis using open source programming languages.; Open Source Programming Languages: Used for large scale data analysis, including Python, Scala, or R.; Entity Resolution: A key ML/AI technique applied to identify and link business entities accurately within large datasets.; Information Retrieval: Employed to enhance data access and usability in the Apollo platform.; Data Science Solutions: Developed to push the envelope with state-of-the-art techniques and deliver impactful business results.; Data Pipelines: Architected to acquire and process massive amounts of business data efficiently for real-time intelligent applications."
rb-LZhza9xIgTYTaAAAAAA==,"['Large Language Models', 'Generative AI', 'PyTorch', 'Hugging Face', 'LangChain', 'Lightning', 'Vector Databases', 'Training Optimization', 'Self-Supervised Learning', 'Explainability', 'Reinforcement Learning from Human Feedback', 'AI Model Engineering and Deployment']","Large Language Models: Expertise required to adapt, fine-tune, and deploy LLMs for customer-facing applications, enabling dynamic and personalized user experiences.; Generative AI: Used to create next-generation experiences powered by emerging generative AI technologies integrated into products like digital assistants.; PyTorch: Deep learning framework used for training and fine-tuning neural network models including large language models.; Hugging Face: Open-source platform and library used for working with transformer models and managing LLMs.; LangChain: Framework used to build applications powered by language models, facilitating integration and orchestration of LLMs.; Lightning: Deep learning framework used to streamline training and deployment of neural network models.; Vector Databases: Used to store and query vector embeddings for efficient retrieval in AI applications involving LLMs.; Training Optimization: Expertise in improving the efficiency and effectiveness of training large AI models.; Self-Supervised Learning: Applied as a key subdomain technique for training large models without extensive labeled data.; Explainability: Used to interpret and communicate the behavior and decisions of AI models to stakeholders.; Reinforcement Learning from Human Feedback: Applied to improve AI model performance by incorporating human feedback during training.; AI Model Engineering and Deployment: Involves delivering libraries, platforms, or solution-level code to production systems, ensuring scalability and resilience for AI models.","['Machine Learning', 'Natural Language Processing', 'Data Analytics', 'SQL', 'Python', 'Scala', 'R', 'AWS']","Machine Learning: Used to build predictive models and NLP models through all phases of development including design, training, evaluation, and validation; operationalized in scalable production systems serving millions of customers.; Natural Language Processing: Applied to harness the power of large language models for customer-facing applications, including adaptation and fine-tuning to improve user experience.; Data Analytics: Performed on large volumes of numeric and textual data to reveal insights that inform business decisions and product features.; SQL: Used as a fundamental skill for querying and managing relational databases as part of data analytics and model development.; Python: Programming language used for data science, machine learning, and model development tasks.; Scala: Programming language experience preferred for data processing and analytics.; R: Programming language experience preferred for statistical analysis and data science.; AWS: Cloud computing platform used to support scalable machine learning and data processing workloads."
VzvO9O1Z2iE_TicQAAAAAA==,['TensorFlow'],TensorFlow: Apply TensorFlow specifically for neural network-based model development within the data science workflow.,"['Machine Learning', 'Data Wrangling and Cleaning', 'Feature Engineering', 'Exploratory Data Analysis and Statistical Modeling', 'Data Visualization and Dashboards', 'Data Science Pipelines and Workflows', 'Programming Languages and Libraries', 'Big Data Processing Tools', 'Cloud Computing Platforms']","Machine Learning: Design, train, and evaluate models for pattern recognition, anomaly detection, and prediction to extract insights from large, complex datasets and enable data-driven decision-making.; Data Wrangling and Cleaning: Perform data wrangling, data cleaning, and feature engineering to prepare data for analysis.; Feature Engineering: Create and select features to improve model performance as part of data preparation.; Exploratory Data Analysis and Statistical Modeling: Conduct exploratory data analysis and statistical modeling to uncover trends, patterns, and relationships in data.; Data Visualization and Dashboards: Develop data visualizations, dashboards, and reports to communicate insights to both technical and non-technical audiences.; Data Science Pipelines and Workflows: Implement pipelines and workflows to automate and scale analytic processes.; Programming Languages and Libraries: Use Python and R programming languages along with libraries such as scikit-learn, pandas, and TensorFlow for data manipulation, model training, and evaluation.; Big Data Processing Tools: Utilize big data tools like Hadoop and Spark to handle large datasets.; Cloud Computing Platforms: Leverage cloud platforms such as AWS and Azure to support data science solutions."
Luhjoi_lXJPh7xdSAAAAAA==,"['Generative AI', 'Neural Networks']","Generative AI: Utilized within the project for advanced data science tasks, including NLP and entity extraction, to enhance analytic capabilities.; Neural Networks: Developed as part of AI/ML models to support computer vision, NLP, and audio modeling applications in intelligence community missions.","['Data Engineering', 'Natural Language Processing', 'Computer Vision', 'Audio Signal Processing', 'Machine Learning Models', 'Python Programming']","Data Engineering: Involves parsing raw data and performing data ingest and ETL into sponsor environments to support data science workflows and analytic software systems.; Natural Language Processing: Applied as part of diverse data science skills to analyze text data, including entity extraction, within cyber-related data science missions.; Computer Vision: Used for image and video processing as part of data analysis tasks in the project.; Audio Signal Processing: Conducting data analysis on audio data as part of the data scientist's responsibilities.; Machine Learning Models: Developing AI/ML models including neural networks and tree-based algorithms to support mission success in intelligence community projects.; Python Programming: Strong Python programming skills are required to implement data science and data engineering tasks."
zJn4GywD96dEf-_xAAAAAA==,"['Large Language Models', 'Generative AI', 'PyTorch', 'Hugging Face', 'LangChain', 'Lightning', 'Vector Databases', 'Training Optimization', 'Self-Supervised Learning', 'Explainability', 'Reinforcement Learning from Human Feedback']","Large Language Models: Expertise required in adapting, fine-tuning, and training LLMs for customer-facing AI applications.; Generative AI: Driving force to experiment and innovate with emerging generative AI technologies to create next-generation customer experiences.; PyTorch: Used as a deep learning framework to build and train neural network models including LLMs.; Hugging Face: Utilized as an open-source platform and library for working with transformer models and LLMs.; LangChain: Employed to build AI applications that integrate LLMs with external data sources and workflows.; Lightning: Used as a framework to simplify and scale deep learning model training and deployment.; Vector Databases: Used to store and query vector embeddings for efficient retrieval in AI applications involving LLMs.; Training Optimization: Expertise in improving the efficiency and effectiveness of training large language and computer vision models.; Self-Supervised Learning: Applied as a key subdomain technique for training models without extensive labeled data.; Explainability: Focus on making AI model decisions interpretable and understandable for stakeholders.; Reinforcement Learning from Human Feedback: Used to improve model behavior and alignment by incorporating human feedback during training.","['Machine Learning', 'Natural Language Processing', 'Data Analytics', 'SQL', 'Python', 'Scala', 'R', 'AWS']","Machine Learning: Used to build predictive models and NLP models through all phases of development including design, training, evaluation, and validation; operationalized in scalable production systems serving millions of customers.; Natural Language Processing: Applied to harness the power of large language models for customer-facing applications, including adaptation and fine-tuning of models.; Data Analytics: Experience performing data analytics is required, involving analysis of numeric and textual data to reveal insights.; SQL: Used for querying and managing relational databases as part of data analytics and model development.; Python: Programming language used for machine learning, data analytics, and model development.; Scala: Programming language experience preferred for data science and machine learning tasks.; R: Programming language experience preferred for statistical analysis and data science.; AWS: Cloud computing platform used to support scalable machine learning and data processing workloads."
SWC1sPoUrYiHb9W8AAAAAA==,[],,"['Anomaly Detection', 'Statistical Models', 'Machine Learning Algorithms', 'Data Analysis of Structured and Unstructured Data', 'Data Visualization and Dashboards', 'Python Programming']","Anomaly Detection: Used to identify unusual patterns or outliers in data to support cyber tradecraft and security objectives.; Statistical Models: Designed, developed, and implemented to analyze data and extract meaningful insights relevant to business and security goals.; Machine Learning Algorithms: Applied to model complex data relationships and support predictive analytics within cyber tradecraft contexts.; Data Analysis of Structured and Unstructured Data: Analyzed large datasets from various sources to uncover trends and patterns that inform decision-making and operational strategies.; Data Visualization and Dashboards: Communicated analytical findings clearly through visual tools and reports to stakeholders across engineering, product, and business teams.; Python Programming: Used as a primary programming language to develop data models, perform analysis, and implement machine learning algorithms."
XmlF2hnsr8waRqdGAAAAAA==,"['Large Language Models (LLMs)', 'Generative AI', 'PyTorch', 'Hugging Face', 'LangChain', 'Lightning', 'Vector Databases', 'Training Optimization', 'Self-Supervised Learning', 'Explainability', 'Reinforcement Learning from Human Feedback (RLHF)', 'Deep Learning']",Large Language Models (LLMs): Harnessed and fine-tuned to build customer-facing AI applications such as digital assistants and personalized experiences.; Generative AI: Used to create next-generation customer experiences powered by emerging AI technologies.; PyTorch: Utilized as a deep learning framework to develop and train neural network models including LLMs.; Hugging Face: Employed as an open-source platform and library for working with transformer models and LLMs.; LangChain: Used to build AI applications that integrate LLMs with external data sources and workflows.; Lightning: Applied as a framework to streamline deep learning model training and deployment.; Vector Databases: Used to store and retrieve high-dimensional embeddings for AI applications such as semantic search and recommendation.; Training Optimization: Expertise in improving efficiency and effectiveness of training large AI models.; Self-Supervised Learning: Applied as a technique to train models using unlabeled data to improve performance.; Explainability: Used to interpret and understand AI model decisions to ensure transparency and trust.; Reinforcement Learning from Human Feedback (RLHF): Applied to fine-tune AI models by incorporating human feedback to improve alignment and performance.; Deep Learning: Used to develop advanced neural network models including language and computer vision models at scale.,"['Statistical Modeling', 'Relational Databases', 'Machine Learning', 'Natural Language Processing (NLP)', 'Python', 'Scala', 'R', 'SQL', 'Data Analytics', 'Machine Learning Model Development Lifecycle']","Statistical Modeling: Used to personalize credit card offers and drive data-driven decision-making at scale.; Relational Databases: Employed historically and currently to manage and query large volumes of structured data.; Machine Learning: Applied to build predictive models and AI-powered products that improve customer financial interactions.; Natural Language Processing (NLP): Used to develop and fine-tune models for customer-facing applications, enabling features like digital assistants and content search.; Python: A primary programming language used for data analytics and machine learning model development.; Scala: Used as a programming language for data analytics and model development.; R: Used as a programming language for data analytics and statistical modeling.; SQL: Used for querying and managing data within relational databases.; Data Analytics: Performed to analyze large datasets and extract actionable insights to support business decisions.; Machine Learning Model Development Lifecycle: Involves design, training, evaluation, validation, and operationalization of models in production systems serving millions of customers."
DDlYUr7BgyKaoURDAAAAAA==,[],,"['Statistical and Analytical Methods', 'Data Integration and Processing', 'Dashboards and BI Tools', 'Data Pipelines and Big Data Processing', 'Programming for Data Science', 'Workflow Documentation']","Statistical and Analytical Methods: Designs, develops, and implements statistical and analytical methods to analyze data and support decision-making processes.; Data Integration and Processing: Examines processes and systems to consolidate and analyze diverse data sets including structured, semi-structured, and unstructured data, and develops software programs, algorithms, and queries to collect, clean, model, integrate, and evaluate datasets.; Dashboards and BI Tools: Develops dashboards and information tools using BI tools such as Superset and Qlik to visualize data and support business decisions.; Data Pipelines and Big Data Processing: Experience with developing data analytics and processing pipeline applications using Apache Spark and Python to handle large-scale data processing.; Programming for Data Science: Employs programming skills, particularly in Python, to develop techniques and methods for data analysis and pipeline development.; Workflow Documentation: Documents workflows and processes to improve knowledge sharing across the Data Science and Engineering team."
3pUDR8Cr6LAiuPx6AAAAAA==,[],,"['Graph Databases', 'Graph Theory and Algorithms', 'Python Data Parsers', 'Data Modeling', 'Data Query Optimization', 'Real-Time Data Processing', 'ETL Processes', 'Analytic Performance Measurement', 'Data Documentation', 'SQL-like Query Languages']","Graph Databases: The job involves designing, modeling, and optimizing complex graph data structures and queries to support mission data analysis and visualization in graph format, with specific mention of Neo4j and its query language Cypher.; Graph Theory and Algorithms: Understanding and applying graph theory and algorithms is important for developing and refining graph data models and optimizing graph queries to meet customer-driven feature requests.; Python Data Parsers: Developing and maintaining data parsers using Python is required to support reliable data ingestion pipelines and ensure data accuracy and integrity.; Data Modeling: Creating and refining data models, particularly graph data models, to accommodate new data sources and fulfill analytical and operational requirements.; Data Query Optimization: Designing and optimizing database queries, especially graph queries, to ensure performant data retrieval and analysis.; Real-Time Data Processing: Familiarity with real-time data processing or streaming analytics is desirable to support near-real-time analysis and traversal of mission data.; ETL Processes: Understanding ETL (Extract, Transform, Load) processes for handling large, complex datasets is beneficial for managing data ingestion and preparation.; Analytic Performance Measurement: Measuring and analyzing query performance to identify slow and fast-running queries, ensuring optimal system efficiency.; Data Documentation: Maintaining comprehensive documentation of graph schemas, data models, and related processes using tools such as Confluence to support collaboration and knowledge sharing.; SQL-like Query Languages: Using Cypher, a query language for graph databases, to write and optimize queries for data retrieval and analysis."
ThZVOMyrozHI7hKEAAAAAA==,['AI/ML Integration'],"AI/ML Integration: Ensure that advanced analytics and AI/ML solutions align with both enterprise and operational warfighter needs; assess and recommend emerging AI/ML tools and architectures for integration into composable, tactical decision support platforms.","['Exploratory Data Analysis', 'Predictive Modeling', 'Machine Learning', 'Dashboards and Data Visualization', 'Data Governance', 'SQL and NoSQL Databases', 'Python and R Programming']","Exploratory Data Analysis: Perform and/or assess statistical and exploratory data analysis (EDA) on portfolio-level data to inform executive decisions and SAFe-aligned strategy execution.; Predictive Modeling: Drive the design and implementation of predictive models tailored for DDIL environments and tactical decision-making contexts.; Machine Learning: Drive the design and implementation of machine learning techniques tailored for DDIL environments and tactical decision-making contexts; assess and recommend AI/ML tools and architectures for integration into tactical decision support platforms.; Dashboards and Data Visualization: Develop and/or support the creation of dashboards and visualizations that support Lean Portfolio Management, strategic portfolio reviews, and operational alignment.; Data Governance: Lead the development and coordination of data governance plans and tagging guidance to ensure alignment with enterprise and mission needs; drive the implementation of enterprise-level data policies into the data strategy.; SQL and NoSQL Databases: Proficiency in data platforms such as SQL and NoSQL databases, big data, and distributed data processing.; Python and R Programming: Proficiency in Python, R, SQL, or other relevant data analysis languages used for data science and analytics tasks."
Kivyrc7fiUv35ibSAAAAAA==,['Amazon SageMaker'],Amazon SageMaker: Used as a platform for developing and deploying machine learning and artificial intelligence models to support cybersecurity data analysis and solutions.,"['Statistical Analysis', 'Data Visualization Tools', 'Data Transformation', 'Python Programming', 'Databases and Data Architectures', 'Machine Learning', 'Relational Database Management Systems (RDBMS)', 'Big Data Technologies']","Statistical Analysis: Used to analyze large cybersecurity data sets and apply statistical methods to interpret data for Information Assurance compliance and vulnerability management.; Data Visualization Tools: Experience with tools such as Tableau, Infogram, Chartbloks, and Microsoft Excel to create graphical presentations and support situational awareness and compliance status reporting.; Data Transformation: Transforming structured data formats and schemas using programming tools like Python and JSON to prepare data for analysis and visualization.; Python Programming: Used for coding, data transformation, and analysis tasks related to cybersecurity data sets.; Databases and Data Architectures: Knowledge of databases, data structures, and architectures to manage and analyze cybersecurity-related data.; Machine Learning: Applied to develop data-driven solutions and analyze cybersecurity data, including experience with AWS SageMaker.; Relational Database Management Systems (RDBMS): Experience managing and querying relational databases relevant to cybersecurity data.; Big Data Technologies: Experience with Apache Hadoop, Hadoop Distributed File System, and Amazon Elastic MapReduce (EMR) to handle large-scale cybersecurity data processing."
z1zA2HhCf8D6og4gAAAAAA==,"['Large Language Models', 'Generative AI', 'PyTorch', 'Hugging Face', 'LangChain', 'Vector Databases', 'Lightning', 'Reinforcement Learning from Human Feedback', 'Self-Supervised Learning', 'Explainability']","Large Language Models: Central to the role for adapting, fine-tuning, and deploying customer-facing AI applications, enabling advanced natural language understanding and generation capabilities.; Generative AI: Employed to create next-generation customer experiences powered by emerging AI technologies, including dynamic and personalized interactions within the mobile app.; PyTorch: Used as a deep learning framework to develop and train neural network models, particularly large language models and other AI solutions.; Hugging Face: Leveraged as an open-source platform and library for accessing, fine-tuning, and deploying transformer-based models and other state-of-the-art AI technologies.; LangChain: Utilized to build AI-powered applications that integrate language models with external data sources and workflows, enhancing the capabilities of AI products.; Vector Databases: Used to store and retrieve high-dimensional vector embeddings generated by AI models, supporting efficient similarity search and retrieval in AI applications.; Lightning: Applied as a framework to streamline and scale deep learning model training and deployment, improving efficiency in AI model development.; Reinforcement Learning from Human Feedback: Implemented as a key subdomain expertise to optimize training of language models, enhancing model performance and alignment with user expectations.; Self-Supervised Learning: Used as an advanced training technique to improve model learning from unlabeled data, contributing to the development of robust AI models.; Explainability: Incorporated to provide transparency and interpretability of AI model decisions, ensuring trust and understanding in AI-powered products.","['Natural Language Processing', 'Machine Learning', 'Data Analytics', 'SQL', 'Python']","Natural Language Processing: Used as a core expertise area to build and adapt models for customer-facing applications, leveraging textual data to create personalized and dynamic user experiences.; Machine Learning: Applied to build models through all phases of development including design, training, evaluation, and validation, and to operationalize scalable production systems serving millions of customers.; Data Analytics: Involves analyzing large volumes of numeric and textual data to reveal insights that inform product features and customer interactions.; SQL: Used as a data querying tool to support data analytics and model development processes.; Python: Utilized as a primary programming language for data science, machine learning, and model development tasks."
7lTh6mDn6ETjNrFtAAAAAA==,['Artificial Intelligence'],Artificial Intelligence: Understanding and applying artificial intelligence concepts as part of the data science and analytics capabilities taught and mentored in the role.,"['Data Analysis', 'Scripting Languages', 'Mathematics and Statistics', 'Data Mining and Metadata Analysis', 'Machine Learning', 'Artificial Intelligence', 'Data Visualization and Automation', 'Big Data Analytics', 'Relational Capabilities Integration', 'Tools and Frameworks']","Data Analysis: Performing data analysis on massive amounts of collected information to pinpoint unique insights and intelligence opportunities within the data.; Scripting Languages: Experience with scripting and data analytics using languages such as Python, Perl, Bash, R, SQL, Scala, and Pig to automate analytic processes and support data science tasks.; Mathematics and Statistics: Proficiency in mathematics and statistics as foundational skills for data science and analytics.; Data Mining and Metadata Analysis: Applying data mining and metadata analysis techniques to extract meaningful patterns and insights from large datasets.; Machine Learning: Utilizing machine learning methods as part of advanced data analytics capabilities and tradecraft.; Artificial Intelligence: Knowledge and application of artificial intelligence concepts within data science and analytics contexts.; Data Visualization and Automation: Using data visualization tools and data automation techniques to present data insights and streamline analytic workflows.; Big Data Analytics: Expertise in big data analytics focusing on data management, data preparation, data governance, and analytic development and production.; Relational Capabilities Integration: Familiarity with integrating relational capabilities and a wide variety of tools and tradecraft to support analytic processes.; Tools and Frameworks: In-depth experience with advanced tools such as Lucene, Jupyter Notebooks, ELK Stack, Splunk, and PowerBI to support data science and analytics activities."
H9PUzNh9aNkfnKxvAAAAAA==,['AI/ML Security and Compliance'],"AI/ML Security and Compliance: Focus on securing AI/ML systems by obtaining authorization under the Risk Management Framework (RMF), writing and maintaining System Security Plans (SSPs), conducting risk assessments, ensuring compliance, defining security and performance requirements, managing secure deployment and configuration, and performing security reviews for AI/ML services and requests.","['Machine Learning', 'Data Mining', 'Graph-Based Algorithms', 'Data Analysis Software', 'Data Visualization', 'Automation of Data Analysis']","Machine Learning: Develop and implement machine learning algorithms for complex datasets and prototype and evaluate models based on performance metrics to support secure AI/ML solutions in cybersecurity contexts.; Data Mining: Apply data mining techniques to extract meaningful patterns from complex datasets as part of developing secure machine learning solutions.; Graph-Based Algorithms: Develop and implement graph-based algorithms to analyze complex datasets relevant to cybersecurity data science tasks.; Data Analysis Software: Utilize programming languages and tools such as R, Python, SAS, and MATLAB for data analysis and developing analytics in cybersecurity data science.; Data Visualization: Generate insightful reports and visualizations to communicate data-driven insights effectively within cybersecurity and AI/ML projects.; Automation of Data Analysis: Collaborate with subject matter experts to automate manual data analysis processes to improve efficiency in cybersecurity data workflows."
rr9tGrODdjuX6FdeAAAAAA==,['Artificial Intelligence'],Artificial Intelligence: Apply AI frameworks as part of advanced data science techniques to develop mission-aligned insights.,"['Machine Learning', 'Statistical Modeling', 'Data Governance', 'Extract, Transform, Load (ETL) Pipelines', 'Data Strategy', 'Data Visualization and Dashboards', 'Advanced Analytics', 'Data Engineering Tools', 'Power BI', 'Jupyter', 'NPIER/DMAIC']","Machine Learning: Apply advanced data science techniques including machine learning to develop mission-aligned insights.; Statistical Modeling: Use statistical modeling as part of advanced data science techniques to generate insights aligned with mission goals.; Data Governance: Coordinate enterprise data governance and strategy across diverse stakeholder groups and guide the adoption of enterprise-wide data governance tools.; Extract, Transform, Load (ETL) Pipelines: Provide subject matter expertise in designing and optimizing ETL pipelines and data architecture.; Data Strategy: Lead the strategic development and execution of data science and analytics initiatives and oversee the implementation of data strategies and frameworks.; Data Visualization and Dashboards: Guide the adoption of enterprise-wide visualization and analysis tools and develop performance metrics and dashboards.; Advanced Analytics: Lead advanced analytics initiatives and apply advanced analytics tools to support mission goals.; Data Engineering Tools: Experience with data engineering tools to support data architecture and pipeline optimization.; Power BI: Use Power BI as a collaboration and visualization tool within the enterprise analytics environment.; Jupyter: Utilize Jupyter notebooks as part of data science and analytics workflows.; NPIER/DMAIC: Develop performance metrics and process improvement strategies using NPIER/DMAIC methodologies."
ipKOsWNirrArF9ziAAAAAA==,[],,"['R programming', 'Python programming', 'Java programming', 'Business analytics', 'Power BI', 'Tableau', 'Data engineering', 'ETL', 'API querying', 'VBA']","R programming: Used for programming and data analysis tasks as part of the data scientist role.; Python programming: Used for programming and data analysis tasks as part of the data scientist role.; Java programming: Used for programming and data analysis tasks as part of the data scientist role.; Business analytics: Involves dashboarding with tools like Power BI or Tableau, business process analysis, and defining and capturing metrics to support decision-making.; Power BI: Used for creating business intelligence dashboards to visualize and analyze data.; Tableau: Used for creating business intelligence dashboards to visualize and analyze data.; Data engineering: Includes ETL (Extract, Transform, Load) processes and querying APIs to manage and prepare data for analysis.; ETL: Part of data engineering responsibilities involving extracting, transforming, and loading data for analysis.; API querying: Used to retrieve data from external or internal sources as part of data engineering tasks.; VBA: Used for automation and scripting within data-related workflows."
_CsHNij4e3_2ocvEAAAAAA==,[],,"['Tableau', 'Alteryx', 'Python', 'R', 'MATLAB']","Tableau: Used as an analytical tool for data visualization and business intelligence to support data analysis tasks.; Alteryx: Utilized as a data analytics platform to prepare, blend, and analyze data for intelligence and business analysis purposes.; Python: Familiarity with Python is required for coding and data analysis tasks within the role.; R: Knowledge of R is expected for statistical analysis and data manipulation.; MATLAB: Experience with MATLAB is relevant for numerical computing and data analysis in the context of intelligence and knowledge management."
HUITRzw7krG6o6tIAAAAAA==,[],,"['Python', 'R', 'SQL', 'Data Mining', 'Statistical Analysis', 'Machine Learning', 'Data Visualization', 'Data Modeling']","Python: Used as a scripting language for advanced data programming and analysis.; R: Used as a scripting language for advanced data programming and statistical analysis.; SQL: Used for data extraction and integration from large datasets.; Data Mining: Applied to analyze large datasets and extract meaningful insights.; Statistical Analysis: Used to transform raw data into meaningful insights through advanced analysis techniques.; Machine Learning: Applied to analyze large datasets and build predictive or analytical models.; Data Visualization: Creating dynamic reports, dashboards, and visualizations to communicate insights.; Data Modeling: Used to represent and analyze data structures and relationships within large datasets."
6rFMigrlJs0dAy89AAAAAA==,['Artificial Intelligence'],"Artificial Intelligence: The job involves designing, implementing, and testing AI applications as part of software components for satellite ground systems, requiring a deep understanding of AI technologies.","['Machine Learning', 'Data Analysis', 'Python', 'Java', 'Unit Testing (JUnit)', 'Data Pipelines', 'Cloud Computing Platforms', 'DevOps Tools', 'Agile Software Development', 'Software Performance and Scalability', 'Secure Coding Practices']","Machine Learning: The role involves designing, implementing, and testing machine learning applications and software components for satellite ground systems, requiring experience in machine learning and data science.; Data Analysis: The job requires working with large datasets and performing data analysis to support software development and system performance.; Python: Python is used for software design, development, and testing of AI and ML applications in a Linux environment.; Java: Java is used for object-oriented software design and development, including AI and ML based applications for satellite ground systems.; Unit Testing (JUnit): Experience with unit testing tools such as JUnit is required to ensure software quality and reliability.; Data Pipelines: The role includes migrating workloads onto new pipelines and maintaining data processing workflows for satellite ground systems.; Cloud Computing Platforms: Experience with cloud platforms like AWS or Azure is preferred to support cloud architecture and enterprise software solutions.; DevOps Tools: Familiarity with DevOps tools such as Git, Jenkins, Docker, and Kubernetes is preferred to support software development and deployment processes.; Agile Software Development: The position operates within an Agile development environment, emphasizing iterative software design and collaboration.; Software Performance and Scalability: Ensuring software performance, reliability, and scalability is a key responsibility in developing advanced applications for satellite systems.; Secure Coding Practices: Familiarity with secure coding practices is important, especially given the defense and aerospace context of the software development."
HWijnsTESQt6covJAAAAAA==,[],,"['Graph Databases', 'Cypher Query Language', 'Python', 'Data Validation', 'Data Modeling', 'Data Pipelines', 'Graph Theory and Algorithms', 'ETL Pipelines and Real-Time Data Processing', 'Collaboration with Stakeholders', 'Documentation Tools']","Graph Databases: Used for designing and optimizing graph data structures and crafting scalable queries to explore and understand complex relationships in real-time mission data, specifically with Neo4j and Cypher.; Cypher Query Language: Employed to write performant queries for graph databases, enabling efficient data retrieval and manipulation tailored to dynamic mission needs.; Python: Utilized for developing data parsers that support data ingestion and streaming workflows, as well as for data manipulation and validation of large datasets to ensure integrity and accuracy.; Data Validation: Performed to ensure the integrity, accuracy, and performance of large datasets under operational constraints in mission-critical environments.; Data Modeling: Involves translating analyst needs into graph-based technical solutions and documenting data models, schemas, and pipelines to support mission objectives.; Data Pipelines: Includes ingestion and streaming workflows supported by Python parsers and integration of queries and ingestion logic into production systems.; Graph Theory and Algorithms: Applied to understand and manipulate complex data structures within graph databases to support mission analysis and intelligence.; ETL Pipelines and Real-Time Data Processing: Experience with building and maintaining pipelines that process data in real-time or batch modes to support mission-focused data environments.; Collaboration with Stakeholders: Engaging with both technical and non-technical users to understand pain points and recommend data-driven solutions.; Documentation Tools: Using tools like Confluence to document data models, schemas, and pipelines for clarity and knowledge sharing."
nY38LhTzSZJlXiKJAAAAAA==,[],,"['Machine Learning Models', 'Statistical Modeling', 'Classification', 'Clustering', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'Confusion Matrix and ROC Curve Interpretation', 'Python', 'Conda', 'AWS', 'H2O', 'Spark', 'SQL', 'Data Retrieval and Integration']","Machine Learning Models: Build machine learning models through all phases of development, including design, training, evaluation, validation, and implementation to prevent fraud at the application stage.; Statistical Modeling: Use statistical modeling techniques to analyze data and personalize credit card offers, as well as to detect and prevent application fraud.; Classification: Apply classification methods as part of building models to identify fraudulent applications.; Clustering: Utilize clustering techniques to analyze data patterns relevant to fraud detection.; Sentiment Analysis: Experience with sentiment analysis as part of data science methods applied in the role.; Time Series Analysis: Use time series methods to analyze data trends and patterns relevant to fraud detection and prevention.; Deep Learning: Apply deep learning techniques as part of the data science toolkit for fraud detection and model building.; Confusion Matrix and ROC Curve Interpretation: Interpret confusion matrices and ROC curves to validate and evaluate the performance of fraud detection models.; Python: Use Python programming language as a primary tool for data analysis, model development, and implementation.; Conda: Leverage Conda for managing Python environments and dependencies in data science workflows.; AWS: Utilize AWS cloud computing platform to handle large-scale data processing and model deployment.; H2O: Use H2O machine learning platform to build and deploy scalable machine learning models.; Spark: Employ Apache Spark for big data processing and analytics on large volumes of numeric and textual data.; SQL: Use SQL to retrieve, combine, and analyze data from various structured data sources.; Data Retrieval and Integration: Retrieve, combine, and analyze data from a variety of sources and structures to support data science solutions."
GpF-f0pzvTLkJcBFAAAAAA==,[],,"['Splunk SPL', 'Python', 'Data Science Techniques', 'NumPy', 'Pandas', 'OpenPyXL', 'SQL', 'MySQL', 'MS Access', 'Visual Basic for Applications (VBA)', 'PowerShell', 'Tableau']","Splunk SPL: Used to extract raw data and create reports and dashboards for data usage auditing, focusing on queries, reports, and dashboards within Splunk Enterprise systems.; Python: Applied for additional processing and augmentation of data extracted from Splunk, as well as for developing, executing, and maintaining scripts and prototypes to analyze, interpret, visualize, and gain insights from large data sets.; Data Science Techniques: Experimented with various data science methods to analyze and interpret data, supporting the creation and refinement of recurring and ad-hoc reports to meet customer requirements.; NumPy: Utilized as a data science library in Python to manipulate and analyze numerical data within large data sets.; Pandas: Used as a data science library in Python for data manipulation and analysis to support reporting and data interpretation tasks.; OpenPyXL: Employed as a Python library to work with Excel files, facilitating data processing and reporting.; SQL: Used for querying databases such as MySQL and MS Access to support data extraction and reporting needs.; MySQL: Referenced as a database system for which SQL queries are written to extract and manage data.; MS Access: Used as a database platform with SQL querying capabilities to support data management and reporting.; Visual Basic for Applications (VBA): Applied for automation and scripting within Microsoft Office environments to support data processing and reporting tasks.; PowerShell: Used for scripting and automation to support data processing and system management tasks.; Tableau: Utilized as a business intelligence tool to create dashboards and visualizations for data reporting and analysis."
9jT5m8Bkw1Y_0r9gAAAAAA==,['Machine Learning'],"Machine Learning: Applied to solve problems in the field of NLP for unstructured data, improve product suites, and develop models that support business insights and client solutions.","['SQL', 'Python', 'Statistical Analysis', 'Narrative Visualization', 'Classification', 'Ranking Models', 'Natural Language Processing (NLP) Techniques', 'PostgreSQL', 'Regular Expressions']","SQL: Used for writing queries to extract and manipulate data from databases, essential for working with diverse data sources in the job.; Python: Used for running models and writing production-ready code, including statistical analysis and data processing in Jupyter notebooks.; Statistical Analysis: Applied to analyze data, build general statistical models, and derive evidence-based, quantifiable solutions to business problems.; Narrative Visualization: Creating compelling visual presentations of data to communicate insights effectively to clients and stakeholders.; Classification: Translating business problems into computational tasks involving classification to solve specific client needs.; Ranking Models: Developing models that order or prioritize items, used to address business targets and computational tasks.; Natural Language Processing (NLP) Techniques: Familiarity with a range of NLP methods or computational linguistics to work on unstructured text data and linguistic models.; PostgreSQL: Using PostgreSQL 12+ as a database system for managing and querying structured data.; Regular Expressions: Utilized for pattern matching and text processing tasks within data cleaning or feature extraction workflows."
Z0h5S9trofVXMJTUAAAAAA==,[],,"['Causal Inference', 'A/B Testing', 'Predictive Modeling', 'Segmentation Models', 'Statistical Analysis', 'Data Analysis', 'Data Collection', 'Reporting and Dashboards', 'SQL', 'Python', 'R']","Causal Inference: Used to measure incrementality and analyze observational studies and quasi-experimental designs such as Difference-in-Differences to understand drivers of business outcomes and demonstrate incremental value and ROI.; A/B Testing: Design, implementation, and analysis of controlled experiments to evaluate business and product changes and their impact on key metrics.; Predictive Modeling: Designing, building, validating, and deploying statistical models to address key business questions and forecast outcomes.; Segmentation Models: Developing models to segment data for deeper insights into business activities and activation levers.; Statistical Analysis: Applying statistical techniques to analyze data, identify operational insights, and transform raw data into actionable business recommendations.; Data Analysis: Conducting in-depth analysis of various datasets including business behavior, product journeys, and training interventions to uncover actionable insights and support decision-making.; Data Collection: Immersing in gathering and preparing data from multiple sources to enable comprehensive analysis and model development.; Reporting and Dashboards: Designing, developing, and launching reporting and dashboard solutions to enable stakeholders to independently track and manage key business metrics consistently.; SQL: Querying databases to extract and manipulate data for analysis and model building.; Python: Using Python programming for coding analytics solutions, statistical modeling, and data manipulation.; R: Utilizing R programming language for statistical analysis and modeling to solve business problems."
MRajmPeaHnLmvVeKAAAAAA==,"['Generative AI', 'Large Language Models', 'Transformers', 'Prompt Engineering']","Generative AI: The role involves applying generative AI models to automate clinical documentation, extract insights, and deliver real-time intelligence, driving innovation at scale in healthcare operations.; Large Language Models: Hands-on expertise with Large Language Models (LLMs) is required, including advanced prompt engineering, instruction fine-tuning, and parameter-efficient tuning techniques to build scalable AI solutions.; Transformers: Experience with Transformer architectures is necessary for developing deep learning models that support complex healthcare AI applications.; Prompt Engineering: The job includes advanced prompt engineering to optimize interactions with generative AI models and LLMs for healthcare-specific tasks.","['Machine Learning', 'Deep Learning', 'Python', 'Clinical Data Standards', 'Semantic Search and Information Retrieval', 'AI Evaluation Strategies']","Machine Learning: The job involves applying classical machine learning models to build predictive and analytical solutions that power healthcare processes such as prior authorization, risk adjustment, and claims adjudication.; Deep Learning: The role requires deep technical fluency in deep learning architectures, including neural networks and Transformers, to develop advanced models for healthcare applications.; Python: Proficiency in Python is essential for building, deploying, and scaling AI and machine learning models, as well as developing clean, scalable code across the AI engineering stack.; Clinical Data Standards: The job requires working with clinical data aligned to interoperability standards such as FHIR, C-CDA, and HL7 to extract meaningful insights from structured and unstructured healthcare data.; Semantic Search and Information Retrieval: Experience building AI models that include semantic search and information retrieval systems to enable intelligent, high-precision workflows in healthcare.; AI Evaluation Strategies: Familiarity with AI evaluation methods such as data labeling, annotation workflows, human-in-the-loop review, and longitudinal model monitoring in high-stakes healthcare environments is required."
qtg_xAHD7eRVhbCoAAAAAA==,[],,"['Machine Learning', 'Statistical Modeling', 'Clustering', 'Classification', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'Python', 'Conda', 'AWS', 'H2O', 'Spark', 'Relational Databases', 'Open Source Programming Languages', 'Model Validation and Backtesting']","Machine Learning: Build machine learning models through all phases of development, including design, training, evaluation, validation, and implementation to drive business growth and insights.; Statistical Modeling: Use statistical modeling techniques to personalize credit card offers and analyze customer data for business decision-making.; Clustering: Apply clustering methods to segment data and understand customer groups as part of data analysis and model building.; Classification: Develop classification models and interpret performance metrics such as confusion matrices and ROC curves to evaluate model effectiveness.; Sentiment Analysis: Perform sentiment analysis on textual data to extract insights relevant to customer behavior and business decisions.; Time Series Analysis: Utilize time series methods to analyze data trends over time as part of predictive modeling and analytics.; Deep Learning: Experience with deep learning techniques as part of advanced analytics and model development.; Python: Use Python programming language extensively for large scale data analysis and building data science solutions.; Conda: Leverage Conda as a package and environment management system to support data science workflows.; AWS: Utilize Amazon Web Services cloud computing platform to handle large datasets and support machine learning model deployment.; H2O: Employ H2O open-source machine learning platform to build and deploy scalable machine learning models.; Spark: Use Apache Spark for distributed data processing and large scale data analytics.; Relational Databases: Work with relational databases to manage and query structured data as part of data analytics.; Open Source Programming Languages: Leverage open source languages such as Python, Scala, or R for large scale data analysis and data science solution development.; Model Validation and Backtesting: Validate and backtest models to ensure accuracy and reliability before deployment."
8xLzY6gxrhSrqLmpAAAAAA==,[],,"['Python', 'SQL', 'R', 'Large-scale data analysis', 'Statistical modeling', 'Transforming complex data into actionable insights', 'Elasticsearch']",Python: Required proficiency in Python programming language for data science tasks.; SQL: Required proficiency in SQL for querying and managing large-scale data.; R: Required proficiency in R programming language or comparable languages for statistical analysis.; Large-scale data analysis: Experience with analyzing large datasets to extract meaningful insights.; Statistical modeling: Experience with building and applying statistical models to data for analysis and prediction.; Transforming complex data into actionable insights: Ability to interpret and convert complex data into practical recommendations or decisions.; Elasticsearch: Preferred experience with Elasticsearch for data search and analytics.
UHRE_CDzNaBk8zTgAAAAAA==,['Deep Learning'],"Deep Learning: Build and deploy deep learning models at production scale using frameworks like PyTorch, applying neural network techniques.","['Machine Learning Models', 'Linear Programming', 'Optimization Techniques', 'Forecasting', 'MLOps', 'Distributed Computing', 'Experimental Design and Analysis', 'Self-Improving Systems', 'Deep Learning Frameworks', 'Python Programming']","Machine Learning Models: Design and implement advanced machine learning models for supply chain optimization and real-world problem solving.; Linear Programming: Apply linear programming techniques to optimize supply chain and other operational problems.; Optimization Techniques: Use optimization methods including combinatorial optimization to improve supply chain processes and model performance.; Forecasting: Research and implement state-of-the-art forecasting techniques to support product decisions.; MLOps: Own the MLOps pipeline for training and serving large-scale machine learning models, ensuring production readiness and scalability.; Distributed Computing: Leverage distributed computing and big data technologies to handle billions of data points and support large-scale model training.; Experimental Design and Analysis: Design experiments and analyze results to drive product decisions and improve system performance.; Self-Improving Systems: Build systems that learn and improve over time from user interactions.; Deep Learning Frameworks: Use deep learning frameworks such as PyTorch to build and deploy deep learning models at production scale.; Python Programming: Write production-quality Python code following software engineering best practices."
FylOg5E7_94Psyu_AAAAAA==,"['Large Language Models', 'Generative AI', 'Deep Learning']","Large Language Models: Train, fine-tune, and enhance large language models (LLMs) using open-source Python libraries to build advanced NLP solutions in healthcare.; Generative AI: Apply generative AI techniques and deep learning methods to improve model performance and develop innovative AI products in the healthcare domain.; Deep Learning: Utilize deep learning approaches, including neural networks, to build and optimize models for clinical data analysis and NLP tasks.","['Exploratory Data Analysis', 'Data Science Pipelines', 'Machine Learning Models', 'Time Series Forecasting', 'Clinical Data Standards', 'Spark NLP']","Exploratory Data Analysis: Conduct comprehensive exploratory data analysis and data enrichment to understand and prepare clinical data for modeling.; Data Science Pipelines: Build data science and data engineering pipelines specific to analyzing clinical data, including extracting information from medical text or images and integrating uncertain information from multiple medical data sources.; Machine Learning Models: Develop and optimize machine learning models tailored to healthcare contexts, ensuring models are validated for bias, overfitting, and concept drift to maintain reliability and effectiveness.; Time Series Forecasting: Create time series forecasting pipelines as part of advanced machine learning solutions to address specific business needs in healthcare.; Clinical Data Standards: Utilize hands-on experience with healthcare data standards such as OMOP, FHIR, and clinical terminologies to understand and analyze patient journey data.; Spark NLP: Leverage John Snow Labs’ Spark NLP library and medical language models to develop clinical NLP software solutions."
D6Y9vrIo1mX-IXCqAAAAAA==,[],,"['Predictive Modeling', 'Statistical Analysis', 'Data Mining', 'Machine Learning', 'Data Visualization', 'SQL', 'Big Data Platforms', 'Business Intelligence Tools', 'Statistical Software Packages', 'Programming Languages', 'Continuous Improvement', 'Data Modeling']","Predictive Modeling: Develops predictive models to forecast business performance metrics and support key decisions to improve safety, employee engagement, operation efficiency, product quality, and customer satisfaction.; Statistical Analysis: Conducts advanced statistical analysis to determine trends and significant data relationships and proactively recommend areas of improvement.; Data Mining: Utilizes data mining techniques to discover insights from Big Data to help shape or meet specific business needs and goals.; Machine Learning: Implements machine learning methodologies including supervised and unsupervised learning and multivariate statistical analysis for modeling or analyses.; Data Visualization: Prepares and delivers insightful presentations and actionable recommendations, educating leaders and employees on complex analytical findings using storytelling and data visualization.; SQL: Uses advanced SQL queries to pull data from relational databases such as SAP BW, ORACLE, and SQL SERVER, working with data from multiple sources including Big Data platforms.; Big Data Platforms: Works with Big Data platforms including Hadoop, AWS, Azure, and Databricks to handle complex data sets.; Business Intelligence Tools: Champions self-service reporting capability and use of Business Intelligence tools such as SAP Business Intelligence to advance the organization's analytical capabilities.; Statistical Software Packages: Utilizes analytical packages and tools including R, SAS, SPSS, Stata, MATLAB, and Minitab for data mining and statistical analysis.; Programming Languages: Employs programming skills in SQL, C/C++/C#, Java, R, Python, PHP, ASP, or SAS to support data analytical or computer programming functions.; Continuous Improvement: Develops data and analytical processes based on Continuous Improvement learnings and practices to enhance analytics capabilities.; Data Modeling: Applies data modeling and data structure knowledge to develop complex data sets and analytical solutions."
LJAy9IiIZGK-jIXJAAAAAA==,"['Large Language Models', 'Generative AI', 'Prompt Engineering', 'Agentic Frameworks', 'Reinforcement Learning', 'Transfer Learning', 'Neural Language Generation', 'AI Agents']","Large Language Models: Designing, developing, implementing, and fine-tuning LLMs such as GPT and LLaMA to build autonomous sales agents and deliver personalized sales experiences with low margin of error.; Generative AI: Specialized practical experience in generative AI to create novel AI-driven solutions, including neural language generation and generative models for customer experience enhancement.; Prompt Engineering: Refining large language models through prompt engineering to improve model responses and task performance.; Agentic Frameworks: Utilizing agentic frameworks to develop autonomous AI agents capable of performing complex tasks in sales and tech support contexts.; Reinforcement Learning: Applying reinforcement learning techniques to fine-tune large language models and improve their decision-making capabilities.; Transfer Learning: Using transfer learning to adapt pre-trained large language models to specific business problems and domains.; Neural Language Generation: Employing neural language generation techniques as part of large language model architectures to produce human-like text for AI agents.; AI Agents: Building autonomous AI agents leveraging large language models to perform targeted and personalized sales tasks with high accuracy.","['Natural Language Processing', 'Machine Learning', 'Statistical Modeling', 'Data Analysis', 'Python', 'Hadoop Ecosystem (Spark/Hive)', 'Optimization Techniques', 'Experimental Design and A/B Testing']","Natural Language Processing: Used to analyze speech and text data, build chatbot and expert systems, and support the development of autonomous sales agents in tech support.; Machine Learning: Designing, deploying, and fine-tuning complex, optimized, and large-scale algorithms to improve customer experiences and support predictive modeling and decision making.; Statistical Modeling: Applying advanced statistical data modeling techniques, including hypothesis testing, experimental design, and time series analysis, to analyze complex, high-volume, and high-dimensional data.; Data Analysis: Gathering and analyzing large volumes of unstructured text data to evaluate scenarios, make predictions on future outcomes, and support decision making.; Python: Used for working with large-scale systems and implementing machine learning and data science solutions.; Hadoop Ecosystem (Spark/Hive): Knowledge of data processing in Hadoop programming environments to handle large-scale data.; Optimization Techniques: Familiarity with optimization methods underlying machine learning algorithms to improve model performance and solution efficiency.; Experimental Design and A/B Testing: Analyzing and interpreting results of product experiments to inform product decisions and improvements."
JbEvU8LZ6KUYqj1uAAAAAA==,[],,"['Predictive Modeling', 'Pricing Strategies', 'Feature Engineering', 'Supervised and Unsupervised Learning', 'A/B Testing and Controlled Experiments', 'Data Engineering', 'SQL, Python, and R', 'Machine Learning Pipelines', 'Statistical Modeling', 'Manufacturing and Supply Chain Analytics', 'Big Data Technologies', 'Cloud Platforms', 'Data-Driven Decision Making']","Predictive Modeling: Building and supporting predictive models to drive pricing and FSN strategies for manufacturing parts, including forecasting results with predictive outcomes.; Pricing Strategies: Designing, evaluating, and refining pricing strategies through data-driven approaches, including price elasticity modeling and demand forecasting.; Feature Engineering: Applying feature engineering techniques to analyze and interpret large, complex pricing-related datasets for improved pricing accuracy.; Supervised and Unsupervised Learning: Leveraging machine learning techniques such as supervised and unsupervised learning, classification, and regression to validate and refine pricing models.; A/B Testing and Controlled Experiments: Using A/B testing and controlled experiments along with statistical analysis to assess the business value of pricing models.; Data Engineering: Performing data cleaning, transformation, and feature engineering to prepare data for modeling and analysis.; SQL, Python, and R: Utilizing SQL, Python, R, or other statistical programming languages for data analysis, statistical modeling, and machine learning pipeline management.; Machine Learning Pipelines: Managing end-to-end machine learning pipelines and deploying models in production environments.; Statistical Modeling: Applying statistical modeling techniques in manufacturing parts pricing analysis, FSN, and POS data science within supply chain or manufacturing contexts.; Manufacturing and Supply Chain Analytics: Analyzing manufacturing parts pricing, FSN, POS data, and supply chain processes to uncover insights that impact product quality, yield, and company revenue.; Big Data Technologies: Familiarity with big data technologies such as Spark, Hadoop, and Databricks to handle large datasets relevant to pricing and manufacturing analytics.; Cloud Platforms: Experience with cloud platforms including AWS, GCP, or Azure to support data processing, storage, and model deployment.; Data-Driven Decision Making: Translating complex analytical findings into actionable insights and collaborating with cross-functional teams to define pricing roadmaps and priorities."
EOB9dCKQUnNyaM61AAAAAA==,[],,"['Data Pipelines', 'Feature Extraction', 'Data Exploration and Analysis', 'Data Models', 'Open-Source Data Analytics Tools', 'Database Technologies', 'Data Workflow Tools', 'Python', 'MATLAB', 'Linux Operating Environment', 'Containerization Technologies', 'Data Analytic Protocols and Standards']","Data Pipelines: Responsible for building and incorporating data pipelines and workflows to manage and process extremely large data sets for analysis by data analysts within the DoD customer’s organization.; Feature Extraction: Conducts feature extraction techniques as part of data exploration and analysis to support complex data analysis needs of the DoD customer’s organization.; Data Exploration and Analysis: Performs data exploration and analysis to extract key data points and support the development of data models and analytic protocols for the DoD customer.; Data Models: Designs data models based on requirements and needs to enable complex analysis by data analysts within the DoD customer’s organization.; Open-Source Data Analytics Tools: Investigates, determines, and implements suitable open-source tools for storage, management, and analysis of extremely large data sets to support the DoD customer’s data needs.; Database Technologies: Works with multiple types of datasets and database technologies, including industry standard cloud and on-premise database systems, to support data storage and retrieval requirements.; Data Workflow Tools: Defines requirements and implements data workflow tools and methodologies to automate and streamline data management and analysis processes for the DoD customer.; Python: Uses Python as a data exploration, analysis, and visualization tool to support data analytic tasks and workflows.; MATLAB: Utilizes MATLAB for data exploration, analysis, and visualization to support complex data analytic requirements.; Linux Operating Environment: Operates within a Linux environment for system configuration, capacity planning, and supporting large scale data analytics both on-premises and in cloud environments.; Containerization Technologies: Employs open-source containerization technologies and tools to optimize database technologies and analytics workflows, enhancing deployment and scalability.; Data Analytic Protocols and Standards: Develops, implements, and maintains data analytic protocols, standards, and documentation to ensure consistency and quality in data analysis processes."
s5Fs7HUVGS5OvSfKAAAAAA==,['Artificial Intelligence'],Artificial Intelligence: Led the execution and interpretation of AI models and AI-powered tools to enhance data-driven solutions and extract actionable insights for commercial strategies.,"['Advanced Analytics', 'Predictive Models', 'Machine Learning', 'SQL and NoSQL Databases', 'Big Data Stack', 'Python and R', 'Data Visualization Techniques', 'Pharmaceutical Data Sources', 'Data Science Models', 'Key Performance Indicators (KPIs)', 'Data Privacy and Security Compliance']","Advanced Analytics: Used to generate and deliver data-driven insights supporting brand, therapeutic area, or enterprise-wide decisions, including resource allocation, market mix measurement, channel optimization, and digital analytics.; Predictive Models: Developed and implemented to extract actionable insights that drive US Commercial strategies and tactics, including decision engines and next best action recommendations.; Machine Learning: Applied as part of advanced analytics techniques to develop data-driven solutions and predictive algorithms for business problem solving and innovation.; SQL and NoSQL Databases: Extensive expertise with traditional SQL and modern NoSQL data stores, including large-scale distributed systems such as Hadoop, Snowflake, and Databricks, to manage and analyze data.; Big Data Stack: Utilized alongside machine learning technologies and programming languages like Python and R to handle large-scale data processing and analytics.; Python and R: Used for machine learning, data analysis, and visualization techniques to support data science projects and deliver insights.; Data Visualization Techniques: Employed to communicate complex data science findings clearly and compellingly to diverse stakeholders.; Pharmaceutical Data Sources: Experience with IQVIA, SHS, Claims, and other syndicated resources, with a strong understanding of patient and claims data (APLD) to inform analytics and modeling.; Data Science Models: Extensively used to solve business problems in a commercial environment, supporting strategic decision-making and innovation.; Key Performance Indicators (KPIs): Defined and tracked to measure the success and impact of data-driven solutions and analytics initiatives.; Data Privacy and Security Compliance: Ensured throughout data science and AI projects to maintain high data integrity and regulatory adherence."
0X-dVx1GOIhD9Oj1AAAAAA==,[],,"['Data Mining', 'Statistical Analysis', 'Machine Learning', 'Natural Language Processing', 'Predictive Modeling', 'Recommendation Engines', 'A/B Testing', 'Big Data Technologies', 'Data Engineering', 'Data Platforms', 'Programming Languages and Tools', 'Pattern Recognition', 'Data Pipelines', 'Statistical Tools']","Data Mining: Used to analyze large amounts of data and build data analysis algorithms to extract insights and patterns for client decision-making.; Statistical Analysis: Applied through techniques like hypothesis testing, segmentation, and advanced statistical modeling to analyze data and support client recommendations.; Machine Learning: Includes building predictive models using algorithms such as k-NN, naive bayes, decision trees, and SVM to support data-driven client solutions.; Natural Language Processing: Utilized for analyzing unstructured data including sentiment analysis and text mining to derive insights from client data.; Predictive Modeling: Developed to help clients make data-driven decisions by forecasting outcomes based on structured and unstructured data.; Recommendation Engines: Implemented as part of building scoring systems and predictive models to enhance client solutions.; A/B Testing: Used to evaluate and optimize experimental platforms and data-driven business outcomes for clients.; Big Data Technologies: Hands-on experience with ecosystems such as Google, AWS, or Microsoft to design, build, and maintain scalable data solutions.; Data Engineering: Involves connecting data sources and structures using APIs, NoSQL, RDBMS, Hadoop, S3, SQL, Hive, Pig, and Blob Storage to unify and enrich client data.; Data Platforms: Leveraged and recommended in-house and new data platforms/solutions to meet and exceed client requirements for data unification and analysis.; Programming Languages and Tools: Experience with Python, R, Java, C#, Scala, and libraries such as Pandas to develop data science and analytics solutions.; Pattern Recognition: Applied to identify trends and insights within client data to support predictive modeling and decision-making.; Data Pipelines: Designed and maintained scalable and robust solutions to unify, enrich, and analyze data from multiple sources.; Statistical Tools: Used for data mining, modeling, and analysis to support client consulting and advisory services."
iqNcZ9-ZrCJwa25CAAAAAA==,"['Transformers and foundation models', 'Deep learning', 'Artificial intelligence']","Transformers and foundation models: Understanding of modern AI architectures relevant to the role, indicating exposure to advanced AI models.; Deep learning: Researching and implementing deep learning techniques to improve data analysis efficiency and model performance.; Artificial intelligence: Applying AI methods alongside machine learning to enhance data-driven decision-making and automation.","['Python', 'scikit-learn', 'Statistical modeling', 'Data visualization libraries', 'SQL', 'Pandas', 'NumPy', 'Jupyter notebooks', 'Time series modeling', 'Causal inference', 'Probabilistic forecasting models', 'Machine learning algorithms', 'Data repositories and reporting tools', 'Data exploration techniques and tools', 'Frequentist statistics and probability', 'Data engineering and ETL', 'Apache Spark, Flink, Nifi, Kafka', 'MLOps tools and frameworks', 'Tabular data analysis', 'Statistical analysis tools', 'Data pipelines', 'Data mining']","Python: Used extensively for coding, data analysis, and building visualization tools in the role.; scikit-learn: A commonly used machine learning library in Python for building and evaluating ML models.; Statistical modeling: Involves building statistical models to test hypotheses, interpret data, and support decision-making.; Data visualization libraries: Includes tools such as Plotly, Streamlit, and matplotlib used to create visual representations of data findings.; SQL: Used for querying and analyzing tabular data from databases and data warehouses.; Pandas: A Python library used for data manipulation and analysis, particularly with tabular data.; NumPy: A Python library used for numerical computing and data processing tasks.; Jupyter notebooks: An interactive environment used for developing and sharing code, data analysis, and visualizations.; Time series modeling: Applied to forecast and analyze temporal data relevant to financial planning and analysis.; Causal inference: Used to identify cause-effect relationships within data to inform business decisions.; Probabilistic forecasting models: Models that predict future outcomes with associated probabilities, supporting scenario analysis.; Machine learning algorithms: Applied to build predictive models and automate data-driven decision-making processes.; Data repositories and reporting tools: Used to store, manage, and report on large datasets to generate business insights.; Data exploration techniques and tools: Employed to investigate data characteristics, identify trends, and prepare data for modeling.; Frequentist statistics and probability: Statistical methods used for predictive modeling and hypothesis testing in the role.; Data engineering and ETL: Involves designing and maintaining data ingestion, transformation, and integration pipelines.; Apache Spark, Flink, Nifi, Kafka: Open source frameworks used for high-volume data ingestion, streaming, and processing on AWS Cloud.; MLOps tools and frameworks: Includes Kubeflow, MLflow, DVC, and TensorBoard used to manage machine learning lifecycle and model deployment.; Tabular data analysis: Analyzing structured data using SQL, R, and Python to extract insights and build models.; Statistical analysis tools: Includes C++, JavaScript, R, SAS, Excel, MATLAB, and SPSS used for various data analysis tasks.; Data pipelines: Designing and deploying data solutions to capture, transform, and utilize data for analytics and BI.; Data mining: Extracting patterns and relationships from large datasets to support business insights."
IAfRu9R8YRzc6ycbAAAAAA==,[],,"['Business Intelligence (BI) Tools and Dashboards', 'Data Warehousing', 'Healthcare Data Analytics', 'Data Integrity and Query Design', 'Financial and Performance Modeling', 'Stakeholder Engagement and Analytics Translation', 'Process Improvement Methodologies', 'Healthcare Regulatory Knowledge', 'Healthcare Systems and EMR Knowledge', 'Mentoring and Training']","Business Intelligence (BI) Tools and Dashboards: Used to develop and implement reporting and BI solutions that support strategic planning, decision-making, and performance measurement; includes creating new BI reports and interactive dashboards and training users on self-service BI tools.; Data Warehousing: Involves working closely with IT to improve an integrated data warehouse to ensure data and reporting consistency across the organization, supporting analytics and reporting needs.; Healthcare Data Analytics: Analyzing diverse healthcare data sources such as enrollment, claims, pharmacy, clinical, contract, medical management, financial, administrative, and other corporate data from both modeled and disparate internal and external sources to generate insights and support business opportunities.; Data Integrity and Query Design: Adhering to corporate standards for performance metrics, data collection, data integrity, query design, and reporting format to ensure high quality and meaningful analytic output.; Financial and Performance Modeling: Conducting financial modeling and performance analyses to gain insights into member/provider processes and business opportunities within healthcare settings.; Stakeholder Engagement and Analytics Translation: Acting as a liaison to understand stakeholder needs and translate them into reporting and analytic solutions, ensuring requests are triaged, recorded, and tracked effectively.; Process Improvement Methodologies: Applying advanced principles and methodologies of process improvement to support a process-focused approach in analytics and reporting responsibilities.; Healthcare Regulatory Knowledge: Utilizing knowledge of healthcare regulations such as New York State Medicaid and CMS Medicare reporting requirements (e.g., STARS, QARR, MMCOR, MEDS, RAPS, HEDIS) to inform analytics and reporting.; Healthcare Systems and EMR Knowledge: Experience with healthcare electronic medical records (EMR) systems like Epic/Clarity, aCW, and hospital/provider systems such as IDX and Soarian, as well as payor claims systems like Facets and Amisys, to support data analytics in healthcare contexts.; Mentoring and Training: Mentoring junior analysts and teaching others how to define meaningful process and performance measures, develop BI queries, and generate and use management reports effectively."
CwULlYgvqe3eUcVtAAAAAA==,"['Natural Language Processing', 'Deep Learning']",Natural Language Processing: Focus on NLP techniques applied to unstructured data as part of AI/ML model development to extract intelligence and support law enforcement analytics.; Deep Learning: Use deep learning methods including neural networks to develop AI models that enhance predictive capabilities and investigative insights.,"['Supervised Learning', 'Unsupervised Learning', 'Deep Learning Models', 'Statistical Modeling', 'Machine Learning Model Validation', 'Neural Networks', 'Decision Trees', 'Surrogate Models', 'Natural Language Processing', 'Data Lakes', 'Serverless Computing', 'ETL Processes', 'Data Visualization Tools', 'Analytical Programming Languages', 'Big Data Management']","Supervised Learning: Develop and support AI/ML models including supervised learning to analyze intelligence data with focus on accuracy and relevancy.; Unsupervised Learning: Deploy and manage unsupervised learning models to identify patterns, trends, and intelligence gaps in large datasets.; Deep Learning Models: Deploy, validate, and manage deep learning models such as neural networks to drive AI initiatives and predictive analytics.; Statistical Modeling: Perform advanced mathematical and statistical modeling to organize, optimize, and predict trends from structured and unstructured data.; Machine Learning Model Validation: Validate and improve machine learning models using rigorous statistical techniques to ensure high accuracy and precision.; Neural Networks: Design and implement neural network algorithms as part of AI initiatives to enhance predictive modeling and data analysis.; Decision Trees: Design and implement decision tree algorithms to support AI-driven business solutions and investigative analytics.; Surrogate Models: Develop surrogate models to support complex AI and machine learning tasks within data analysis workflows.; Natural Language Processing: Apply NLP techniques focused on unstructured data to extract insights and support AI/ML model development.; Data Lakes: Utilize Data Lakes for managing large-scale data storage and integration to support AI-driven business solutions.; Serverless Computing: Employ serverless computing technologies such as Athena and Lambda to manage data storage and computational needs efficiently.; ETL Processes: Design and implement ETL processes to prepare, clean, and manage big data sets for advanced analytics and AI applications.; Data Visualization Tools: Use tools like Power BI, Tableau, Elastic X-Pack, and Qlik to translate complex data insights into actionable intelligence for stakeholders.; Analytical Programming Languages: Leverage programming languages such as Python, R, Scala, and Closure for statistical analysis, data manipulation, and model development.; Big Data Management: Expertise in data architecture and big data management to handle extensive and diverse data sources for analytics and AI."
npE7ACWV6keCaLZQAAAAAA==,"['Artificial Intelligence', 'Deep Learning']",Artificial Intelligence: Applied alongside machine learning and deep learning techniques to enhance algorithm performance and develop impactful solutions in digital pathology.; Deep Learning: Used as part of image analysis and algorithm development to improve diagnostic accuracy and automate complex tasks in digital pathology.,"['Machine Learning', 'Image Analysis', 'Predictive Modeling', 'Python Programming', 'Data Integration and Integrity', 'Graphical User Interface (GUI) Development', 'Data Modeling and Processing', 'High-Performance Computing (HPC)', 'Cloud Computing Platforms']","Machine Learning: Used to develop and improve algorithms for digital pathology projects, including predictive modeling and data analysis on large-scale datasets.; Image Analysis: Applied to process and analyze digital pathology images to support cancer diagnostics and algorithm development.; Predictive Modeling: Performed on large-scale digital pathology datasets to build computational tools that aid in diagnostics and research.; Python Programming: Primary language used to build, test, and deploy computational tools and algorithms, ensuring code quality and maintainability.; Data Integration and Integrity: Ensuring seamless integration and maintaining the accuracy and traceability of complex digital pathology data and associated metadata.; Graphical User Interface (GUI) Development: Developed for technical applications and data visualization to improve usability and interaction with computational tools.; Data Modeling and Processing: Conducted complex data modeling and processing on large or intricate datasets to support algorithm development and analysis.; High-Performance Computing (HPC): Utilized for computationally intensive tasks related to image processing and algorithm development in digital pathology.; Cloud Computing Platforms: Experience with AWS, Azure, or GCP to support scalable computing and data storage needs for digital pathology projects."
HD_Y3R_tuV5zgVAeAAAAAA==,"['Generative AI', 'Large Language Models (LLMs)', 'LLMOps']","Generative AI: Experience with generative AI technologies and frameworks such as LLamaIndex and LangChain to build AI-driven solutions.; Large Language Models (LLMs): Familiarity with LLMs and their application in AI infrastructure and solutions, including managing and deploying these models.; LLMOps: Understanding of operations specifically related to managing and deploying large language models and AI pipelines.","['Machine Learning', 'Statistical Analysis', 'Feature Engineering', 'Data Pipelines', 'SQL', 'Python', 'Spark', 'Scikit-learn', 'XGBoost', 'SparkML', 'Deep Learning Frameworks', 'ML Model Deployment', 'CI/CD Pipelines', 'Infrastructure as Code', 'Advanced Machine Learning Techniques', 'Data Visualization', 'MLOps']","Machine Learning: Utilized to build high quality data science models that solve client business problems and to develop and deploy ML models across various platforms such as Azure, AWS, GCP, and Databricks.; Statistical Analysis: Applied as part of delivering solutions that extract insights and support decision sciences within client engagements.; Feature Engineering: Designing and developing feature engineering pipelines to prepare data for machine learning and analytical models.; Data Pipelines: Building and orchestrating pipelines that support feature engineering and model deployment as part of end-to-end analytical solutions.; SQL: Writing code in SQL to manipulate and query data as part of data science workflows following software engineering best practices.; Python: Used for coding in data science projects, including building machine learning models and feature engineering pipelines.; Spark: Utilized for big data processing and coding within data science projects, leveraging Databricks features.; Scikit-learn: Familiarity with this traditional machine learning tool is required for building and deploying ML models.; XGBoost: Experience with this gradient boosting framework is expected for developing machine learning models.; SparkML: Used as a machine learning library within Spark for building scalable ML models.; Deep Learning Frameworks: Experience with TensorFlow and PyTorch for advanced machine learning techniques including computer vision and natural language processing.; ML Model Deployment: Knowledge of deploying ML models using options such as Azure Functions, FastAPI, and Kubernetes for real-time and batch processing.; CI/CD Pipelines: Experience with continuous integration and continuous deployment pipelines like DevOps pipelines and GitHub Actions to automate ML model deployment.; Infrastructure as Code: Knowledge of tools such as Terraform, ARM Template, and Databricks Asset Bundles to manage infrastructure supporting data science and ML workflows.; Advanced Machine Learning Techniques: Understanding of graph-based processing, computer vision, natural language processing, and simulation modeling to solve complex data science use-cases.; Data Visualization: Utilized to communicate analytical insights effectively to stakeholders and clients.; MLOps: Understanding of machine learning operations to manage the lifecycle of ML models and infrastructure."
Db5tG7QmN3FBIZ98AAAAAA==,"['Generative AI', 'AI Algorithms']","Generative AI: Designing, building, and implementing advanced generative AI solutions to augment decision making in healthcare and life sciences, including helping customers adopt generative AI methods and build applications leveraging GenAI.; AI Algorithms: Collaborating with AI/ML scientists and architects to research, design, develop, and evaluate AI algorithms that address real-world challenges in healthcare and life sciences.","['Healthcare and Life Sciences Data', 'Machine Learning', 'Deep Learning Frameworks', 'Python Programming']","Healthcare and Life Sciences Data: Experience working with diverse healthcare and life sciences data sources such as EHR, HL7/FHIR, insurance claims, genomics, and medical imaging to build models and data-intensive applications at scale.; Machine Learning: Building models for business applications using machine learning techniques including algorithms, data structures, numerical optimization, data mining, parallel and distributed computing, and high-performance computing.; Deep Learning Frameworks: Hands-on experience building models using deep learning frameworks such as TensorFlow, Keras, PyTorch, MXNet, and JAX to develop neural deep learning methods.; Python Programming: Using Python as a primary programming language for developing machine learning and deep learning models."
h902T2VUZpEt8OGEAAAAAA==,"['Large Language Models', 'Retrieval-Augmented Generation', 'AI-Enabled Frontend Development']","Large Language Models: Familiarity with open source or closed LLMs is required, indicating use or integration of advanced AI language models.; Retrieval-Augmented Generation: Knowledge of RAG architecture suggests involvement with AI systems that combine retrieval of external information with generative language models.; AI-Enabled Frontend Development: Experience with frontend web app development using frameworks like Flask and Gradio, which are often used to build interfaces for AI models.","['Statistical Techniques', 'Machine Learning', 'Azure Databricks', 'Power BI', 'Python', 'Predictive Models']",Statistical Techniques: Used to analyze and interpret datasets to extract meaningful insights.; Machine Learning: Applied to build predictive models and machine-learning algorithms for data analysis.; Azure Databricks: Utilized as a data science tool for processing and analyzing large datasets.; Power BI: Used to prepare and deliver analysis results through interactive dashboards and reports.; Python: Employed as a programming language for data analysis and building machine learning models.; Predictive Models: Developed to forecast outcomes based on historical data.
dda2-nHv7bZhJWViAAAAAA==,[],,"['Big Data Platform Management', 'Programming Languages', 'Data Visualization', 'Data Processing', 'Mathematical, Computational, and Statistical Foundations', 'Modeling, Inference, and Prediction', 'Analytic Modeling and Statistical Analysis', 'Data Cleaning and Transformation', 'Machine Learning', 'Statistical Analysis Techniques', 'Data Mining', 'Data Modeling and Assessment', 'Software Engineering']","Big Data Platform Management: Involves managing large-scale data platforms to support data processing and analytics tasks as part of the data scientist role.; Programming Languages: Utilized for developing prototype algorithms, algorithm refinements, and supporting data visualization and analytics.; Data Visualization: Used to represent data graphically to aid in analysis and communication of insights.; Data Processing: Includes data management, curation, description, visualization, workflow, and reproducibility to handle large datasets effectively.; Mathematical, Computational, and Statistical Foundations: Provides the theoretical basis for data modeling, inference, and prediction tasks.; Modeling, Inference, and Prediction: Involves data modeling and assessment with domain-specific considerations to extract meaningful insights from data.; Analytic Modeling and Statistical Analysis: Used to develop and implement qualitative and quantitative methods for characterizing, exploring, and assessing large datasets.; Data Cleaning and Transformation: Processes applied to prepare data for analysis by improving data quality and structure.; Machine Learning: Designing and implementing algorithms to enable predictive modeling and data-driven decision making.; Statistical Analysis Techniques: Includes variability analysis, sampling error, inference, hypothesis testing, exploratory data analysis, and application of linear models.; Data Mining: Techniques used to discover patterns and relationships in large datasets.; Data Modeling and Assessment: Creating representations of data structures and evaluating their suitability for analysis.; Software Engineering: Applying programming and development skills to build and maintain data science solutions."
BQFGF4-a8emlvysjAAAAAA==,[],,"['Statistical Methods', 'Bidding Optimization', 'Data Extraction and Compilation', 'Data Validation and Formatting', 'Custom Data Infrastructure and Data Models', 'Mathematical Modeling', 'Cross-functional Collaboration', 'SQL', 'Python', 'R', 'Constrained Optimization Theory', 'Ads Systems Knowledge']","Statistical Methods: Used to analyze data, identify opportunities, and tackle product creation, development, and improvement while considering end user behaviors.; Bidding Optimization: Managing bidding processes to ensure exceptional advertiser experiences and contribute to business growth.; Data Extraction and Compilation: Gathering, extracting, and compiling data across sources using tools such as SQL, R, and Python.; Data Validation and Formatting: Independently formatting, restructuring, and validating data to ensure quality and readiness for analysis.; Custom Data Infrastructure and Data Models: Using specialized knowledge to apply custom or existing data models and infrastructure for analysis.; Mathematical Modeling: Designing and evaluating models to mathematically express and solve defined problems with limited precedent.; Cross-functional Collaboration: Working with engineers, product managers, sales associates, and marketing teams to adjust practices based on data findings and clarify business or product questions.; SQL: Used for querying databases as part of data extraction and analysis.; Python: Used for coding, data extraction, and analysis tasks.; R: Used for coding, statistical analysis, and data extraction.; Constrained Optimization Theory: Applied knowledge in optimization theory relevant to improving advertiser experiences and business outcomes.; Ads Systems Knowledge: Understanding of auction, serving, and ads quality systems relevant to advertiser optimization."
wAr7PaXYId54vgswAAAAAA==,['Generative AI'],"Generative AI: Recognized as a potential tool to improve team efficiency and processes, with basic usage of GenAI tools like ChatGPT and Claude mentioned for integration into product strategy.","['Statistical and Quantitative Modeling', 'SQL', 'R', 'Python', 'ETL Frameworks', 'Data Transformation and Validation', 'BI Dashboards (Looker, Tableau)', 'Data Pipelines', 'Data-Driven Experimentation', 'Feature Engineering', 'Data Mining, Clustering, and Segmentation', 'dbt (Data Build Tool)']","Statistical and Quantitative Modeling: Used to analyze large healthcare datasets and conduct data mining, clustering, and segmentation to derive insights relevant to healthcare analytics.; SQL: Employed for querying and managing data within the company's data warehouse to support data transformation and analysis.; R: Utilized as a data science tool for statistical analysis and visualization in healthcare analytics.; Python: Used for programming, data analysis, and building data transformation pipelines to support data-driven decision making.; ETL Frameworks: Applied to extract, transform, and load clinical, member, and claims data from the data warehouse into usable formats for analysis and reporting.; Data Transformation and Validation: Involves designing and building data flows and pipelines to ensure data quality and readiness for analysis and reporting.; BI Dashboards (Looker, Tableau): Used to build interactive dashboards and reports that answer real business questions and monitor KPIs and product metrics.; Data Pipelines: Built and maintained to support scalable systems and complex data structures, enabling end-to-end data processing and actionable recommendations.; Data-Driven Experimentation: Leading and enabling frequent, small experiments to speed learning and inform product decisions through data analysis.; Feature Engineering: Implied through the process of translating vague product hunches into sharp analytical questions and designing approaches to answer them.; Data Mining, Clustering, and Segmentation: Used to analyze healthcare data for identifying patterns and grouping similar data points to support strategic decision making.; dbt (Data Build Tool): Used for building data transformation pipelines to support scalable and maintainable data workflows."
fC1QkJ4uJ6xxBy5dAAAAAA==,['AI Solution Optimization'],AI Solution Optimization: Optimizing AI solutions for scalability to ensure efficient and effective deployment in production environments.,"['Data Analytics', 'Big Data Tools and Frameworks']",Data Analytics: Transforming raw data into valuable business insights to support data-driven decision-making.; Big Data Tools and Frameworks: Managing big data tools and frameworks to handle large-scale data processing and analysis.
cNQzFnQ5KGZiafZLAAAAAA==,[],,"['Data Pipelines', 'Data Analysis', 'Quantitative Models', 'Data Validation and Quality Assurance', 'Python Data Libraries', 'Database Management', 'Large-Scale Data Processing', 'Cloud Computing Environments']","Data Pipelines: Design, develop, and launch efficient and reliable data pipelines to move, analyze, and model data from large and complex datasets.; Data Analysis: Perform data curation, data evaluation, and data analysis to discover insights and support solution development.; Quantitative Models: Design, create, and implement quantitative models to analyze data and provide actionable insights.; Data Validation and Quality Assurance: Validate and ensure quality of data, models, and results to maintain reliability and accuracy.; Python Data Libraries: Use Python with data handling libraries such as Pandas and PyArrow for data manipulation and processing.; Database Management: Maintain strong background in database management solutions, including familiarity with MySQL and Oracle databases.; Large-Scale Data Processing: Implement batch processing pipelines for large-scale data processing in HPC or cloud architectures.; Cloud Computing Environments: Experience with cloud computing platforms such as AWS, Google Cloud, and Azure is considered a plus."
fZCgCsjP2DX7ZTUXAAAAAA==,"['Generative AI', 'Natural Language Processing', 'Large Language Models']","Generative AI: The role involves hands-on experience designing and deploying generative AI solutions, indicating development and implementation of AI models that generate content or data.; Natural Language Processing: The job requires developing roadmaps and strategies for NLP, focusing on processing and understanding human language as part of AI model development.; Large Language Models: Involves strategy and lifecycle implementation for LLMs, indicating work with advanced AI models that understand and generate human-like text.","['Python', 'R', 'Shiny']","Python: Used for coding and development in data science tasks as part of the job requirements.; R: Used for coding and development in data science tasks as part of the job requirements.; Shiny: Used for building interactive web applications in R, supporting data visualization and analytics."
TJmvQbs8fjvZtjc3AAAAAA==,[],,"['Quantitative Analysis', 'Data Mining', 'Data-Driven Storytelling', 'Hypothesis Development and Testing', 'Product and Business Analytics', 'Team Leadership in Data Science']","Quantitative Analysis: Used to develop data-informed strategies for growing and improving product offerings by applying rigorous numerical and statistical methods.; Data Mining: Applied to extract useful patterns and insights from large datasets to inform product and business decisions.; Data-Driven Storytelling: Involves presenting data insights in a compelling narrative form to influence partners and drive strategic decisions.; Hypothesis Development and Testing: Focuses on formulating hypotheses and employing a broad toolkit of analytical methodologies and frameworks to rigorously test them.; Product and Business Analytics: Used to shape product development, quantify new opportunities, identify challenges, and ensure products deliver value to users and the business.; Team Leadership in Data Science: Involves inspiring, leading, and growing distributed teams of data scientists and leaders to drive analytics initiatives and data-informed decision-making."
uodVhdEIF9g9sV9JAAAAAA==,[],,"['Statistical Analysis', 'Machine Learning Models', 'A/B Testing', 'Data Infrastructure and Pipelines', 'SQL and NoSQL Databases', 'Data Warehouses', 'Python Programming', 'Machine Learning Libraries']","Statistical Analysis: Used to analyze large data sets to extract insights and support decision-making in the product context.; Machine Learning Models: Building, evaluating, and deploying models to improve user experience and product performance.; A/B Testing: Running and monitoring experiments to evaluate the impact of changes and optimize product features.; Data Infrastructure and Pipelines: Maintaining, configuring, monitoring, and orchestrating data infrastructure and pipelines to ensure reliability and scalability based on application needs and growth.; SQL and NoSQL Databases: Utilizing SQL and NoSQL databases such as MongoDB or DynamoDB for data storage and retrieval.; Data Warehouses: Experience with data warehouses like Snowflake, BigQuery, and Redshift to manage large-scale data storage and analytics.; Python Programming: Using Python as a primary programming language for data analysis, model development, and infrastructure tasks.; Machine Learning Libraries: Exposure to libraries such as scikit-learn, LightGBM, and XGBoost for implementing machine learning algorithms."
lNUvseFlY2wc0alQAAAAAA==,['AI-augmented Decision Support'],AI-augmented Decision Support: Leverage AI techniques to enhance decision-making processes related to sensor data analysis and operational support.,"['Image Processing', 'Sensor Calibration', 'Data Simulation and Modeling', 'Statistical Inference', 'Machine Learning', 'Predictive Models and Advanced Algorithms', 'Data Analysis and Reduction Procedures', 'Programming in Python and Matlab', 'Data Modeling', 'Physics-based and Phenomenological Simulation Tools']","Image Processing: Develop algorithms related to processing images from sensors to extract meaningful information and improve data quality.; Sensor Calibration: Create and apply algorithms to calibrate sensors ensuring accurate and reliable data collection from imaging systems.; Data Simulation and Modeling: Develop and use simulation tools and models to replicate sensor data and physical systems for analysis and algorithm testing.; Statistical Inference: Apply statistical concepts and techniques to analyze sensor data and support decision-making processes.; Machine Learning: Use machine learning methods for target detection, discrimination, and predictive modeling to extract value from sensor data.; Predictive Models and Advanced Algorithms: Design, develop, and evaluate models and algorithms that predict outcomes and optimize data value extraction.; Data Analysis and Reduction Procedures: Perform standard data analysis and reduction techniques to process and interpret sensor and imaging data.; Programming in Python and Matlab: Utilize Python and Matlab programming languages for scientific computing, algorithm development, and data analysis.; Data Modeling: Apply data modeling techniques to represent and analyze sensor and imaging data effectively.; Physics-based and Phenomenological Simulation Tools: Use simulation tools based on physical principles and phenomenological models to support sensor data analysis and system understanding."
YLxEIPUAI7fhR_mYAAAAAA==,"['Deep Learning', 'Natural Language Processing', 'AI Model Fine-Tuning']",Deep Learning: Apply current and emerging deep learning techniques to develop AI models for gameplay and content creation.; Natural Language Processing: Utilize natural language processing methods as part of AI solutions integrated into gaming products and services.; AI Model Fine-Tuning: Optimize and fine-tune AI models for improved performance and scalability within production environments.,"['Machine Learning Models', 'Data Collection and Management', 'SQL', 'Python', 'Data Visualization', 'DevOps Tools and Principles', 'Cloud Environments', 'Containerization and Orchestration']","Machine Learning Models: Build, fine-tune, and implement machine learning models to solve challenging problems and improve AI models' performance and scalability.; Data Collection and Management: Collect, clean, manage, analyze, and visualize large datasets using multiple data platforms, tools, and techniques to extract meaningful insights.; SQL: Use SQL for data querying and management within cloud environments and data platforms.; Python: Utilize Python programming language for data science tasks including model development, data analysis, and automation of machine learning models.; Data Visualization: Visualize large sets of data to communicate insights and support decision-making processes.; DevOps Tools and Principles: Apply DevOps tools and principles such as Git, CI/CD pipelines, and Docker to automate machine learning model deployment and ensure scalable production pipelines.; Cloud Environments: Work effectively within cloud platforms like Google Cloud Platform (GCP) and Amazon Web Services (AWS) to deploy and manage data and machine learning workflows.; Containerization and Orchestration: Use containerization and orchestration technologies such as Docker and Kubernetes to support scalable and repeatable machine learning production pipelines."
-XlZLA8_bDy1vpVpAAAAAA==,"['Deep Learning Frameworks', 'Natural Language Processing with Deep Learning', 'Computer Vision with Deep Learning']",Deep Learning Frameworks: Using TensorFlow and PyTorch specifically for developing and fine-tuning deep learning architectures in computer vision and NLP applications.; Natural Language Processing with Deep Learning: Applying deep learning-based NLP techniques such as language modeling and word embeddings to build AI solutions.; Computer Vision with Deep Learning: Leveraging deep learning architectures for advanced computer vision tasks including object detection and image segmentation.,"['Machine Learning', 'Deep Learning', 'Computer Vision', 'Natural Language Processing', 'Predictive Analytics', 'Training Data Development', 'Model Evaluation and Assessment', 'Data Manipulation and Analysis', 'Data Visualization', 'Python Programming', 'Machine Learning Frameworks', 'Computer Vision Libraries', 'Data Science']","Machine Learning: Developing machine learning solutions to address business challenges and drive strategic decision-making in construction planning technology.; Deep Learning: Optimizing and fine-tuning deep learning models to improve performance, accuracy, and efficiency, particularly for computer vision and NLP tasks.; Computer Vision: Applying computer vision techniques such as classification, object detection, and image segmentation to construction industry problems.; Natural Language Processing: Utilizing NLP methods including text preprocessing, word embeddings, and language modeling to build models and develop training data.; Predictive Analytics: Using predictive analytics to support data-driven insights and business growth.; Training Data Development: Curating high-quality training datasets to support machine learning and AI model development.; Model Evaluation and Assessment: Conducting thorough evaluations and assessments of models to provide recommendations for enhancements and optimizations.; Data Manipulation and Analysis: Using Python libraries such as Pandas and NumPy for data manipulation and analysis.; Data Visualization: Employing visualization tools like Matplotlib to present data insights effectively.; Python Programming: Strong programming skills in Python to support data manipulation, analysis, visualization, and model development.; Machine Learning Frameworks: Proficiency with frameworks such as scikit-learn, TensorFlow, and PyTorch for building and deploying machine learning models.; Computer Vision Libraries: Familiarity with libraries like OpenCV and scikit-image to implement computer vision techniques.; Data Science: Applying data science principles and statistical methods to solve complex problems in the construction industry."
FlHGei9nG1MoCK2bAAAAAA==,['TensorFlow'],"TensorFlow: Used as a machine learning library, potentially for building neural network models related to music content analysis and recommendation, though not explicitly described as deep learning in this context.","['A/B Testing', 'Recommender Systems', 'Machine Learning Systems', 'Music Information Retrieval', 'SQL', 'Distributed Computing Systems', 'Python', 'Scala', 'Java', 'Scikit-learn']","A/B Testing: Used to design, build, and evaluate improvements and innovations to algorithmic radio, playlist, and recommendation products by comparing different versions to determine effectiveness.; Recommender Systems: Designing and building systems that suggest music content to listeners based on their listening history and preferences.; Machine Learning Systems: Developing and implementing machine learning models to support music information retrieval and recommendation tasks.; Music Information Retrieval: Analyzing audio and metadata to improve content understanding capabilities related to music characteristics and listener preferences.; SQL: Utilized for querying and managing large-scale music listening data and metadata.; Distributed Computing Systems: Experience with systems like Spark to process and analyze large volumes of music listening history and metadata efficiently.; Python: Programming language used for data analysis, machine learning model development, and building data pipelines.; Scala: Programming language used for working with distributed computing systems and data processing.; Java: Programming language used for building scalable data and machine learning systems.; Scikit-learn: A machine learning library used for building and experimenting with machine learning models related to music recommendation and analysis."
tWRA07qJ9cETDxqgAAAAAA==,['TensorFlow'],"TensorFlow: TensorFlow is preferred for machine learning roles, indicating use of deep learning frameworks for building and training neural networks.","['Statistics', 'SAS', 'Python', 'Computer Vision', 'Data Visualization Tools', 'NLP', 'Machine Learning']","Statistics: Knowledge of statistics is required for data science and machine learning positions to analyze and interpret data effectively.; SAS: Experience with SAS is mentioned as a required skill for data science roles, indicating its use for statistical analysis and data management.; Python: Python programming language is required for data science and machine learning roles, supporting tasks such as data analysis, visualization, and model development.; Computer Vision: Knowledge of computer vision is preferred, suggesting involvement with image or video data processing in data science or machine learning projects.; Data Visualization Tools: Experience with data visualization tools like Tableau and PowerBI is preferred to create dashboards and visual representations of data insights.; NLP: Natural Language Processing is a preferred skill, indicating work with text mining and analysis in data science projects.; Machine Learning: Machine learning is a core skill for the data science positions, involving building predictive models and applying algorithms to data."
_5rM8LBIEtUYvKx8AAAAAA==,"['Generative AI', 'Large Language Models', 'Retrieval-Augmented Generation', 'Prompt Engineering', 'Transformers', 'PyTorch (Deep Learning)', 'AWS SageMaker', 'AI Strategy and Development']","Generative AI: The role involves working on generative AI use cases, including developing solutions and services related to large language models and generative AI technologies.; Large Language Models: Experience with LLMs is required, including developing use cases and solutions involving these models.; Retrieval-Augmented Generation: Developing RAG solutions, tools, and services is part of the job, involving integration of retrieval mechanisms with generative AI models.; Prompt Engineering: The job includes responsibilities related to designing and optimizing prompts for generative AI models to improve performance and relevance.; Transformers: Transformers are applied as part of deep learning techniques in AI/ML projects, especially related to NLP and generative AI.; PyTorch (Deep Learning): PyTorch is specifically used for implementing deep learning models, including neural networks such as CNNs, RNNs, and GANs, within AI projects.; AWS SageMaker: AWS SageMaker is used to develop, train, and deploy AI/ML models, including generative AI and LLM workloads.; AI Strategy and Development: The role includes guiding clients in AI strategy, development, and deployment of AI services to meet organizational needs and maximize business impact.","['Machine Learning', 'Deep Learning', 'Python', 'PyTorch', 'Natural Language Processing', 'Time-Series Analysis', 'Computer Vision', 'Kubernetes', 'Docker', 'TensorRT', 'RAPIDs', 'Kubeflow', 'MLflow', 'Cloud Platforms', 'Exploratory Data Analysis', 'Model Validation and Testing', 'Data Strategy']","Machine Learning: The role involves developing and deploying machine learning algorithms and models, including traditional ML techniques and model tuning and validation in production environments.; Deep Learning: The job requires applying deep learning techniques such as CNNs, RNNs, and GANs across real-world projects, including model tuning and performance validation.; Python: Python is used as a core data science language for AI/ML algorithm development and data analysis.; PyTorch: PyTorch is used as a framework for AI/ML algorithm development and deep learning model implementation.; Natural Language Processing: NLP is applied as part of data analysis tasks within AI/ML algorithm development.; Time-Series Analysis: Time-series analysis is used as a data analysis technique in AI/ML algorithm development.; Computer Vision: Computer vision techniques are applied as part of AI/ML algorithm development and data analysis.; Kubernetes: Kubernetes is used for deploying and optimizing machine learning models in production environments.; Docker: Docker is utilized for containerizing and deploying machine learning models.; TensorRT: TensorRT is employed for optimizing machine learning models for inference performance.; RAPIDs: RAPIDs is used to accelerate machine learning model deployment and optimization.; Kubeflow: Kubeflow is used to deploy and manage machine learning workflows and models.; MLflow: MLflow is used for managing the machine learning lifecycle, including experimentation, reproducibility, and deployment.; Cloud Platforms: Cloud environments such as AWS, Azure, and GCP are leveraged to deploy AI/ML workloads and support scalable model deployment.; Exploratory Data Analysis: Exploratory data analysis is performed to understand client data sets and inform model development and long-term solution design.; Model Validation and Testing: The role includes validating AI models and algorithms through code reviews, unit tests, and integration tests to ensure quality and performance.; Data Strategy: Defining data strategy is a key responsibility to align technical development with client operational requirements and business objectives."
i0Qyb4YESwPo3TNeAAAAAA==,"['Large Language Models', 'Ablation Studies', 'Model Distillation', 'AI Research']","Large Language Models: In-depth experience and familiarity with training and working with large language models (LLMs) and agents, including staying up-to-date with the latest advancements in LLM research.; Ablation Studies: Use of ablation techniques to evaluate and improve the effectiveness of synthetic data creation and data quality methods in pretraining, post-training, and distillation.; Model Distillation: Application of distillation techniques as part of the evaluation and improvement process for synthetic data effectiveness in model training.; AI Research: Engagement in cutting-edge AI research, demonstrated by a strong publication record in top machine learning conferences and a passion for advancing pretraining data methodologies.","['Synthetic Data', 'Data Quality', 'Pretraining Data']","Synthetic Data: Research and development of methods to create diversified high-quality synthetic data and scale its creation through collaborations, with evaluation and improvement of its effectiveness in pretraining, post-training, and distillation phases.; Data Quality: Research and development of methods to identify and fix quality issues horizontally in the pretraining data corpus, including evaluation and improvement of these methods through ablation studies.; Pretraining Data: Focus on organizing and curating high-quality tokens for core model training, with responsibilities including improving the quality and effectiveness of pretraining data."
nb_X9MFGMA5UnNR0AAAAAA==,"['Generative AI', 'Large Language Models', 'Prompt Engineering', 'Deep Learning Frameworks for AI']","Generative AI: Developing application-specific interfaces leveraging generative AI capabilities to enhance associate and customer experiences.; Large Language Models: Utilizing LLMs and foundation models (FMs) as part of AI solutions to improve user interactions and business processes.; Prompt Engineering: Translating user requirements into AI solutions involving prompt design and optimization for generative AI models.; Deep Learning Frameworks for AI: Using frameworks such as TensorFlow, PyTorch, and Keras specifically for building and deploying neural network-based AI models including NLP and computer vision.","['Statistical Methods', 'Machine Learning', 'Deep Learning', 'Natural Language Processing', 'Computer Vision', 'Feature Engineering and Data Integration', 'Python and R', 'Databases', 'Cloud Environments', 'Data Visualization', 'Ethical Data Science Practices', 'ServiceNow and Salesforce Integration']","Statistical Methods: Used for data analysis, exploratory data analysis (EDA), hypothesis testing, and drawing meaningful conclusions to support business decisions.; Machine Learning: Building analytical and statistical machine learning models including supervised, unsupervised, and reinforcement learning algorithms for classification, regression, clustering, predictive and prescriptive modeling, and automated root cause analysis.; Deep Learning: Applied deep learning methods to solve business problems including model training and development using frameworks like TensorFlow, PyTorch, and Keras.; Natural Language Processing: Applied NLP techniques such as sentiment analysis, topic modeling, and graph theory in real-world applications to extract insights and solve business problems.; Computer Vision: Utilized computer vision methods as part of machine learning applications to address business challenges.; Feature Engineering and Data Integration: Experience reading data from multiple sources including databases, files, APIs, and server logs, cleansing, enhancing, and integrating data for analysis and model building.; Python and R: Extensive use of Python libraries and R for univariate and bivariate analysis, building advanced machine learning models, and data science tasks.; Databases: Experience with relational databases such as Postgres, Redshift, and MSSQL for data storage, querying, and integration.; Cloud Environments: Knowledge and experience working with cloud platforms including Azure, AWS, and Google Cloud Platform for data storage, processing, and deployment of data science solutions.; Data Visualization: Training machine learning models to communicate complex data insights through clear and informative visualizations.; Ethical Data Science Practices: Incorporating bias detection, fairness, and responsible data handling into data science models and workflows.; ServiceNow and Salesforce Integration: Enabling AI and analytics solutions within core technologies such as ServiceNow and Salesforce to enhance internal and external user experiences."
ON9FoPBRATzj3HmNAAAAAA==,[],,"['Feature Engineering', 'Predictive Modeling', 'Anomaly Detection', 'Dashboards and Reporting', 'SQL and BigQuery', 'Python and Pandas', 'Machine Learning Tooling', 'Data Pipelines and Workflow Orchestration', 'Real-time Data Streaming', 'User Segmentation and Lifetime Value Modeling', 'Looker and dbt']","Feature Engineering: Responsible for building data pipelines that transform raw bet, player, and market data into features used for pricing and exposure models.; Predictive Modeling: Train, validate, and deploy supervised and probabilistic models to forecast player performance, market volatility, and user value.; Anomaly Detection: Develop rule-based limiters and anomaly-detection jobs that run frequently to flag and throttle outlier exposure, preventing tail risk.; Dashboards and Reporting: Build Grafana dashboards and SQLX reports to surface live liability, promotional uptake, and key performance indicators for trading and executive stakeholders.; SQL and BigQuery: Use SQLX and BigQuery for data querying and transformation within feature engineering and reporting pipelines.; Python and Pandas: Utilize Python and Pandas for data manipulation, feature engineering, and model development.; Machine Learning Tooling: Apply modern machine learning tools such as scikit-learn and XGBoost for model training and tuning.; Data Pipelines and Workflow Orchestration: Use Airflow or similar tools to manage and automate data workflows and model pipelines.; Real-time Data Streaming: Experience with real-time data streams and event-driven architectures using Kafka or Pub/Sub to support timely data processing and alerting.; User Segmentation and Lifetime Value Modeling: Build user-level segmentation and lifetime value (LTV) models to support business insights and decision-making.; Looker and dbt: Experience with analytics stacks such as Looker and dbt for data modeling, transformation, and visualization."
fqFG2JFX5aevR7w_AAAAAA==,['TensorFlow'],"TensorFlow: TensorFlow is a preferred skill, indicating use of this deep learning framework for building and training neural network models.","['Statistics', 'SAS', 'Python', 'Data Visualization Tools', 'NLP', 'Text Mining', 'Tableau', 'Power BI', 'Databricks', 'Machine Learning', 'Computer Vision', 'Java Programming']","Statistics: Knowledge of statistics is required for data science and machine learning positions to analyze and interpret data effectively.; SAS: Experience with SAS is mentioned as a required skill, indicating its use for statistical analysis or data management in the role.; Python: Python programming is required, likely for data analysis, scripting, and implementing machine learning models.; Data Visualization Tools: Familiarity with data visualization tools is preferred, suggesting responsibilities involving creating visual representations of data insights.; NLP: Natural Language Processing is a preferred skill, indicating potential work with text mining or language data analysis.; Text Mining: Text mining is listed as a preferred skill, implying analysis and extraction of information from text data.; Tableau: Tableau is a preferred data visualization tool mentioned for creating interactive dashboards and reports.; Power BI: Power BI is another preferred business intelligence tool for data visualization and reporting.; Databricks: Databricks is preferred, indicating use of this platform for big data processing and collaborative data science workflows.; Machine Learning: Machine learning is a key focus area, with candidates expected to have knowledge or experience in this field.; Computer Vision: Computer vision knowledge is required, suggesting work involving image or video data analysis.; Java Programming: Experience in Java programming is required, relevant for software development and possibly data engineering tasks."
_Lvr-60CKa0fuda6AAAAAA==,"['Generative AI', 'Agentic Workflows', 'Retrieval-Augmented Generation', 'Model Fine-Tuning', 'AI Model Deployment and Monitoring', 'Natural Language Processing and Computer Vision']","Generative AI: Pioneer generative AI healthcare solutions aimed at revolutionizing healthcare operations and enhancing member experience by developing and deploying advanced AI models.; Agentic Workflows: Develop and implement agentic workflows that utilize AI agents for autonomous task execution, improving operational efficiency and decision-making capabilities.; Retrieval-Augmented Generation: Employ retrieval-augmented generation techniques to enhance language model performance by enabling access to and utilization of external knowledge sources.; Model Fine-Tuning: Fine-tune pre-trained AI models to adapt them to specific healthcare tasks or datasets, ensuring optimal performance and relevance.; AI Model Deployment and Monitoring: Deploy AI models into production environments, monitor their performance continuously, and adjust as necessary to maintain accuracy, effectiveness, and compliance with governance and regulatory requirements.; Natural Language Processing and Computer Vision: Apply NLP and computer vision techniques within AI workflows to support healthcare applications and improve model capabilities.","['Machine Learning', 'Deep Learning', 'Reinforcement Learning', 'Statistical Methods', 'Data Cleaning and Preprocessing', 'Feature Engineering', 'Data Visualization', 'SQL and NoSQL Databases', 'Big Data Technologies', 'Python and R Programming', 'Machine Learning Frameworks', 'Model Governance and Model Ops']","Machine Learning: Design, develop, and train machine learning models using a variety of algorithms and techniques, including supervised and unsupervised learning, to support healthcare and business objectives.; Deep Learning: Apply deep learning techniques, including neural networks, to develop advanced predictive models and improve healthcare operations.; Reinforcement Learning: Utilize reinforcement learning methods to enhance machine learning models and support autonomous decision-making workflows.; Statistical Methods: Employ statistical analysis techniques such as k-NN, Naive Bayes, and SVM to analyze data and build predictive models.; Data Cleaning and Preprocessing: Prepare data for analysis by performing data cleaning, handling missing values, and removing outliers to ensure high-quality inputs for modeling.; Feature Engineering: Develop and implement feature engineering techniques as part of the machine learning model development process to improve model performance.; Data Visualization: Use data visualization tools like Tableau and Power BI to present complex data insights effectively to stakeholders.; SQL and NoSQL Databases: Manage and utilize SQL and NoSQL databases, data warehousing, and ETL processes to support data storage and retrieval for analytics and modeling.; Big Data Technologies: Leverage big data technologies such as Hadoop and Spark to process and analyze large healthcare datasets.; Python and R Programming: Use Python and R programming languages for data analysis, statistical modeling, and machine learning model development.; Machine Learning Frameworks: Utilize machine learning frameworks including TensorFlow, Keras, and PyTorch to build and train models.; Model Governance and Model Ops: Lead initiatives on model governance and model operations to ensure models meet regulatory and security requirements and maintain accuracy and effectiveness in production."
d96-yMLRAA3mGfySAAAAAA==,['TensorFlow'],"TensorFlow: TensorFlow is preferred as a skill, indicating use of this deep learning framework for building AI models, likely neural networks, in data science and machine learning roles.","['Statistics', 'SAS', 'Python', 'Data Visualization Tools', 'Computer Vision', 'NLP (Natural Language Processing)', 'Databricks', 'Machine Learning']","Statistics: Knowledge of statistics is required for data science and data analyst roles to analyze and interpret data effectively.; SAS: Experience with SAS is needed for statistical analysis and data management tasks in data science positions.; Python: Python programming skills are required for data manipulation, analysis, and building data science projects.; Data Visualization Tools: Familiarity with data visualization tools like Tableau and PowerBI is preferred to create dashboards and visual insights from data.; Computer Vision: Knowledge of computer vision is mentioned as a skill relevant to data science roles, indicating work with image or video data analysis.; NLP (Natural Language Processing): NLP is preferred as a skill, indicating text mining and analysis capabilities relevant to data science tasks.; Databricks: Experience with Databricks is preferred, suggesting use of this platform for big data processing and analytics.; Machine Learning: Machine learning knowledge is required for data science and machine learning engineer positions to build predictive models and algorithms."
56LS-3FzhsqSA6rWAAAAAA==,[],,"['Causal Analysis', 'Experimentation', 'Statistics', 'SQL', 'Python', 'R', 'Data Science', 'Machine Learning', 'Optimization Models', 'Spark', 'Scala', 'Scikit-learn', 'TensorFlow', 'Torch']","Causal Analysis: Used as a technical skill to understand cause-effect relationships in data to inform product and business decisions.; Experimentation: Applied to design and analyze experiments to measure the impact of product initiatives and validate hypotheses.; Statistics: Fundamental for analyzing data, measuring metrics, and supporting data-driven decision making in product operations.; SQL: Used to query and manage data from databases to provide insights and support analytics workflows.; Python: A programming language employed for data analysis, statistical modeling, and building data science solutions.; R: A programming language used for statistical computing and data analysis within the data science team.; Data Science: The core discipline applied to extract insights from data, measure product impact, and support strategic decision making.; Machine Learning: Mentioned as a preferred qualification, indicating use of predictive and optimization models to enhance product and business outcomes.; Optimization Models: Used to improve product and operational decisions by mathematically optimizing key metrics and processes.; Spark: Referenced as a technology for big data processing and analytics, supporting scalable data workflows.; Scala: Mentioned as a language for data engineering and analytics, often used with Spark for large-scale data processing.; Scikit-learn: An open source machine learning framework used for building predictive models and data analysis.; TensorFlow: Listed as an open source framework, likely for machine learning model development and experimentation.; Torch: Included as an open source framework, used for machine learning and statistical modeling tasks."
27OjGwBBtqXGdv6PAAAAAA==,['Artificial Intelligence'],"Artificial Intelligence: Understanding and leveraging AI technologies to develop independent perspectives on the return on investment of infrastructure and resource allocation, and to influence product strategy and investment decisions.","['Data querying languages', 'Scripting languages', 'Statistical/mathematical software', 'Quantitative analysis', 'Experimentation', 'Data mining', 'End-to-end modeling', 'Forecasting', 'Resource allocation modeling']","Data querying languages: Used to extract and manipulate large and complex data sets to solve challenging problems and support decision making in infrastructure finance.; Scripting languages: Applied for quantitative analysis, experimentation, data mining, and building end-to-end models for long range planning and strategic decisions.; Statistical/mathematical software: Utilized to perform quantitative analysis and build models that compute and explain infrastructure operational and capital expenditures at various levels.; Quantitative analysis: Employed to analyze data and build models that support infrastructure investment decisions and resource allocation.; Experimentation: Used to test and validate models and assumptions related to infrastructure investments and resource allocation policies.; Data mining: Applied to discover patterns and insights from large data sets to inform infrastructure finance decisions.; End-to-end modeling: Building and maintaining comprehensive models for long range planning and strategic decision making in infrastructure finance.; Forecasting: Used to predict trends and measure success of infrastructure investments through goal setting and monitoring key metrics.; Resource allocation modeling: Developing models and policies to allocate infrastructure resources efficiently across products and platforms based on technical, operational, and financial perspectives."
cr3DQwZIyk28A0G7AAAAAA==,[],,"['SQL', 'Python', 'R', 'Spark', 'DBT', 'Airflow', 'Snowflake', 'Great Expectations', 'ETL/ELT pipelines', 'Data modeling', 'Descriptive, inferential, and predictive statistical techniques', 'Business Intelligence (BI) tools', 'Data governance']","SQL: Used for advanced data manipulation and analysis to support data science and analytics engineering tasks.; Python: Programming language utilized for data manipulation, analysis, and building scalable ETL/ELT pipelines.; R: Programming language employed for data manipulation and statistical analysis.; Spark: Used as a programming framework for large-scale data processing and analysis.; DBT: Used to develop and maintain data models and curated BI layers that empower product managers and stakeholders to self-serve with confidence.; Airflow: Orchestration tool used to design, build, and maintain scalable ETL/ELT pipelines in cloud environments.; Snowflake: Cloud data platform used to support scalable data storage and processing for analytics and data science.; Great Expectations: Data observability tool used to ensure data quality, consistency, and alignment with enterprise data standards.; ETL/ELT pipelines: Designed and maintained to enable scalable, transparent data processing workflows in collaboration with Product Analytics and Data Engineering teams.; Data modeling: Strong skills emphasized for building analytics layers that serve product and business stakeholders effectively.; Descriptive, inferential, and predictive statistical techniques: Applied to uncover insights from data, communicate findings clearly, and drive action through data.; Business Intelligence (BI) tools: Tools such as Sigma, Tableau, and Power BI are used to visualize and present data to both technical and non-technical audiences.; Data governance: Championed to ensure data consistency, discoverability, and alignment with enterprise data standards."
Xg80OBTFD7IoJL0sAAAAAA==,[],,"['Python', 'Pandas', 'R', 'SQL', 'Tableau', 'Power BI', 'Matplotlib', 'Microsoft Office Suite', 'AWS Redshift', 'Databricks']","Python: Used for analyzing and processing data sets, including with libraries such as Pandas and Matplotlib for data manipulation and advanced data visualization.; Pandas: A Python library utilized for data analysis and processing of datasets relevant to the job.; R: Used for analyzing and processing data sets as part of the data science toolkit.; SQL: Applied for ETL (extract, transform, and load) processes, business intelligence reporting, data mining, and analytics across cloud-based data warehouses, operational databases, data lakes, and API-brokered data.; Tableau: Used to develop advanced data visualizations, including Tableau Server, Tableau Desktop, and Tableau Prep Builder.; Power BI: Employed for creating advanced data visualizations and business intelligence reporting.; Matplotlib: A Python library used for creating advanced data visualizations.; Microsoft Office Suite: Includes Word, Outlook, PowerPoint, Access, Excel, Visio, and SharePoint, used for documentation, presentations, and data management.; AWS Redshift: A cloud-based data warehousing platform used for managing and querying large datasets as part of SaaS/cloud platforms.; Databricks: A cloud-based platform used for data analytics and visualization, including tools like Redash."
j9BM52GxZAd3Ib5jAAAAAA==,"['Transformers', 'PyTorch']","Transformers: Used as part of natural language processing solutions to address client problems in financial services, including surveillance and fraud detection.; PyTorch: Employed as a deep learning framework to develop AI/ML models, including those involving natural language processing techniques.","['Machine Learning', 'Natural Language Processing', 'Time-Series Analysis', 'Sentiment Analysis', 'Topic Modeling', 'Statistical Inference', 'Regression', 'Classification', 'Explanatory Data Analysis', 'Data Science Packages', 'Statistical Programming Languages']","Machine Learning: Used to develop advanced AI/ML solutions and apply techniques such as regression, classification, and predictive modeling to solve business problems and review financial services client risks.; Natural Language Processing: Applied to engineer solutions for trade surveillance, electronic communications surveillance, payments fraud detection, and third-party risk management in financial services.; Time-Series Analysis: Utilized as a statistical method to analyze financial services client risks and develop innovative solutions for operational risk and regulatory compliance.; Sentiment Analysis: Used as a natural language processing technique to assess financial services client risks.; Topic Modeling: Employed as a natural language processing method to analyze and interpret textual data related to financial services risks.; Statistical Inference: Applied to validate models and analyze data patterns in financial services operational risk and compliance programs.; Regression: Used as a statistical method for predictive modeling and risk analysis in financial services.; Classification: Applied as a machine learning technique to categorize and assess financial services client risks.; Explanatory Data Analysis: Performed to generate and test hypotheses, analyze historical data, and identify patterns for developing risk and compliance solutions.; Data Science Packages: Experience with tools such as Pandas and Scikit-learn for data manipulation and machine learning model development.; Statistical Programming Languages: Proficiency in languages like Python and R used for scripting, data analysis, and implementing machine learning algorithms."
N-Sh8FpaBR1RMO4lAAAAAA==,[],,"['Machine Learning Models', 'Statistical Modeling', 'Behavioral Data Analysis', 'Production-Ready Models', 'SQL and Distributed Computing', 'Python (or R)', 'Experimentation and Impact Measurement', 'End-to-End Modeling Projects']","Machine Learning Models: Build, test, and optimize models to forecast user behavior, personalize promotions, and enhance product engagement in the Sportsbook context.; Statistical Modeling: Apply statistical modeling techniques to solve real-world business problems related to marketing and customer lifecycle.; Behavioral Data Analysis: Analyze large-scale behavioral datasets to understand and optimize player engagement with online Sportsbook products over time.; Production-Ready Models: Structure and execute data science projects to deliver business value through models that are deployed in production environments.; SQL and Distributed Computing: Use SQL and distributed computing platforms to work with large datasets efficiently.; Python (or R): Utilize Python or R programming languages for data analysis, modeling, and building machine learning solutions.; Experimentation and Impact Measurement: Drive measurable impact through experimentation and productionized solutions to improve customer engagement and retention.; End-to-End Modeling Projects: Lead projects from ideation through production deployment to improve customer engagement and retention."
4Pz9OfKNaMqqstSAAAAAAA==,[],,"['Predictive Modeling', 'Data Analysis', 'Feature Engineering', 'Machine Learning', 'Model Evaluation', 'Data Integration', 'SQL', 'Python', 'Cloud Data Platforms', 'MLOps', 'Business Intelligence Tools']","Predictive Modeling: Develop, train, and validate predictive models to support safety forecasting, scheduling optimization, and project risk analysis.; Data Analysis: Analyze structured and unstructured project data such as scheduling information, safety reports, and IoT sensor data to identify trends, risks, and operational opportunities.; Feature Engineering: Support data preparation and feature engineering to improve model performance.; Machine Learning: Translate machine learning outputs into actionable insights for construction and safety teams.; Model Evaluation: Evaluate model performance using tools such as pandas, scikit-learn, or XGBoost.; Data Integration: Assist in integrating predictive analytics into dashboards and field tools used by project managers and jobsite personnel.; SQL: Use SQL for data querying and manipulation as part of data science workflows.; Python: Utilize Python programming language and libraries such as pandas, scikit-learn, matplotlib, and XGBoost for data science tasks.; Cloud Data Platforms: Familiarity with cloud platforms like Azure, AWS, or GCP to support data storage, processing, and analytics.; MLOps: Experience using MLflow or Databricks to manage machine learning workflows and model lifecycle.; Business Intelligence Tools: Experience working with or integrating data into enterprise systems such as Procore and Power BI for reporting and dashboarding."
FG8ZsaZcqiRRb7AZAAAAAA==,[],,"['Machine Learning', 'SQL', 'Power BI', 'Cloud-based Data Platforms', 'Data Warehousing and Data Lakes', 'Analytics Solutions Delivery', 'Data Product Roadmap and Backlog Management', 'Collaboration with Data Engineers, Data Scientists, and BI Developers', 'KPI Monitoring and Business Impact Evaluation']","Machine Learning: The role requires experience in machine learning as part of analytics, data, and statistics delivery roles, indicating involvement in building predictive or analytical models.; SQL: Proficiency in SQL is required for querying and managing data within the analytics solutions.; Power BI: Experience with Power BI is needed for data visualization and creating dashboards to support analytics and business intelligence.; Cloud-based Data Platforms: Experience working with cloud-based data platforms such as AWS and Snowflake is preferred, supporting scalable data storage and processing.; Data Warehousing and Data Lakes: The candidate should have experience with data warehousing and data lake architectures to manage large-scale enterprise data.; Analytics Solutions Delivery: The role involves delivering scalable end-to-end analytics solutions from ideation through deployment, ensuring integration into business processes.; Data Product Roadmap and Backlog Management: Responsibilities include defining and maintaining the product roadmap for analytics data products and managing the backlog to prioritize and execute user stories efficiently.; Collaboration with Data Engineers, Data Scientists, and BI Developers: The candidate will work closely with technical teams to ensure accurate implementation of analytics solutions aligned with business needs.; KPI Monitoring and Business Impact Evaluation: The role includes monitoring product performance using KPIs, user feedback, and business impact metrics to guide analytics strategy and delivery."
WrkVYutY_Bn1TmQrAAAAAA==,[],,"['Classification Models', 'Numeric Forecasting Models', 'Customer Segmentation', 'Customer Propensity Models', 'Attribution Models', 'Statistical Analysis', 'Data Extraction, Cleaning, and Transformation', 'SQL Programming', 'Python or R Programming', 'Mathematical Modeling and Probability', 'ARIMA Models', 'Linear Regression', 'Logistic Regression', 'Centroid-based Clustering', 'Hierarchical Clustering', 'Principal Component Analysis (PCA)', 'Decision Trees and Random Forests', 'Bayesian Inference', 'Markov Chain Monte Carlo (MCMC)', 'Marketing Mix Modeling (MMM)', 'Multi-touch Attribution (MTA)', 'Media Analytics', 'Ad Servers, DSP, DMPs, CRMs, Syndicated Research']","Classification Models: Design, estimate, tune, score and maintain classification models to solve marketing problems and understand customer behavior.; Numeric Forecasting Models: Develop numeric forecasting models to predict future marketing and business performance.; Customer Segmentation: Apply customer segmentation techniques to group audiences for targeted marketing and analysis.; Customer Propensity Models: Build models to estimate customer propensity for specific behaviors or outcomes relevant to marketing goals.; Attribution Models: Design and maintain attribution models to understand the impact of marketing touchpoints on business outcomes.; Statistical Analysis: Produce accurate statistical analyses to ensure high quality data insights and support business decisions.; Data Extraction, Cleaning, and Transformation: Extract, clean, and transform customer-level and aggregated data for analysis, modeling, segmentation, and reporting.; SQL Programming: Use SQL programming to query and manage relational databases for data analysis and model development.; Python or R Programming: Utilize Python or R programming languages for machine learning and statistical modeling tasks.; Mathematical Modeling and Probability: Apply mathematical modeling, probability, and statistics principles to design and simulate stochastic systems relevant to marketing analytics.; ARIMA Models: Use ARIMA models for time-series forecasting related to marketing and business performance.; Linear Regression: Implement linear regression models to analyze relationships between variables and predict outcomes.; Logistic Regression: Apply logistic regression for classification tasks such as customer propensity and segmentation.; Centroid-based Clustering: Use centroid-based clustering methods to identify natural groupings within customer data.; Hierarchical Clustering: Apply hierarchical clustering techniques for audience segmentation and pattern discovery.; Principal Component Analysis (PCA): Use PCA for dimensionality reduction and feature extraction in marketing data analysis.; Decision Trees and Random Forests: Build decision tree and random forest models for classification and regression tasks in marketing analytics.; Bayesian Inference: Employ Bayesian inference methods to incorporate prior knowledge and uncertainty in statistical models.; Markov Chain Monte Carlo (MCMC): Use MCMC techniques for simulation and estimation in complex probabilistic models.; Marketing Mix Modeling (MMM): Apply marketing mix modeling to quantify the impact of various marketing channels on sales and ROI.; Multi-touch Attribution (MTA): Use multi-touch attribution models to assign credit to multiple marketing touchpoints influencing customer actions.; Media Analytics: Leverage media analytics expertise, especially in digital media, to analyze audience behavior and campaign effectiveness.; Ad Servers, DSP, DMPs, CRMs, Syndicated Research: Utilize data and insights from ad servers, demand-side platforms, data management platforms, customer relationship management systems, and syndicated research to inform marketing analytics."
qvzZMIg_4APWJ89BAAAAAA==,['Large Language Models'],"Large Language Models: Experience with Large Language Models as an added bonus, indicating familiarity with creation, maintenance, and utilization of LLMs in AI/ML solutions.","['Knowledge Graphs', 'Statistical Analysis', 'Machine Learning Models', 'Data Integration and Extraction', 'Data Visualization', 'Python Programming', 'Machine Learning Frameworks', 'Data Science Project Lifecycle', 'Optimization Models', 'Graph Databases', 'Collaborative Data Science', 'Programming Languages for Analytics']","Knowledge Graphs: Used to model complex relationships within large data sets and enhance data integration and analysis through construction, ontology development, and semantic reasoning.; Statistical Analysis: Applied advanced statistical techniques to communicate complex concepts and findings effectively to both technical and non-technical stakeholders.; Machine Learning Models: Developed and deployed machine learning models to uncover patterns, insights, and predictive analytics within knowledge graphs and real-world applications, including model evaluation, optimization, and tuning for performance improvements.; Data Integration and Extraction: Collaborated with cross-functional teams to identify and extract relevant data from various sources while ensuring data quality and integrity.; Data Visualization: Utilized data visualization methods to effectively communicate complex concepts and findings.; Python Programming: Used Python as a primary programming language for statistical analysis and machine learning model development.; Machine Learning Frameworks: Experience with mainstream machine learning frameworks such as TensorFlow and PyTorch for building and deploying models.; Data Science Project Lifecycle: Led end-to-end data science projects including solution design, data collection, preprocessing, model development, validation, and deployment.; Optimization Models: Applied optimization models as part of data science and machine learning solutions.; Graph Databases: Worked with graph databases such as Neo4j for knowledge graph creation, maintenance, and utilization.; Collaborative Data Science: Collaborated with domain experts and multidisciplinary teams to translate business objectives into actionable data science solutions.; Programming Languages for Analytics: Proficient in programming languages including Python, Spark, Scala, and R for analytics and data science tasks."
EcS_-mF0XtNHRfx0AAAAAA==,[],,"['Data Ingestion', 'Data Cleansing', 'Feature Engineering', 'Data Filtering and Aggregation', 'Statistical Estimation and Hypothesis Testing', 'Machine Learning', 'Iterative Development and Continuous Production Integration and Deployment']",Data Ingestion: Responsible for acquiring and importing healthcare and other client data assets for analysis.; Data Cleansing: Involves cleaning and preparing data to ensure quality and accuracy for downstream analysis.; Feature Engineering: Creating and transforming variables from raw data to improve model performance.; Data Filtering and Aggregation: Processing data by filtering and summarizing to support analysis and insight generation.; Statistical Estimation and Hypothesis Testing: Applying statistical methods to estimate parameters and test assumptions within healthcare data.; Machine Learning: Developing predictive models and iterative development to extract actionable insights from data.; Iterative Development and Continuous Production Integration and Deployment: Engaging in repeated cycles of model development and deploying models into production environments for ongoing use.
B_2B0qIwNl6JcClEAAAAAA==,[],,"['Python', 'R', 'SQL', 'Tableau', 'Power BI', 'Machine Learning', 'Optimization Models', 'Data Science', 'Big Data', 'Dash', 'Flask', 'Git', 'AWS', 'Databricks', 'Azure', 'Virtual Environments', 'Docker Containers', 'Project Management']","Python: Used as a primary programming language for data science applications and analytics projects during the internship.; R: Utilized for data analysis and statistical computing as part of the data science toolkit in the internship.; SQL: Employed for data querying and management to gather and validate necessary data for analytics projects.; Tableau: Used as a BI tool to create dashboards and visualize data insights for stakeholders.; Power BI: Applied as a business intelligence tool to develop dashboards and support data-driven decision making.; Machine Learning: Interns develop machine learning models as part of project deliverables to solve business problems and optimize processes.; Optimization Models: Built during projects to improve operational efficiency and support decision-making in production and manufacturing contexts.; Data Science: The core focus of the internship, involving hands-on experience with data analysis, modeling, and collaboration with cross-functional teams.; Big Data: Basic knowledge expected; relevant to handling large datasets and applying data science techniques in the internship.; Dash: Used as a digital tool for building interactive web applications and dashboards in data science projects.; Flask: Utilized as a web framework to develop data science applications and deploy models or dashboards.; Git: Used for version control and collaboration on code during data science projects.; AWS: Cloud platform exposure provided to support data science workflows and infrastructure needs.; Databricks: Platform exposure offered for big data processing and collaborative data science work.; Azure: Cloud service used to support data science projects and infrastructure during the internship.; Virtual Environments: Used to manage project dependencies and isolate Python environments for reproducible data science workflows.; Docker Containers: Applied to containerize applications and ensure consistent deployment of data science projects.; Project Management: Experience beneficial for leading analytics projects from definition through completion, coordinating deliverables and stakeholder communication."
LSw3Xmtkv-uhV02OAAAAAA==,[],,"['Graph Databases', 'Graph Algorithms', 'SQL', 'Python', 'SAS', 'Java', 'Advanced Machine Learning Methodologies', 'XGBoost', 'H2O', 'Data Analytics', 'Business Analytics', 'Data Visualization', 'Quantitative Models', 'Data Science Products', 'Data Pipelines', 'Agile Practices', 'Link Analysis', 'Hadoop', 'Kafka']","Graph Databases: Used for managing and querying graph-structured data to support scalable graph analytics and fraud detection by analyzing densely connected fraud networks.; Graph Algorithms: Developed and tuned to maximize detection of fraud by analyzing relationships and patterns within graph data.; SQL: Proficient use required for querying structured data as part of data analysis and solution deployment.; Python: Used as a programming language for data analysis, model development, and deployment of data science solutions.; SAS: One of the programming tools used for data analysis and statistical modeling.; Java: Used as a programming language option for data science and application development.; Advanced Machine Learning Methodologies: Includes neural networks, ensemble learning such as XGBoost, and other techniques applied to improve fraud detection and risk management.; XGBoost: An ensemble learning method used for predictive modeling and improving fraud detection accuracy.; H2O: An advanced analytical tool used for building and deploying machine learning models.; Data Analytics: Involves reviewing and interpreting large datasets to uncover revenue opportunities, improve portfolio risk, profitability, and operational performance.; Business Analytics: Used to enable data-driven decision making through trend identification, pattern recognition, and advanced analytical techniques.; Data Visualization: Creating visual representations of data analysis results to communicate insights effectively to stakeholders.; Quantitative Models: Developed and delivered as part of data science products to support risk management and fraud detection.; Data Science Products: Includes models and analytical solutions developed and deployed to production environments to reduce fraud losses and improve client experience.; Data Pipelines: Implied through managing data science use cases lifecycle and deployment to production, ensuring scalability and sustainability.; Agile Practices: Applied for project management, solution development, deployment, and maintenance of data science and analytics projects.; Link Analysis: Used as a graph analytics technique to identify and mitigate fraud networks by analyzing connections between entities.; Hadoop: Referenced as part of expertise handling data across various storage technologies including big data platforms.; Kafka: Included as a data streaming technology used for handling data across its lifecycle."
1JXbwdk4P2mIYU2kAAAAAA==,"['Generative AI', 'PyTorch', 'TensorFlow']",Generative AI: Understanding generative AI model development to stay at the forefront of AI advancements and contribute to innovative solutions that drive business success.; PyTorch: Working with the PyTorch AI framework to build and deploy neural network models as part of AI/ML system development.; TensorFlow: Using the TensorFlow AI framework for developing and deploying neural network models within AI/ML systems.,"['Predictive Modeling', 'Statistical Analysis', 'Data Visualization', 'Machine Learning', 'Advanced Analytics', 'Python Programming', 'MLOps']",Predictive Modeling: Developing predictive models to solve complex business problems and drive data-driven decision making by extracting insights from large datasets.; Statistical Analysis: Conducting statistical analysis to interpret data and inform insights and recommendations for business growth.; Data Visualization: Creating data visualizations to communicate complex data insights effectively and support decision making.; Machine Learning: Building and deploying machine learning systems as part of designing AI/ML systems that transform client operations and enable scalable business solutions.; Advanced Analytics: Leveraging advanced analytics techniques to extract insights from large datasets and support informed decision making.; Python Programming: Using Python programming proficiency to develop machine learning models and data science solutions.; MLOps: Familiarity with MLOps and deployment tools to manage the lifecycle of machine learning models and ensure production readiness.
qt8_DrqpgWG4SnphAAAAAA==,[],,"['Statistical Modeling', 'Relational Databases', 'Python', 'Spark', 'Machine Learning', 'Conda', 'AWS', 'H2O', 'Scala', 'R', 'SQL']","Statistical Modeling: Used to personalize credit card offers and drive data-driven decision-making in financial services.; Relational Databases: Utilized for managing and querying structured data as part of data analytics and machine learning workflows.; Python: A primary programming language used for large scale data analysis, building machine learning models, and working with big data technologies like Spark.; Spark: Employed for processing and analyzing huge volumes of numeric and textual data at scale.; Machine Learning: Applied to build models that detect fraud and protect customers, involving all phases from design to implementation.; Conda: Used as a package and environment management system to support data science workflows.; AWS: Cloud platform leveraged to support data storage, processing, and machine learning model deployment.; H2O: A machine learning platform used to build and manage predictive models within the data science team.; Scala: A programming language used for large scale data analysis alongside Python and R.; R: A programming language used for statistical computing and large scale data analysis.; SQL: Used for querying and managing data within relational databases as part of data analytics."
X6v2WbpOkslKZsjCAAAAAA==,[],,"['Predictive Modeling', 'Statistical and Machine Learning Techniques', 'Data Collection and Exploration', 'Advanced Analytical Tools', 'Statistical and Advanced Analytics Methodologies']","Predictive Modeling: Develop predictive models for customer attrition and digital marketing to support retention programs, vertical segmentation, and marketing strategies.; Statistical and Machine Learning Techniques: Apply statistical and machine learning techniques to solve complex, multidimensional problems using quantitative information.; Data Collection and Exploration: Identify, collect, and explore the right data used for predictive modeling and algorithm development.; Advanced Analytical Tools: Build and maintain advanced analytical tools to support data-driven analyses and problem solving.; Statistical and Advanced Analytics Methodologies: Design and develop state-of-the-art data-driven analyses using statistical and advanced analytics methodologies to solve business problems."
_QeDs0JMADGJt1J9AAAAAA==,"['Large Language Models', 'Multimodal Generative Modeling', 'AI Agents', 'Reinforcement Learning', 'Deep Learning', 'PyTorch']","Large Language Models: The position focuses on research and development of large language models (LLMs) as foundational AI models to accelerate drug discovery processes.; Multimodal Generative Modeling: The candidate will drive research on novel foundation models with a specific focus on multimodal generative models, contributing to cutting-edge AI methods in drug discovery.; AI Agents: The role includes working on AI agents as part of foundation models, aiming to advance AI research and engineering challenges in biomedical applications.; Reinforcement Learning: The job requires expertise in reinforcement learning techniques applied to the development of novel AI methods for drug and target discovery.; Deep Learning: The candidate must have excellent knowledge of deep learning theory and practice, demonstrated through past projects and publications, and apply deep learning methods in building biomedical foundation models.; PyTorch: Proficiency in PyTorch programming is required for implementing deep learning models and system design within the full model-development lifecycle.","['Machine Learning', 'MLOps']","Machine Learning: The role involves developing and delivering innovative machine learning solutions applied to drug discovery and target discovery, including large-scale representation learning and reinforcement learning techniques.; MLOps: The candidate is expected to have extensive knowledge of MLOps best practices, including code version control, high-performance compute infrastructures, and machine learning experiment monitoring workflows, collaborating with the MLOps team on system design and scalability."
v3gSeLJdguHyvKpoAAAAAA==,[],,"['Statistical Modeling', 'Causal Inference', 'A/B Testing', 'Machine Learning', 'Python', 'R', 'scikit-learn', 'TensorFlow', 'PyTorch', 'SQL', 'Relational Databases', 'Large-Scale Data Processing', 'BigQuery', 'Data Dashboards', 'Data Analysis']","Statistical Modeling: Used to design and apply data science solutions for improving ad platform performance and advertiser outcomes, including causal inference and experimental design.; Causal Inference: Applied to drive continuous improvement through understanding cause-effect relationships in ad performance and experimentation.; A/B Testing: Used for experimentation and online experiment design to measure and optimize ad delivery and advertiser experience.; Machine Learning: Develop ML-based systems for anomaly detection, forecasting, and pattern recognition to enhance ad tech solutions.; Python: Programming language used for data science tasks, including building ML models and data analysis.; R: Programming language used for statistical analysis and data science modeling.; scikit-learn: Python library utilized for building machine learning models relevant to ad tech challenges.; TensorFlow: Used as a machine learning framework to develop models for anomaly detection, forecasting, and pattern recognition.; PyTorch: Employed as a deep learning framework for building ML models in the ad tech environment.; SQL: Used for querying and managing relational databases to analyze large-scale datasets and support data infrastructure.; Relational Databases: Data storage systems accessed via SQL to handle large-scale data for analysis and model development.; Large-Scale Data Processing: Experience with tools like Spark, Hadoop, and Hive to process and analyze big data relevant to digital advertising.; BigQuery: Cloud-based data warehouse used for large-scale data analysis in ad tech environments.; Data Dashboards: Built self-service dashboards to provide product and engineering teams with insights and support decision-making.; Data Analysis: Analyzing large-scale datasets to uncover insights, trends, and strategic opportunities in digital advertising."
xFjqQDYVd1oYxSEjAAAAAA==,[],,"['Media Mix Modeling', 'Message Mix Modeling', 'Multi-Touch Attribution', 'Forecasting', 'Testing and Experimentation', 'Multivariate Regression', 'Bayesian Regression', 'Randomized Control Trials', 'Predictive Modeling', 'Discrete Choice Models', 'Time-Series Analysis', 'Python', 'R', 'SQL', 'Statistical Tools', 'Marketing Science', 'Proprietary Tooling Development', 'Data Set Innovation', 'Client Communication and Visualization']","Media Mix Modeling: Used to lead client engagements involving advanced analytics to understand the impact of various media channels on marketing outcomes.; Message Mix Modeling: Applied in client projects to analyze the effectiveness of different messaging strategies within marketing campaigns.; Multi-Touch Attribution: Employed to attribute credit to multiple marketing touchpoints in the customer journey for better campaign optimization.; Forecasting: Used to predict future marketing and business outcomes as part of advanced analytics deliverables.; Testing and Experimentation: Supported testing and experimentation efforts to optimize marketing strategies and validate model results.; Multivariate Regression: Mastered as an advanced analytical technique for modeling relationships between multiple variables in marketing science.; Bayesian Regression: Utilized as a statistical method to incorporate prior knowledge and uncertainty in regression modeling for marketing analytics.; Randomized Control Trials: Applied to design and analyze experiments for causal inference in marketing effectiveness studies.; Predictive Modeling: Developed predictive models to forecast marketing outcomes and support client decision-making.; Discrete Choice Models: Used to model consumer choice behavior as part of marketing science analytics.; Time-Series Analysis: Employed to analyze temporal data patterns for forecasting and understanding marketing trends.; Python: Used as a primary programming language for data analysis, statistical modeling, and building proprietary tooling.; R: Utilized for statistical computing and advanced analytics in marketing science projects.; SQL: Applied to query and manage data from databases to support analytics and modeling efforts.; Statistical Tools: Used various commonly employed statistical software and tools to perform data analysis and modeling.; Marketing Science: Developed frameworks and bespoke offerings to apply advanced analytics techniques to marketing challenges.; Proprietary Tooling Development: Built custom tools in partnership with technology and media teams to support marketing analytics and Martech solutions.; Data Set Innovation: Identified and incorporated new and innovative data sets to improve existing models or create new ones.; Client Communication and Visualization: Distilled complex statistical results into clear graphical presentations to influence clients and stakeholders."
uXHo1IJOM4IwE46KAAAAAA==,['Deep Learning'],Deep Learning: Develop deep learning models and related algorithms specifically to support cyber analytic capabilities and prototype future cyber capabilities for large-scale implementation.,"['Machine Learning', 'Deep Learning', 'Natural Language Processing', 'Statistical Data Analysis', 'Graph Theory and Network Analysis', 'Programming Languages', 'SQL', 'Databricks Intelligence Platform']","Machine Learning: Develop machine learning models and algorithms to create actionable insights and automate scoring for cyber analytic capabilities; includes productization and delivery of machine learning components as part of successful project deliverables.; Deep Learning: Develop deep learning models and related algorithms to support cyber analytic capabilities and prototype future cyber capabilities for large-scale implementation.; Natural Language Processing: Apply NLP techniques such as question answering, text mining, information retrieval, and distributional semantics to support cyber security analytic programs and knowledge engineering.; Statistical Data Analysis: Use statistical data analysis, experimental design, and hypothesis validation methods to analyze data and support cyber analytic research and development.; Graph Theory and Network Analysis: Utilize graph theory and network analysis methods to analyze cyber threats and support cyber security analytic programs.; Programming Languages: Use programming languages such as Python, JavaScript, and R to develop machine learning and data science solutions for cyber analytic capabilities.; SQL: Perform database querying using SQL to extract and manipulate data for analysis and model development.; Databricks Intelligence Platform: Leverage the Databricks Intelligence Platform to support data science workflows and cyber analytic capabilities development."
PB6jkB5FPMUGC660AAAAAA==,[],,"['Regression Models', 'Classification', 'Clustering', 'Statistical Profiling', 'Inference', 'Predictive Analysis', 'Supervised Learning', 'Unsupervised Learning', 'Feature Engineering', 'Data Pipelines', 'SQL', 'Python', 'R', 'Tableau', 'Jupyter', 'Spark', 'Hadoop', 'Bayesian Methods', 'Time Series Models', 'Graph Models', 'Random Forest', 'Machine Learning', 'Deep Learning Frameworks', 'Data Visualization', 'Data Storytelling', 'Shell Scripting']","Regression Models: Used to develop statistical models analyzing large amounts of data to find patterns and solve problems for strategic business decisions in cybersecurity.; Classification: Applied in supervised machine learning scenarios to detect threats, predict attacks, and support prevention, detection, and response in cybersecurity data.; Clustering: Used in unsupervised learning scenarios including KNN, K-means, Bayesian, and Mean-shift clustering for forensic analysis, network anomaly detection, and encrypted traffic classification.; Statistical Profiling: Implemented to analyze cybersecurity data for inference and predictive analysis to identify patterns and threats.; Inference: Used alongside statistical profiling and classification to derive insights from cybersecurity data for threat detection and prediction.; Predictive Analysis: Employed to forecast cybersecurity threats and attacks by analyzing data from multiple sources and applying machine learning models.; Supervised Learning: Leveraged for classification and regression tasks to detect and predict cybersecurity threats and attacks.; Unsupervised Learning: Used for clustering, association, and dimension reduction to analyze network traffic and detect anomalies in cybersecurity data.; Feature Engineering: Implied in the design and implementation of machine learning solutions analyzing diverse cybersecurity data sources to improve model performance.; Data Pipelines: Utilized to process and manage large volumes of cybersecurity data from various sources such as Splunk, Qualis, and network traffic systems.; SQL: Used for querying and managing large databases and data lakes containing cybersecurity and network data.; Python: Applied with libraries such as SciPy, NumPy, and PySpark for data analysis, machine learning model development, and handling big data in cybersecurity contexts.; R: Used for statistical analysis, visualization, and data storytelling in cybersecurity data science tasks.; Tableau: Employed as a visualization and data storytelling tool to communicate cybersecurity insights and model results.; Jupyter: Used as an interactive environment for developing, validating, and deploying machine learning models and data analysis workflows.; Spark: Hands-on experience with Spark is used to process and analyze high volume cybersecurity data efficiently.; Hadoop: Used to manage and analyze large-scale data lakes and big data relevant to cybersecurity analytics.; Bayesian Methods: Applied as part of advanced statistical concepts for clustering, classification, and graph models to analyze cybersecurity data.; Time Series Models: Used to analyze temporal cybersecurity data for threat detection and prediction over time.; Graph Models: Utilized to analyze relationships and patterns in cybersecurity data, such as network traffic and threat connections.; Random Forest: Used as a machine learning technique for classification and regression tasks in cybersecurity threat detection.; Machine Learning: Applied broadly to develop models for detecting, predicting, and preventing cybersecurity threats using various supervised and unsupervised techniques.; Deep Learning Frameworks: Experience with deep learning frameworks is used for model validation and deployment in cybersecurity machine learning solutions.; Data Visualization: Used to create visual representations of cybersecurity data and model outputs to support decision-making and communication.; Data Storytelling: Employed to effectively communicate insights derived from cybersecurity data analysis and machine learning models.; Shell Scripting: Used to automate data processing and management tasks within cybersecurity data pipelines."
EtmlA8aarewd1UubAAAAAA==,"['Generative AI', 'Large Language Models', 'Deep Learning Frameworks for Neural Networks']","Generative AI: The role involves designing, building, and implementing advanced generative AI solutions, including text-to-image generation and multi-modal models, to augment decision making for the US Intelligence Community.; Large Language Models: Experience with Large Language Models (LLMs) is required to build state-of-the-art generative models for text and other modalities tailored to mission-specific use cases.; Deep Learning Frameworks for Neural Networks: Hands-on experience with deep learning frameworks like TensorFlow, PyTorch, and JAX is necessary specifically for building and deploying neural network models within generative AI and advanced AI algorithms.","['Machine Learning', 'Data Mining', 'Numerical Optimization', 'Algorithms and Data Structures', 'Parallel and Distributed Computing', 'High-Performance Computing', 'Python', 'Deep Learning Frameworks']","Machine Learning: The role involves building models for business applications using traditional machine learning methods to address real-world challenges and support customer needs.; Data Mining: Experience with data mining techniques is required to extract useful information from diverse data sources to build data-intensive applications at scale.; Numerical Optimization: The job requires applying numerical optimization methods as part of algorithm development and model building for business and intelligence community applications.; Algorithms and Data Structures: The position demands expertise in algorithms and data structures to efficiently process and analyze data for model development and deployment.; Parallel and Distributed Computing: Experience with parallel and distributed computing is necessary to handle high-performance computing tasks and scale data-intensive applications.; High-Performance Computing: The role includes leveraging high-performance computing resources to build and deploy complex models and data applications at scale.; Python: Hands-on experience using Python is essential for building models and implementing data science and machine learning solutions.; Deep Learning Frameworks: The job requires practical experience building models using deep learning frameworks such as TensorFlow, Keras, PyTorch, MXNet, or JAX to develop neural deep learning methods."
8LeYF2XmL_c0lKQeAAAAAA==,['TensorFlow'],"TensorFlow: TensorFlow is preferred for machine learning and deep learning model development, indicating use of neural networks and AI frameworks.","['Statistics', 'SAS', 'Python', 'Computer Vision', 'Data Visualization Tools', 'NLP', 'Text Mining', 'Tableau', 'Power BI']","Statistics: Knowledge of statistics is required for data science and machine learning positions to analyze and interpret data effectively.; SAS: Experience with SAS is needed for statistical analysis and data management in data science roles.; Python: Python programming skills are required for data science and machine learning tasks, including data manipulation and analysis.; Computer Vision: Familiarity with computer vision techniques is preferred, indicating work with image or video data analysis.; Data Visualization Tools: Experience with data visualization tools is preferred to create visual representations of data insights.; NLP: Natural Language Processing skills are preferred, suggesting work with text data and language analysis.; Text Mining: Text mining skills are preferred for extracting useful information from text data.; Tableau: Experience with Tableau is preferred for building interactive business intelligence dashboards and visualizations.; Power BI: Power BI experience is preferred for creating business intelligence reports and dashboards."
bbUxVNWuJFKrOXU-AAAAAA==,['Deep Learning'],Deep Learning: Applies advanced deep learning techniques as part of predictive and prescriptive modeling to improve business outcomes and model performance.,"['Advanced Analytics', 'Behavioral Data Analysis', 'Machine Learning', 'Consumer Analytics', 'ETL (Extract, Transform, Load)', 'Data Science Project Lifecycle', 'Data Infrastructure Collaboration', 'Statistical Modeling Techniques', 'Predictive and Prescriptive Modeling', 'MLOps (Model Deployment and Operations)', 'Programming and Coding Standards', 'Data Manipulation and Extraction Tools', 'Cloud Computing Services', 'Data Visualization Tools', 'Experimental Design and Hypothesis Testing', 'Clustering Analysis', 'Time Series Modeling', 'Data Ethics and Privacy']","Advanced Analytics: Utilizes specialized knowledge and experience to apply advanced analytics and modeling to improve business results.; Behavioral Data Analysis: Leverages unique customer information and behavioral data to influence strategic business decisions using complex, innovative analytics and multi-variate models.; Machine Learning: Uses machine learning and data mining technologies to develop predictive and prescriptive models; reviews AI/ML modeling completed by team members and provides coaching and feedback.; Consumer Analytics: Utilizes specialized knowledge of consumer analytics including retention models, agency economics, and leads optimization in daily work.; ETL (Extract, Transform, Load): Applies broad knowledge of advanced programming and complex ETL processes to execute projects and lead the team through effective technical skills.; Data Science Project Lifecycle: Acts as main contributor to multiple phases of data science projects including ideation, experiment design, exploratory data analysis (EDA), feature engineering, model building, and deployment.; Data Infrastructure Collaboration: Partners closely with IT, business, and data management/engineering teams to understand, utilize, and improve data infrastructure to support fact-based decision making.; Statistical Modeling Techniques: Applies explanatory, diagnostic, and inferential techniques such as experimental design, hypothesis testing, clustering analysis, time series, and other statistical modeling algorithms to decide appropriate methodologies.; Predictive and Prescriptive Modeling: Demonstrates advanced proficiency in predictive and prescriptive modeling using advanced machine learning and deep learning techniques.; MLOps (Model Deployment and Operations): Manages specialized model deployments via MLOps techniques, partners with analytics and IT teams to deploy models/rules on various platforms, leads testing of new solutions, and actively improves the MLOps environment.; Programming and Coding Standards: Demonstrates clean reusable code, effective documentation, advanced knowledge of coding standards, and version control using Git.; Data Manipulation and Extraction Tools: Possesses advanced knowledge of data analysis and manipulation tools including SQL, Python, SAS, R, and Snowflake for statistical, modeling, and monitoring needs.; Cloud Computing Services: Utilizes cloud computing services such as AWS to work on large-scale structured and unstructured multidimensional data.; Data Visualization Tools: Uses data visualization tools like Tableau and Power BI to communicate complex data insights clearly to technical and non-technical audiences.; Experimental Design and Hypothesis Testing: Employs experimental design and hypothesis testing techniques as part of advanced statistical analysis to support data-driven decision making.; Clustering Analysis: Applies clustering analysis as part of diagnostic and inferential statistical techniques to analyze data patterns.; Time Series Modeling: Uses time series and other statistical modeling algorithms to analyze temporal data and support business decisions.; Data Ethics and Privacy: Maintains broad knowledge of data ethics and data privacy to ensure responsible handling of data in all projects."
W7z9qXFh2jQjilApAAAAAA==,['Generative AI'],Generative AI: Evaluate and recommend generative AI applications to improve analytical workflows and accelerate insight delivery.,"['Predictive Modeling', 'Statistical Analysis', 'Data Cleaning and Ingestion', 'Segmentation and Attribution', 'A/B Testing', 'Data Visualization and Dashboarding', 'Data Quality and Governance', 'SQL', 'Python', 'CRM and Marketing Systems']","Predictive Modeling: Develop customer and account-level models to predict behavior, inform targeting strategies, and personalize engagement across digital and direct channels.; Statistical Analysis: Run statistical analyses to translate findings into actionable insights for business leaders.; Data Cleaning and Ingestion: Own the full analytics lifecycle including ingesting and cleaning raw data to prepare it for analysis and modeling.; Segmentation and Attribution: Partner with stakeholders to uncover opportunities through segmentation, attribution, funnel optimization, and campaign performance analysis.; A/B Testing: Apply A/B testing methodologies to evaluate marketing and campaign performance.; Data Visualization and Dashboarding: Build interactive dashboards using Power BI (preferred) or Tableau to surface insights clearly and intuitively to both technical and non-technical users.; Data Quality and Governance: Identify data gaps and inconsistencies across systems such as CRM and campaign tools, and work with internal partners to resolve data quality issues and improve governance standards.; SQL: Use SQL for data querying and manipulation as part of data science and analytics workflows.; Python: Utilize Python for data science tasks including data processing, modeling, and analysis.; CRM and Marketing Systems: Work with CRM and marketing platforms such as Hubspot, Salesforce, and LinkedIn Campaign Manager to integrate and analyze marketing data."
y9FYKoONTLbfhN-DAAAAAA==,"['Large Language Models (LLMs)', 'Retrieval-Augmented Generation (RAG)', 'Continued Pre-training (CPT)', 'Supervised Fine-tuning (SFT)', 'Generative AI', 'MLOps for AI Models', 'Flask and FastAPI', 'SageMaker Pipelines, Step Functions, Metaflow', 'Open-source LLM Serving (TGI/vLLM)']","Large Language Models (LLMs): The position requires developing and advising on LLM solutions for enterprise-wide documentation to enhance and expedite decision-making processes.; Retrieval-Augmented Generation (RAG): RAG techniques are applied to improve LLM-based solutions by integrating external knowledge retrieval to support enterprise documentation and AI initiatives.; Continued Pre-training (CPT): CPT is used to further train open-source LLMs to adapt them to specific enterprise needs and improve performance on domain-specific tasks.; Supervised Fine-tuning (SFT): SFT is employed to fine-tune LLMs with labeled data to enhance their accuracy and relevance for enterprise applications.; Generative AI: The role involves working on generative AI initiatives that transform decision-making processes and deliver impactful AI-driven solutions.; MLOps for AI Models: MLOps pipelines are utilized specifically for training, deploying, and managing AI models such as LLMs, including containerization and CI/CD practices tailored to AI workflows.; Flask and FastAPI: These serving frameworks are used to deploy AI models and APIs as part of MLOps pipelines to enable scalable and efficient AI service delivery.; SageMaker Pipelines, Step Functions, Metaflow: Orchestration pipelines on AWS and other platforms are used to automate and manage AI model training, deployment, and lifecycle operations.; Open-source LLM Serving (TGI/vLLM): Experience with serving open-source LLMs using frameworks like Text Generation Inference (TGI) and vLLM is required to support enterprise AI applications.","['Machine Learning', 'Deep Learning', 'Python', 'scikit-learn', 'TensorFlow', 'PyTorch', 'MLOps', 'Containerization (Docker)', 'CI/CD', 'AWS', 'Databricks', 'Dataiku', 'GitHub', 'Jira', 'Agile Project Methodology', 'Business Intelligence']","Machine Learning: The role involves designing, training, and deploying machine learning models to develop data science products that transform client needs into quantifiable solutions and enhance decision-making.; Deep Learning: The job requires expertise in deep learning to build advanced models, including training and deployment on platforms like AWS, Databricks, and Dataiku, to support business intelligence and data science solutions.; Python: Python is a core programming language used extensively in the role, including working with related packages such as scikit-learn, TensorFlow, and PyTorch for machine learning and deep learning projects.; scikit-learn: Used as a key machine learning package within Python to develop and deploy machine learning models as part of data science solutions.; TensorFlow: Employed as a deep learning framework to design, train, and deploy neural network models in the data science projects.; PyTorch: Utilized as a deep learning framework for building and deploying neural network models within the data science initiatives.; MLOps: The role includes working with MLOps pipelines for training and deploying models, incorporating containerization technologies like Docker and CI/CD practices to ensure efficient model lifecycle management.; Containerization (Docker): Docker is used to containerize machine learning and deep learning models as part of MLOps pipelines to facilitate scalable and reproducible deployment.; CI/CD: Continuous Integration and Continuous Deployment pipelines are implemented to automate the training and deployment of machine learning and deep learning models.; AWS: Amazon Web Services is a key platform for deploying machine learning and deep learning models, including use of various AWS services such as Textract, Transcribe, Lambda, SageMaker Pipelines, and Step Functions.; Databricks: Databricks is used as a platform for designing, training, and deploying machine learning and deep learning models within the data science workflow.; Dataiku: Dataiku is leveraged as a platform to develop and maintain data science and business intelligence solutions, including model deployment.; GitHub: GitHub is used for version control and collaboration, ensuring clean, well-commented code and structured project documentation.; Jira: Jira is utilized for project management and maintaining structured documentation within the data science and AI projects.; Agile Project Methodology: Familiarity with agile methodologies is required to manage project development lifecycles effectively and deliver data science solutions iteratively.; Business Intelligence: The role involves developing and maintaining business intelligence solutions that support decision-making and strategic mission goals."
AewwRofIWcwySa44AAAAAA==,[],,"['Mass Spectrometry Proteomics Data', 'R and Python', 'Metadata Capture and Versioning Control', 'Bioinformatics', 'Data Management and Visualization', 'Machine Learning', 'Relational and Graph Databases', 'Workflow Languages (Nextflow, CWL)', 'Shell Scripting and Cloud/Cluster Computing (AWS, GCP)']","Mass Spectrometry Proteomics Data: The job involves annotating and curating high dimensional proteomics data generated from mass spectrometry to support downstream data processing and analysis.; R and Python: Scripting languages R and Python are used for data curation, processing, and analysis of proteomics datasets.; Metadata Capture and Versioning Control: Capturing metadata and maintaining version control are essential for ensuring reproducibility and traceability of proteomics data.; Bioinformatics: Applying bioinformatics knowledge to curate and register proteomics data into internal data registries.; Data Management and Visualization: The role includes managing proteomics data and creating visualizations to facilitate understanding and analysis.; Machine Learning: The curated proteomics data will be used to enable machine learning workflows with high reproducibility and scalability.; Relational and Graph Databases: Knowledge of relational or graph databases is preferred for organizing and registering proteomics data.; Workflow Languages (Nextflow, CWL): Familiarity with workflow languages like Nextflow and CWL is desirable to support scalable and reproducible data processing pipelines.; Shell Scripting and Cloud/Cluster Computing (AWS, GCP): Experience with shell scripting and cloud or cluster computing infrastructure is desirable to facilitate data processing and computational workflows."
FtbFbUPhXlyFkXaXAAAAAA==,[],,"['SQL', 'Python', 'Machine Learning', 'Predictive Analytics', 'Natural Language Processing', 'Data Visualization Tools', 'Data Warehousing', 'Big Data Technologies']","SQL: Used for querying and managing large, complex datasets as part of data analysis and data mining tasks.; Python: Utilized as a programming language for data analysis, machine learning, and working with data mining techniques.; Machine Learning: Applied to develop predictive analytics models and uncover trends in data to generate business insights.; Predictive Analytics: Used to build models that forecast outcomes and support decision-making based on data trends.; Natural Language Processing: Employed to analyze and extract insights from text data as part of data science tasks.; Data Visualization Tools: Used to communicate technical concepts and insights effectively to non-technical audiences through visual representations.; Data Warehousing: Involved in managing and organizing large volumes of data to support analysis and mining activities.; Big Data Technologies: Applied to handle and process large, complex datasets efficiently for analysis and insight generation."
iYDM8RABtSBWYFzLAAAAAA==,"['Deep Learning', 'Natural Language Processing', 'Computer Vision', 'Reinforcement Learning']","Deep Learning: Experience with deep learning techniques is preferred, indicating involvement with neural network-based models for complex data tasks.; Natural Language Processing: Experience with natural language processing is desired, suggesting work with text data and language understanding models.; Computer Vision: Experience with computer vision is preferred, implying work with image or video data using AI techniques.; Reinforcement Learning: Experience with reinforcement learning is noted as a nice-to-have skill, indicating familiarity with AI methods for decision-making and autonomous systems.","['Machine Learning', 'Python', 'Mathematics for Data Science', 'Scalable Machine Learning', 'Cloud Platforms for Deployment']","Machine Learning: Research, design, implement, and deploy machine learning algorithms for enterprise applications, including regression and classification, supervised and unsupervised learning.; Python: Excellent programming skills in Python are required to develop and implement machine learning models and analytics capabilities.; Mathematics for Data Science: Strong mathematical background in linear algebra, calculus, probability, and statistics is essential to support machine learning and data analysis tasks.; Scalable Machine Learning: Experience with scalable machine learning techniques such as MapReduce and streaming to handle large datasets and enterprise-scale applications.; Cloud Platforms for Deployment: Hands-on experience deploying and operating applications using Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) on major cloud providers like Amazon AWS, Microsoft Azure, or Google Cloud Services."
9rv1R7t03lIBqnisAAAAAA==,[],,"['Machine Learning Models', 'Optimization Algorithms', 'A/B Testing and Multi-Armed Bandits', 'Cost Structure Modeling', 'Data Science Methodologies', 'Data Pipelines and Real-Time Decisioning', 'SQL, Python, and Spark', 'Experimentation and Performance Measurement', 'Data Governance and Regulatory Compliance', 'Feature Engineering and Behavioral Modeling', 'Team Leadership in Data Science and Analytics']","Machine Learning Models: Build and deploy machine learning models that inform routing decisions based on issuer behavior, cost structures, geo/location, network rules and eligibility, and approval likelihood.; Optimization Algorithms: Develop scalable, low-latency algorithms integrated into real-time payment decisioning engines to optimize interchange fees, transaction approval rates, and margin expansion.; A/B Testing and Multi-Armed Bandits: Design and validate experiments such as A/B tests and multi-armed bandits to quantify the financial impact of routing and interchange strategies.; Cost Structure Modeling: Model complex cost structures including interchange fees, scheme fees, and foreign exchange margins to recommend optimal routing paths for each transaction.; Data Science Methodologies: Apply advanced data science methodologies to translate technical insights into strategic business actions and foster strong relationships with clients and stakeholders.; Data Pipelines and Real-Time Decisioning: Develop and operationalize data pipelines and real-time payment decisioning engines to enable dynamic routing and pricing optimization.; SQL, Python, and Spark: Utilize programming languages and frameworks such as Python, SQL, and Spark for data processing, analysis, and model development.; Experimentation and Performance Measurement: Define and track key performance indicators (KPIs) to measure the business and client impact of dynamic routing and provide regular updates to executive leadership.; Data Governance and Regulatory Compliance: Ensure models and data practices comply with data privacy regulations such as PCI-DSS and GDPR, working closely with data governance and legal teams.; Feature Engineering and Behavioral Modeling: Model historical issuer behavior including approval decline patterns, time-of-day trends, and geo-specific sensitivity to improve routing decisions.; Team Leadership in Data Science and Analytics: Build, lead, and mentor a multidisciplinary team of data scientists and analysts, promoting best practices in modeling, experimentation, and domain knowledge."
6fGpmHTcSlpADv6KAAAAAA==,"['Large Language Models (LLMs)', 'TensorFlow (Neural Network Application)']","Large Language Models (LLMs): Referenced as part of the candidate's experience in building machine learning/AI models to customize patient services, indicating involvement with modern AI techniques beyond traditional machine learning.; TensorFlow (Neural Network Application): Used specifically for implementing neural network models, indicating application of deep learning frameworks in AI model development.","['Multivariate Linear Regression', 'Cluster Algorithms', 'Decision Trees', 'Logistic Regression', 'Principal Component Analysis (PCA)', 'Time Series Analysis', 'Survival Analysis', 'Machine Learning (Supervised and Unsupervised)', 'Bayesian Methods', 'Neural Networks', 'Python (NumPy, pandas, Scikit-learn, TensorFlow)', 'SQL', 'Exploratory Data Analysis', 'Data Integration and Preparation', 'Data Visualization and Automation', 'Dashboard Metrics Summarization']","Multivariate Linear Regression: Used as a statistical model to generate advanced insights and predictive capabilities from data.; Cluster Algorithms: Applied to analyze and segment data for deriving meaningful patient journey insights.; Decision Trees: Employed as part of statistical modeling techniques to support predictive analytics and data-driven decision making.; Logistic Regression: Utilized for predictive modeling to understand and forecast patient-related outcomes.; Principal Component Analysis (PCA): Used for dimensionality reduction and feature extraction to handle complex datasets effectively.; Time Series Analysis: Applied to analyze temporal data patterns relevant to patient journeys and other time-dependent metrics.; Survival Analysis: Used to model and analyze time-to-event data, likely related to patient outcomes or treatment durations.; Machine Learning (Supervised and Unsupervised): Implemented to build predictive models and uncover patterns in patient and business data to customize patient services and drive business value.; Bayesian Methods: Used as a statistical approach to enhance predictive modeling and data analysis.; Neural Networks: Applied as part of advanced modeling techniques to improve predictive capabilities and insights from complex data.; Python (NumPy, pandas, Scikit-learn, TensorFlow): Used extensively for data analysis, modeling, and implementing machine learning models in production environments.; SQL: Utilized for data manipulation, aggregation, and optimization to prepare and engineer data from disparate systems.; Exploratory Data Analysis: Conducted to understand the strengths and limitations of complex datasets and to identify appropriate analytical approaches.; Data Integration and Preparation: Involves combining large, varied datasets and preparing them for modeling and analysis.; Data Visualization and Automation: Leveraged to communicate data-driven insights effectively and to streamline analytical workflows.; Dashboard Metrics Summarization: Summarizing key metrics for dashboards to present actionable insights to internal clients and senior management."
jlx4B7G84QjAVJjjAAAAAA==,['Artificial Intelligence'],Artificial Intelligence: The role focuses on developing and validating AI models that empower data-driven decision-making and advance healthcare innovation.,"['Machine Learning', 'MLOps', 'Data Science', 'Structured and Unstructured Healthcare Data']","Machine Learning: This role involves designing, testing, and validating machine learning models to support data-driven decision-making across clinical, financial, and operational healthcare domains.; MLOps: The position contributes to process improvements and the adoption of governance policies related to machine learning operations to support enterprise-wide AI/ML efforts.; Data Science: Expertise in data science is required to develop and validate AI/ML models and translate domain knowledge into scalable technical solutions for healthcare.; Structured and Unstructured Healthcare Data: The job requires working with both structured and unstructured healthcare data to develop AI/ML models that improve clinical, financial, and operational outcomes."
GLzGWz9HNOjBml78AAAAAA==,[],,"['Machine Learning', 'Large Data Sets', 'ETL Processes', 'Relational Databases', 'Scripting Languages', 'Statistical Analysis Tools', 'Distributed Data Processing', 'Quantitative Analysis Techniques', 'Data Mining and Presentation', 'Exploratory Data Analysis']","Machine Learning: The job requires expertise in machine learning techniques to analyze data and contribute to product development.; Large Data Sets: The role involves working with large data sets and network-based data such as TCP or HTTP to extract insights.; ETL Processes: The position requires experience with Extract, Transform, Load (ETL) processes to manage and prepare data for analysis.; Relational Databases: Proficiency in relational databases using SQL or PL/SQL is necessary for querying and managing structured data.; Scripting Languages: Developing in scripting languages like PHP, Python, or Perl is required for data manipulation and automation tasks.; Statistical Analysis Tools: The job involves using statistical analysis software such as R, MATLAB, Mathematica, ROOT, SPSS, SAS, or Stata to perform quantitative analysis.; Distributed Data Processing: Experience with large scale data processing infrastructures using distributed systems like Hadoop, Hive, MapReduce, LCG, or MPI is needed to handle big data workloads.; Quantitative Analysis Techniques: The role requires applying quantitative analysis methods including clustering, regression, pattern recognition, and descriptive and inferential statistics to interpret data.; Data Mining and Presentation: The position involves data mining and presenting data insights to understand user interactions and support product decisions.; Exploratory Data Analysis: The job may include exploratory analysis to identify trends, opportunities, and factors influencing product performance."
3tuYGjYGj111rYs3AAAAAA==,"['Generative AI Tools', 'Large Language Models (LLMs)']",Generative AI Tools: Leverage generative AI tools such as ChatGPT to perform data analysis tasks more efficiently and to research new business problems related to fraud and AML.; Large Language Models (LLMs): Utilize LLM tools like ChatGPT for researching business problems and automating documentation and analysis tasks within fraud and AML projects.,"['Fraud and AML Strategies and Models', 'Data Integration and Quality Validation', 'Risk Pattern Analysis and Detection Strategies', 'SQL', 'Data Streaming and Processing Technologies', 'Machine Learning Model Evaluation and Tuning', 'BI Dashboards and Reporting Tools', 'Data Pipelines and Platform Configuration']","Fraud and AML Strategies and Models: Develop and evaluate fraud detection and anti-money laundering models using both supervised and unsupervised machine learning techniques to identify risk patterns and improve detection accuracy.; Data Integration and Quality Validation: Lead technical discussions and manage data transfers to ensure high-quality, comprehensive data is provided for fraud and AML analysis and model development.; Risk Pattern Analysis and Detection Strategies: Conduct data and pattern analyses to suggest, create, and iteratively test fraud and AML detection strategies based on domain expertise and business needs.; SQL: Use complex SQL queries with advanced joining and layered logic to extract and manipulate data relevant to fraud and AML use cases.; Data Streaming and Processing Technologies: Utilize technologies such as REST API, Kafka, SFTP, and Hadoop for data streaming, processing, and integration within fraud and AML solutions.; Machine Learning Model Evaluation and Tuning: Collaborate with ML modeling teams to develop, tune, and evaluate fraud detection models, analyzing performance metrics to guide threshold selection and decision-making.; BI Dashboards and Reporting Tools: Design and configure business intelligence dashboards and reporting components to provide tailored insights and strong business value to clients.; Data Pipelines and Platform Configuration: Lead configuration of data pipelines and platform components to support fraud and AML detection solutions and client-specific customizations."
zQbX8Kunk1O0HLj_AAAAAA==,[],,"['SQL', 'NoSQL', 'Data Quality', 'Data Management', 'Data Governance', 'Data Visualization', 'Python', 'R', 'Java', 'C++', 'Model Evaluation Metrics', 'Feature Engineering', 'Exploratory Data Analysis', 'Statistical Models', 'Machine Learning', 'Optimization Techniques', 'Model Deployment', 'Model Scaling', 'Code Testing', 'Big Data Analytics', 'Spark', 'Scala', 'Scikit-learn', 'TensorFlow', 'Torch', 'Data Pipelines', 'Business Intelligence Tools', 'Advanced Excel Techniques']","SQL: Used as a coding language for data extraction, querying databases, and performing initial data quality checks on extracted data.; NoSQL: Referenced as a type of distributed datastore relevant for data source identification and storage.; Data Quality: Involves performing initial data quality checks on extracted data and understanding data quality standards as part of data strategy.; Data Management: Part of the data ecosystem knowledge required to understand data governance, accessibility, storage, and scalability to unlock the monetary value of data assets.; Data Governance: Included in the data ecosystem knowledge to ensure data quality standards, accessibility, and compliance with organizational policies.; Data Visualization: Involves generating graphical representations of data and model outcomes using multiple tools and techniques to communicate insights effectively to business stakeholders.; Python: Used as a programming language for code development, testing, data visualization, and advanced analytical modeling.; R: Used for programming in advanced statistical methods, exploratory data analysis, and data visualization.; Java: Mentioned as a coding language used for developing solutions and application features.; C++: Mentioned as a coding language used for developing solutions and application features.; Model Evaluation Metrics: Includes techniques such as Chi square, ROC curve, and root mean square error used for model fit testing, tuning, and validation to assess accuracy, fit, validity, and robustness.; Feature Engineering: Involves defining and finalizing features based on model responses and introducing new or revised features to enhance analysis and outcomes.; Exploratory Data Analysis: Conducting basic statistical analysis, hypothesis testing, and statistical inferences on available data to understand data characteristics and inform modeling.; Statistical Models: Includes advanced statistical methods and best-practice modeling techniques such as graphical models and Bayesian inference used for analytical modeling.; Machine Learning: Applied through standard ML models like SVM, Random Forest, and neural networks to develop custom analytical models for structured, complex data.; Optimization Techniques: Utilizes classical optimization methods like Newton-Raphson and gradient descent, as well as numerical methods such as linear, integer, and quadratic programming for analytical modeling.; Model Deployment: Supports efforts to deploy analytical models into production, including understanding model formats and server environments.; Model Scaling: Involves supporting the scalability and sustainability of analytical models in production environments.; Code Testing: Includes static, dynamic, software composition analysis, and manual penetration testing methods to ensure code quality and security.; Big Data Analytics: Knowledge of analytics and automation techniques applied to large datasets to translate business problems into data or mathematical solutions.; Spark: Referenced as a technology for which successful completion of assessments is valued, indicating its use in data processing and analytics.; Scala: Mentioned as a language for which assessments are valued, often used with big data frameworks like Spark.; Scikit-learn: An open source framework used for machine learning model development and evaluation.; TensorFlow: Listed as an open source framework used for machine learning model development.; Torch: Included as an open source framework used for machine learning model development.; Data Pipelines: Implied through responsibilities involving data source identification, extraction, quality checks, and preparation for modeling and visualization.; Business Intelligence Tools: Includes Tableau and PowerBI used for creating dashboards and visualizations to communicate data insights to stakeholders.; Advanced Excel Techniques: Used for data analysis and modeling tasks as part of analytical modeling responsibilities."
gnaaq_1XFeoRQdOoAAAAAA==,[],,"['Data Science', 'Data Visualization']",Data Science: The role involves applying data science techniques to analyze real data and solve business problems.; Data Visualization: The job requires using visualization tools to represent data insights effectively.
vfZjZiS8ZdRbtRuKAAAAAA==,[],,"['Predictive Modeling', 'Data Processing Tools', 'Data Pipeline Testing and Diagnostics', 'Data Validation and Cleaning']","Predictive Modeling: Develop and implement predictive modeling solutions for internal and external business applications to support decision-making and commercial use.; Data Processing Tools: Create and develop tools to process various data types including tables, texts, and images to support analytics workflows.; Data Pipeline Testing and Diagnostics: Perform tests and develop diagnostic methodologies to ensure the accuracy and reliability of data processing pipelines.; Data Validation and Cleaning: Validate and clean data from multiple sources to ensure data quality and integrity for effective modeling."
8Upetwi2ajJ92iZ0AAAAAA==,[],,"['Predictive Modeling', 'Machine Learning', 'Statistical Inference', 'Data Preparation and Cleaning', 'Cloud Data Processing', 'Database Querying', 'Data Analysis and Visualization', 'Python Data Libraries', 'Linear Algebra and Statistics', 'Scientific Software Practices']","Predictive Modeling: Develop and optimize predictive models using machine learning and statistical modeling techniques to forecast business outcomes.; Machine Learning: Apply machine learning principles and tools to construct, optimize, and evaluate models that predict likelihoods of various business scenarios.; Statistical Inference: Use knowledge of probability and statistics to make defensible statistical inferences from data.; Data Preparation and Cleaning: Write software to prepare, clean, and sample data for use in developing predictive models.; Cloud Data Processing: Utilize cloud resources such as Amazon Web Services to prepare and process data.; Database Querying: Extract and query data from databases, specifically Snowflake, to support data analysis and modeling.; Data Analysis and Visualization: Use tools like SQL, Python, Jupyter Notebooks, and Looker to analyze data and create visualizations that inform business strategy.; Python Data Libraries: Employ Python libraries such as pandas, numpy, SciPy, and scikit-learn for data manipulation, scientific computing, and machine learning.; Linear Algebra and Statistics: Apply proficiency in linear algebra and statistics to support modeling and data analysis tasks.; Scientific Software Practices: Follow scientific software principles including versioning systems and reproducibility to ensure reliable and maintainable code."
Kja4P_O8wJbP0NWVAAAAAA==,[],,"['Data Import and Integration', 'Data Visualization and Dashboards', 'Data Management', 'Statistical Programming', 'Machine Learning', 'Probability and Statistics', 'Econometrics']","Data Import and Integration: Importing data from PDH OBIA and OBIEE systems to finalize and reconcile reports across various geographies.; Data Visualization and Dashboards: Creating graphs and dashboards to compare durations between planned, trending, and actual deliveries, and to track early, on-time, and delayed deliveries segmented by geography.; Data Management: Managing data including cleaning, backup, and troubleshooting access issues in Smartsheets, as well as granting and removing access as required.; Statistical Programming: Applying statistical programming skills to support data analysis and reporting tasks.; Machine Learning: Utilizing machine learning techniques as part of the data analysis and modeling efforts.; Probability and Statistics: Employing probability and statistical methods to analyze data and support decision-making.; Econometrics: Using econometric methods to analyze economic data and support statistical modeling."
qB4HAz9l6SsBV-KYAAAAAA==,"['Transformers', 'Large Language Models (LLMs)', 'Generative AI', 'Conversational AI']","Transformers: Experience with recent deep learning models based on transformer architectures such as BERT, LLaMA, GPTs, and Gemini, including safe fine-tuning of these models.; Large Language Models (LLMs): Familiarity with serving optimizations and multi-LoRa techniques for large language models used in conversational AI.; Generative AI: Involvement in building and evolving generative AI-powered conversational platforms and personal assistants.; Conversational AI: Design and implementation of AI-driven conversational platforms that support natural voice commands, text messages, and multi-modal user interactions.","['Python', 'SQL', 'Classical Machine Learning Models', 'Statistical Measures', 'Data Analysis and Pattern Recognition', 'Data Pipelines and Model Lifecycle', 'Optimization Models', 'Open Source Frameworks', 'Experimental and Analytic Planning', 'Business Analytics', 'Natural Language Understanding (NLU) and Natural Language Processing (NLP)']","Python: Required experience with Python programming language for data analysis and model development.; SQL: Solid knowledge of SQL for data extraction and querying relational databases.; Classical Machine Learning Models: Hands-on experience with traditional machine learning models including training, testing, evaluation metrics, and tuning key parameters to optimize model performance.; Statistical Measures: Understanding of statistical concepts such as confidence intervals, significance of error measurements, and the use of development and evaluation datasets.; Data Analysis and Pattern Recognition: Experience analyzing data to identify patterns, conducting error and deviation analysis, and improving data representation.; Data Pipelines and Model Lifecycle: Hands-on expertise across the full model lifecycle including data pipelines, data extraction, model training, model serving, labeling tools, ML-ops, and ad-hoc tooling.; Optimization Models: Use of optimization models as part of data science and machine learning tasks.; Open Source Frameworks: Experience using open source frameworks such as scikit-learn, TensorFlow, and PyTorch for machine learning and data science tasks.; Experimental and Analytic Planning: Ability to develop experimental and analytic plans for data modeling processes, including use of strong baselines and accurate determination of cause and effect relationships.; Business Analytics: Ability to communicate model results effectively to business stakeholders and present insights in intuitive ways.; Natural Language Understanding (NLU) and Natural Language Processing (NLP): Familiarity and experience with NLU/NLP techniques to improve conversational AI capabilities, including handling multi-modal interactions and conversational context."
otZsAj2XLUruWbW4AAAAAA==,"['Large Language Models', 'Vision Transformers', 'Multimodal AI']","Large Language Models: Designing, training, fine-tuning, and evaluating foundation models such as GPT and BERT for clinical or biomedical research applications.; Vision Transformers: Utilizing vision transformer models as part of machine learning approaches for medical imaging and multimodal AI research.; Multimodal AI: Focusing on AI models that integrate multiple data modalities, including medical imaging and large language models, to advance clinical or biomedical research.","['Machine Learning Models', 'Statistical Analysis', 'Data Processing and Cleaning', 'Study Design', 'Programming in Python or R', 'Interactive Data Visualization Tools']","Machine Learning Models: Design and development of machine learning models for clinical or biomedical research, including training, fine-tuning, and evaluation of models such as CNNs and other vision models.; Statistical Analysis: Conducting experimental design and evaluation involving statistical methodologies, performance assessment, error analysis, and robustness testing within clinical or biomedical research.; Data Processing and Cleaning: Retrieving, processing, and cleaning large-scale biomedical data to build high-quality datasets for research purposes.; Study Design: Applying in-depth knowledge of study design to support clinical or biomedical data analysis and research.; Programming in Python or R: Utilizing strong programming skills in Python or R, including experience with machine learning frameworks, to develop and implement data science and machine learning solutions.; Interactive Data Visualization Tools: Developing interactive visualization tools using technologies such as Plotly, Dash, D3.js, or React to communicate data insights clearly."
Ehv1fZjmJt__2icEAAAAAA==,"['Large Language Models', 'Natural Language Processing', 'AI-Driven Data Preparation']","Large Language Models: Apply LLM techniques to generate insights and infographics as part of data analysis and visualization.; Natural Language Processing: Use NLP methods in conjunction with LLMs to extract insights and support AI-powered dashboard development.; AI-Driven Data Preparation: Leverage AI and machine learning techniques to automate data preparation, particularly for dynamic visualizations in Power BI.","['Data Extraction and Analysis', 'Data Preparation', 'Data Integration', 'Data Modeling Techniques', 'Predictive Modeling', 'Insight Generation', 'Data Visualization', 'Programming with Python or R']","Data Extraction and Analysis: Manipulate complex datasets to generate reports, charts, and graphs, analyzing for outliers, root causes, and correlations to propose solutions that optimize outcomes.; Data Preparation: Clean datasets, handle missing values, and remove outliers to ensure accurate modeling and high-quality data for predictive models.; Data Integration: Combine diverse data sources to enable comprehensive analysis supporting various projects.; Data Modeling Techniques: Apply expertise in data modeling to support analysis and predictive modeling efforts.; Predictive Modeling: Ensure data quality and train models to generate actionable insights for critical business initiatives.; Insight Generation: Identify patterns in data to support drug discovery processes and provide actionable insights.; Data Visualization: Develop dashboards, reports, and visualizations using tools like Tableau and matplotlib to clearly communicate complex data.; Programming with Python or R: Utilize Python or R programming languages for data analysis and manipulation."
bP4L-99QPwarZkutAAAAAA==,[],,"['Statistical Modeling', 'Relational Databases', 'Machine Learning Models', 'Model Validation and Backtesting', 'Classification and Clustering', 'Sentiment Analysis', 'Time Series Analysis', 'Deep Learning', 'Open Source Programming Languages', 'Cloud Computing Platforms', 'Confusion Matrix and ROC Curve Interpretation', 'Benchmark and Challenger Models']","Statistical Modeling: Used to personalize credit card offers and drive data-driven decision-making in financial services.; Relational Databases: Utilized for managing and querying large-scale customer data to support analytics and modeling efforts.; Machine Learning Models: Built, trained, evaluated, validated, and implemented to improve decision-making and model risk management.; Model Validation and Backtesting: Performed to assess model performance, ensure reliability, and defend models to internal and regulatory partners.; Classification and Clustering: Applied as part of modeling techniques to segment data and predict outcomes relevant to business decisions.; Sentiment Analysis: Used as a data science technique to analyze textual data for insights relevant to model development.; Time Series Analysis: Employed to analyze temporal data patterns for forecasting and risk assessment.; Deep Learning: Experience required in applying deep learning methods as part of advanced modeling techniques.; Open Source Programming Languages: Leveraged for large-scale data analysis and development of data science solutions, including Python, Scala, and R.; Cloud Computing Platforms: Used to support scalable data science workflows and model development in a cloud environment.; Confusion Matrix and ROC Curve Interpretation: Used to evaluate classification model performance and inform decision-making.; Benchmark and Challenger Models: Developed to stress test critical modeling decisions and improve model robustness."
OyvMi6BztJoRyLuVAAAAAA==,[],,"['Exploratory Data Analysis', 'Big Data', 'Dashboards and Data Visualization', 'Structured and Unstructured Data', 'Data Integration and Data Pipelines', 'Pattern Recognition and Business Question Formulation', 'Machine Learning', 'Predictive and Prescriptive Modeling', 'Statistical Methods and Advanced Mathematics', 'Python (Pandas, NumPy, SciPy)', 'R (tidyverse)', 'SQL and Database Management Systems', 'Low-Code/No-Code Platforms', 'Databricks', 'ElasticSearch and MongoDB', 'GTFS Data', 'Data Quality and Process Automation', 'Data Communication and Reporting']","Exploratory Data Analysis: Used to produce innovative solutions from complex and high dimensional qualitative and quantitative datasets across the AEC industry.; Big Data: Leading projects involving large datasets across planning, transportation safety, traffic operations, and transit/rail to support business functions and decision making.; Dashboards and Data Visualization: Developing and maintaining dashboards and websites that support large databases and facilitate complex search queries, using tools like Power BI, Tableau, and Esri products (ArcGIS Online) to communicate insights and influence business and IT leaders.; Structured and Unstructured Data: Mining and analyzing highly complex structured and unstructured datasets using advanced statistical methods for data-driven decision making.; Data Integration and Data Pipelines: Designing, modifying, and building new data processes and large, complex datasets ensuring data integrity and statistical accuracy, including extracting relevant information from databases and systems.; Pattern Recognition and Business Question Formulation: Recognizing patterns, identifying opportunities, posing business questions, and making discoveries that lead to prototype development and product improvement.; Machine Learning: Employing sophisticated analytical and machine learning methods to prepare data for predictive and prescriptive modeling, including designing, developing, testing, validating, and analyzing models and advanced algorithms to extract optimal value from data.; Predictive and Prescriptive Modeling: Using machine learning and statistical methods to predict future data trends and provide strategic business solutions.; Statistical Methods and Advanced Mathematics: Applying advanced statistical predictive modeling, simulation, and mathematics to analyze data and support decision making.; Python (Pandas, NumPy, SciPy): Utilizing Python libraries for data manipulation, numerical computation, and scientific computing in analytics and modeling tasks.; R (tidyverse): Using R and the tidyverse collection for data analysis and statistical modeling.; SQL and Database Management Systems: Working with SQL Server, Oracle, and other database management systems to manage and query data.; Low-Code/No-Code Platforms: Familiarity with platforms like Microsoft PowerApps to support data management and application development.; Databricks: Using Databricks for big data processing and analytics.; ElasticSearch and MongoDB: Employing ElasticSearch for search and analytics and MongoDB for handling unstructured data.; GTFS Data: Working with General Transit Feed Specification data relevant to transportation planning and analysis.; Data Quality and Process Automation: Implementing automated processes for efficient model production and writing quality procedures to ensure data and process integrity.; Data Communication and Reporting: Communicating data insights, foresights, and strategic solutions effectively to technical and non-technical stakeholders through reports and visualizations."
R2QB7JrLkZh-8nJ3AAAAAA==,"['Large Language Models', 'Vision Transformers', 'Multimodal AI']","Large Language Models: Designing, training, fine-tuning, and evaluating foundation models such as GPT and BERT for clinical or biomedical research applications.; Vision Transformers: Applying vision transformer models as part of large-scale machine learning efforts in medical imaging and multimodal AI research.; Multimodal AI: Developing AI models that integrate multiple data modalities, including medical imaging and text, to enhance clinical or biomedical research outcomes.","['Machine Learning Models', 'Statistical Analysis', 'Data Processing and Cleaning', 'Study Design', 'Programming in Python or R', 'Interactive Data Visualization Tools']","Machine Learning Models: Design and development of machine learning models for clinical or biomedical research, including training, fine-tuning, and evaluation of models such as CNNs and other vision models.; Statistical Analysis: Conducting experimental design and evaluation involving statistical methodologies, performance assessment, error analysis, and robustness testing within clinical or biomedical research.; Data Processing and Cleaning: Retrieving, processing, and cleaning large-scale biomedical data to build high-quality datasets for research purposes.; Study Design: Applying in-depth knowledge of study design to support clinical or biomedical data analysis and research.; Programming in Python or R: Utilizing strong programming skills in Python or R, including experience with machine learning frameworks, to develop and implement data science and machine learning solutions.; Interactive Data Visualization Tools: Developing interactive visualization tools using technologies such as Plotly, Dash, D3.js, or React to communicate data insights clearly."
Y7KH4dh7O3uUnfIeAAAAAA==,"['Generative AI', 'Large Language Models (LLMs)', 'Amazon SageMaker', 'Computer Vision']","Generative AI: Knowledge of Generative AI is required, highlighting the role's involvement with modern AI techniques for generating content or data.; Large Language Models (LLMs): Experience with LLMs is required, indicating work with advanced AI models for natural language understanding and generation.; Amazon SageMaker: Familiarity with Amazon SageMaker is required, showing the use of cloud-based AI/ML model development and deployment services.; Computer Vision: Knowledge of computer vision is required, indicating involvement with AI techniques for image and video analysis.","['Statistics', 'Python', 'Data Visualization Tools', 'NLP (Natural Language Processing)', 'TensorFlow']","Statistics: Knowledge of statistics is required for data science, data analyst, and machine learning positions to analyze and interpret data effectively.; Python: Python programming skills are necessary for data science, machine learning, and data analyst roles to develop data pipelines, perform analysis, and build models.; Data Visualization Tools: Experience with data visualization tools such as Tableau and PowerBI is preferred to create dashboards and visual representations of data insights.; NLP (Natural Language Processing): Preferred skill in NLP and text mining indicates the role involves processing and analyzing textual data for insights.; TensorFlow: TensorFlow is a preferred skill, indicating use in building and deploying machine learning models."
nvd4TKHvearttw2aAAAAAA==,['Generative AI'],"Generative AI: Leading and delivering generative AI solutions integrated with supply chain analytics to enable better, faster, and more accurate decision making.","['Machine Learning', 'Generative AI', 'Decision Intelligence', 'Graph Data Science', 'Optimization and Simulation', 'MLOps', 'Python, R, SQL']","Machine Learning: Leading a team of data scientists to deliver global machine learning modeling solutions for supply chain projects including Planning, Procurement, Manufacturing, and Global Logistics.; Generative AI: Delivering generative AI solutions as part of advanced analytics projects to support enterprise supply chain digitization and decision making.; Decision Intelligence: Deploying decision intelligence solutions at enterprise scale to improve supply chain responsiveness and agility.; Graph Data Science: Applying graph data science techniques as part of advanced analytics solutions for supply chain optimization.; Optimization and Simulation: Utilizing optimization and simulation methods to enhance supply chain processes and support strategic decision making.; MLOps: Expertise in machine learning operations at enterprise scale to support deployment and management of ML models.; Python, R, SQL: Experience coding in Python, R, and SQL to develop and implement data science and analytics solutions."
g8X_IFaPzLpUL6I6AAAAAA==,[],,"['Machine Learning Models', 'Descriptive and Diagnostic Analytics', 'SQL', 'Python', 'R', 'Causal Models', 'Digital Clean Rooms']","Machine Learning Models: Used to cluster and segment audiences and to scale and deploy custom audiences across various platforms and publishers.; Descriptive and Diagnostic Analytics: Performed to measure campaign performance within digital clean rooms and other analytics platforms such as Google ADH, Facebook advanced analytics, and Amazon Marketing Cloud.; SQL: A programming language used hands-on for data querying and manipulation relevant to the role.; Python: A programming language used hands-on for coding tasks to support data science workflows and process efficiencies.; R: A programming language used hands-on for statistical analysis and data science tasks.; Causal Models: Applied as intermediate-level machine learning techniques to support the analysis of information and campaign performance.; Digital Clean Rooms: Knowledge of digital clean rooms and their related concepts and strategy is required to perform analytics in privacy-safe environments."
bEuejHaG2OpO7IW5AAAAAA==,['Artificial Intelligence'],"Artificial Intelligence: Applied to analyze imagery, text, and other unstructured data to improve modeling accuracy and generate business insights, including opportunities to leverage AI techniques alongside machine learning.","['Regression', 'Classification', 'Machine Learning', 'Natural Language Processing', 'Deep Learning', 'Statistical Modeling', 'Statistical Software Programs', 'Business Intelligence and Analytics']","Regression: Used as one of the advanced statistical modeling techniques to develop sophisticated models that solve business problems and enhance customer experience.; Classification: Applied as an advanced statistical method to build predictive models within the Loss Analytics Research team and improve modeling accuracy.; Machine Learning: Leveraged to improve modeling accuracy and insights, including the development and refresh of claim level predictive models and other analytical solutions.; Natural Language Processing: Utilized as one of the advanced knowledge areas to analyze text and unstructured data for business insights and model development.; Deep Learning: Included as an advanced knowledge area potentially applied to analyze imagery and unstructured data to enhance modeling and business problem solving.; Statistical Modeling: Employed extensively to develop advanced models, interpret results, and apply emerging statistical procedures to business data.; Statistical Software Programs: Used to perform advanced modeling, research, and analytics required for developing and interpreting complex data models.; Business Intelligence and Analytics: Incorporated as part of the heavy concentration in mathematics and programming to support data science tools and research using large data sets."
94GX-3cOtn2mqtkfAAAAAA==,['TensorFlow'],"TensorFlow: TensorFlow is preferred as a deep learning framework, indicating potential use in neural network-based AI projects.","['Statistics', 'SAS', 'Python', 'Data Visualization Tools', 'Computer Vision', 'NLP', 'Java', 'Software Development Life Cycle', 'Core Java', 'JavaScript', 'C++', 'Spring Boot', 'Microservices', 'Docker', 'Jenkins', 'REST APIs', 'TensorFlow', 'Project Work']","Statistics: Knowledge of statistics is required for data science and data analyst roles to analyze and interpret data effectively.; SAS: Experience with SAS is mentioned as a required skill, indicating its use for statistical analysis and data management.; Python: Python programming is required, likely for data manipulation, analysis, and building data science models.; Data Visualization Tools: Familiarity with data visualization tools such as Tableau and PowerBI is preferred to create dashboards and visual reports.; Computer Vision: Knowledge of computer vision is listed, suggesting involvement with image data processing and analysis.; NLP: Natural Language Processing is preferred, indicating work with text data and text mining techniques.; Java: Experience in Java programming is required, relevant for software development and possibly data engineering tasks.; Software Development Life Cycle: Understanding of the software development life cycle is necessary, implying involvement in end-to-end project development.; Core Java: Knowledge of Core Java is required for software programming tasks.; JavaScript: JavaScript knowledge is required, likely for front-end development or data visualization.; C++: C++ programming knowledge is mentioned as part of software programming skills.; Spring Boot: Experience with Spring Boot framework is required, indicating backend development skills.; Microservices: Experience with microservices architecture is required, relevant for building scalable applications.; Docker: Docker experience is required, indicating containerization skills for deployment and development.; Jenkins: Experience with Jenkins is required, suggesting knowledge of continuous integration and deployment pipelines.; REST APIs: Experience with REST APIs is required, indicating skills in building and consuming web services.; TensorFlow: TensorFlow is preferred, suggesting familiarity with deep learning frameworks for machine learning projects.; Project Work: Candidates are expected to have project experience on relevant technologies, demonstrating practical application of skills."
3L7SmRsLsP-eMQApAAAAAA==,"['Generative AI', 'Large Language Models']","Generative AI: Designing and implementing advanced generative AI solutions for social listening that leverage large language models to process and extract sentiment, intent, and brand affinity signals from unstructured social media comment data across influencer and social campaigns.; Large Language Models: Building LLM-powered systems to transform unstructured social content into structured, high-signal intelligence for actionable insights delivered to campaign managers and advertisers.","['Clean Room Technologies', 'Experimental Design', 'Statistical Analysis Techniques', 'Regression Modeling', 'Machine Learning', 'Data Visualization Techniques', 'SQL and Spark SQL', 'Python and R', 'Econometric Modeling', 'A/B Testing and Campaign Incrementality Testing', 'Predictive Modeling', 'Data Pipelines and Governance', 'Audience Modeling and Automation', 'Third-Party Data Integration', 'Power Analysis']","Clean Room Technologies: Leading initiatives involving clean-room data science to enable privacy-safe data sharing, audience joins, closed-loop attribution measurement, campaign incrementality testing, and cross-channel measurement workflows with partners like Meta and Pinterest.; Experimental Design: Providing expert advice and support on experimental design, learning agenda, success metrics, and requirements for testing new product features, ad partnerships, data adoption, and campaign audience test & learn.; Statistical Analysis Techniques: Applying advanced statistical methods including t-tests for normally distributed data, non-parametric tests for non-normal data, normality tests, difference-in-difference regression for biased tests, propensity scores, and bootstrapping tests to analyze experiment results and calculate statistical significance.; Regression Modeling: Utilizing regression models such as difference-in-difference regression to analyze biased tests and support campaign measurement and audience insights.; Machine Learning: Developing and deploying machine learning-based targeting and measurement solutions, including advanced ML algorithms like KD Tree Nearest Neighbor Synthetic control matching, and using ML frameworks such as Spark MLlib for custom analysis.; Data Visualization Techniques: Employing data visualization methods to communicate complex statistical and analytical findings effectively to stakeholders and campaign managers.; SQL and Spark SQL: Using SQL and Spark SQL for data querying, extraction, and processing to support analytics, modeling, and measurement workflows.; Python and R: Leveraging Python and R programming languages for statistical analysis, machine learning, and custom data science workflows.; Econometric Modeling: Applying econometric models for digital multi-touch attribution, marketing analytics, and campaign performance measurement.; A/B Testing and Campaign Incrementality Testing: Designing and analyzing experimental A/B tests and incrementality tests to measure campaign effectiveness and optimize marketing strategies.; Predictive Modeling: Building predictive models to enhance targeting accuracy, segmentation depth, and audience activation across media channels.; Data Pipelines and Governance: Collaborating to define scalable data schemas, governance protocols, and privacy-safe data-sharing pipelines that support campaign measurement and audience activation.; Audience Modeling and Automation: Developing scalable audience automation and sharing frameworks integrating clean-room capabilities with internal and external media platforms to enable dynamic audience modeling and streamlined data activation.; Third-Party Data Integration: Incorporating external data sources such as Freeosk sampling, Experian, Epsilon, and Circana panel data to improve targeting accuracy, segmentation, and personalization in marketing campaigns.; Power Analysis: Conducting statistical power analyses using baseline metrics, expected impact, and population size to determine minimum sample sizes for statistically significant tests."
4JMZcXuU2wbJEZJJAAAAAA==,['TensorFlow'],"TensorFlow: TensorFlow is a preferred skill, indicating use of deep learning frameworks for AI model development in data science and machine learning roles.","['Statistics', 'SAS', 'Python', 'Data Visualization Tools', 'Machine Learning', 'Computer Vision', 'NLP', 'Java', 'Core Java', 'JavaScript', 'C++', 'Spring Boot', 'Microservices', 'Docker', 'Jenkins', 'REST APIs', 'Project Work']","Statistics: Knowledge of statistics is required for data science and machine learning positions to analyze and interpret data effectively.; SAS: Experience with SAS is mentioned as a required skill for data science roles, indicating its use for statistical analysis and data management.; Python: Python programming language is required for data science and machine learning roles, supporting data analysis, scripting, and project work.; Data Visualization Tools: Familiarity with data visualization tools such as Tableau and PowerBI is preferred to create dashboards and visual insights from data.; Machine Learning: Machine learning knowledge is required for certain positions, including project work and understanding of related technologies.; Computer Vision: Computer vision is listed as a required skill area, indicating involvement with image or video data analysis in data science roles.; NLP: Natural Language Processing is a preferred skill, suggesting work with text mining and language data analysis.; Java: Java programming language experience is required for both software development and data science/machine learning roles.; Core Java: Knowledge of Core Java is required for software programming roles.; JavaScript: JavaScript knowledge is required for software programming and full stack development.; C++: C++ programming knowledge is required for software programming roles.; Spring Boot: Experience with Spring Boot framework is required for software development roles.; Microservices: Experience with microservices architecture is required for software development roles.; Docker: Docker experience is required for containerization in software development.; Jenkins: Jenkins experience is required for continuous integration and deployment in software development.; REST APIs: Experience with REST APIs is required for software development roles.; Project Work: Hands-on project work is emphasized as essential for candidates to demonstrate practical skills in both software development and data science/machine learning."
Q5yyelc-6pcT_Az8AAAAAA==,[],,"['Competitive Intelligence Framework', 'Exploratory Data Analysis', 'Data Models', 'Data Pipelines', 'SQL', 'Python', 'Analytics and Insights', 'Dashboards and Reporting']","Competitive Intelligence Framework: Develop and maintain a framework to monitor industry trends and competitors' performance, products, and activities to inform strategic decisions.; Exploratory Data Analysis: Conduct exploratory data analysis (EDA) to uncover insights that explain business drivers and support strategic decision-making.; Data Models: Develop and maintain data models to support data collection, analysis, and reporting for business insights.; Data Pipelines: Develop and maintain data pipelines to facilitate data collection, processing, and analysis from multiple internal and external sources.; SQL: Use SQL professionally to query and manage data as part of data analysis and model development.; Python: Use Python professionally for data processing, analysis, and building data science projects.; Analytics and Insights: Provide analytics and insights on business performance, competitive landscape, and metric trends to senior leadership to influence decision-making.; Dashboards and Reporting: Prepare and present clear, concise readouts and dashboards tailored for senior-level audiences to communicate complex information effectively."
XW0IRUn7bngH8eOnAAAAAA==,[],,"['A/B Testing', 'Power Analysis', 'Experimental Inference', 'SQL', 'Python', 'Statistical Analysis', 'Telemetry and Instrumentation Frameworks']","A/B Testing: Used to design, implement, and analyze online experiments to evaluate product features and user experience changes.; Power Analysis: Performed to ensure the validity and reliability of experiments by determining the appropriate sample size and statistical power.; Experimental Inference: Applied to draw valid conclusions from experimentation data and assess the impact of changes on user behavior and metrics.; SQL: Utilized to extract and manipulate data efficiently for analysis of experiment outcomes and user behavior.; Python: Used for data analysis and ad hoc querying to provide actionable insights from experimentation data.; Statistical Analysis: Employed to analyze experiment data, including metrics evaluation and user behavior changes, ensuring robust conclusions.; Telemetry and Instrumentation Frameworks: Collaborated with partners to ensure robust data collection and instrumentation for accurate experiment tracking and analysis."
lmVpVHR6235f71D8AAAAAA==,"['Large Language Models', 'Applied AI Research', 'AI Adoption Leadership']","Large Language Models: Prior experience in LLM is desirable, indicating involvement with large language models in AI systems.; Applied AI Research: Part of a global interdisciplinary team driving AI research and digital transformation through innovative AI products.; AI Adoption Leadership: Helping lead the adoption of AI across the enterprise by communicating AI concepts effectively to technical and non-technical audiences.","['Model Development Lifecycle', 'Stakeholder Management', 'Business Problem Translation', 'Collaboration in Cross-Functional Teams']","Model Development Lifecycle: Driving the end-to-end model development lifecycle, ensuring reproducible research and well-managed software delivery.; Stakeholder Management: Taking ownership of projects and adeptly managing and engaging stakeholders while adhering to industry best practices.; Business Problem Translation: Translating complex business problems into projects with clearly defined scope and guiding projects to successful production.; Collaboration in Cross-Functional Teams: Working collaboratively with product, engineering, UX teams, and global colleagues to share information and value diverse ideas."
2X2i3m-VlA5XoRFuAAAAAA==,"['Generative AI', 'Vision-Language Models', 'Deep Learning Frameworks for Neural Networks', 'Diffusion Models', 'Vision Transformers', 'Multimodal Foundation Models', 'MLOps for AI Models']","Generative AI: Employing generative AI methods including diffusion models and text-to-image synthesis to create advanced retail solutions such as generative image content and personalized visual recommendations.; Vision-Language Models: Using models like CLIP, BLIP, and Flamingo that integrate vision and language modalities to enhance customer understanding and content personalization in retail.; Deep Learning Frameworks for Neural Networks: Utilizing PyTorch and TensorFlow specifically for developing and deploying neural network architectures such as CNNs, vision transformers, and diffusion-based generative models.; Diffusion Models: Leading innovation in image generative modeling using diffusion-based generative models aligned with retail contexts for image generation and manipulation.; Vision Transformers: Applying vision transformer architectures (ViTs) for advanced image understanding and personalization tasks in retail.; Multimodal Foundation Models: Building and scaling pipelines that leverage multimodal signals (text, images, embeddings) to power personalized visual experiences and generative AI applications.; MLOps for AI Models: Designing model workflows integrated with MLOps to optimize performance and latency of AI models, especially for customer-facing visual features.","['Computer Vision', 'Deep Learning', 'Multimodal Models', 'Data Pipelines', 'Python', 'Machine Learning', 'Open Source Frameworks', 'Image Modeling Libraries', 'Data Science']","Computer Vision: Used to solve retail problems by applying algorithms for object detection, image-based personalization, visual similarity, visual search, and classification across a large product catalog.; Deep Learning: Designing, training, and deploying models such as CNNs and vision transformers for image understanding, personalization, and generative image content in retail applications.; Multimodal Models: Leveraging models that combine text, images, and customer embeddings to create highly personalized visual experiences and richer customer understanding.; Data Pipelines: Experience working with large-scale datasets and building scalable data pipelines to support distributed training and model deployment.; Python: Proficiency in Python programming language used for developing deep learning models and data processing.; Machine Learning: Applying machine learning techniques including optimization models and predictive analytics relevant to retail and e-commerce contexts.; Open Source Frameworks: Using frameworks such as scikit-learn, TensorFlow, and PyTorch for building and deploying machine learning and deep learning models.; Image Modeling Libraries: Utilizing libraries like OpenCV, torchvision, and Hugging Face for image processing, manipulation, and model development.; Data Science: Applying statistical and analytical methods to extract insights and build models that support retail solutions and personalization."
TlSdO-H7sbBty-NgAAAAAA==,"['Generative AI', 'Large Language Models', 'PyTorch', 'TensorFlow', 'LLM Fine-Tuning', 'Retrieval-Augmented Generation', 'NLP Applications', 'Transformer Neural Networks', 'ML Model Monitoring']","Generative AI: Apply generative AI techniques to synthesize time series data, enhance model training pipelines, and build intelligent NLP-based assistants for analytics enablement.; Large Language Models: Build and maintain production-ready large language model (LLM) applications integrated with internal systems to deliver scalable insights.; PyTorch: Use PyTorch for developing and training deep learning models, including transformer-based neural networks.; TensorFlow: Employ TensorFlow/Keras frameworks for designing and training deep neural networks applied to time series and NLP tasks.; LLM Fine-Tuning: Engage in fine-tuning large language models to adapt them for specific business applications and improve performance.; Retrieval-Augmented Generation: Implement retrieval-augmented generation (RAG) techniques to enhance NLP applications by integrating external knowledge retrieval with generative models.; NLP Applications: Develop NLP-based intelligent assistants and applications to support analytics and decision-making.; Transformer Neural Networks: Train and deploy transformer architectures such as GPT variants for real-world business applications involving sequence modeling and language understanding.; ML Model Monitoring: Monitor machine learning models in production for drift detection and implement retraining strategies to maintain model accuracy over time.","['Time Series Forecasting', 'Advanced Regression', 'Feature Engineering', 'Big Data Processing', 'Machine Learning', 'Neural Networks', 'Transformer Models', 'Model Deployment Pipelines', 'SQL', 'Python', 'Scikit-learn', 'Statsmodels', 'Spark', 'Ray', 'GPU-based Model Training', 'Multi-GPU and TPU Workflows', 'Econometrics', 'Optimization Theory']","Time Series Forecasting: Develop scalable forecasting systems using advanced global models to enable robust predictions across thousands of retail and e-commerce time series.; Advanced Regression: Apply foundational knowledge of advanced regression techniques to support forecasting and predictive modeling tasks.; Feature Engineering: Perform feature engineering on large datasets using distributed compute platforms like Spark and Ray to improve model performance.; Big Data Processing: Handle large-scale data processing tasks leveraging distributed computing frameworks such as Spark and Ray.; Machine Learning: Utilize machine learning methods including optimization models and sequence modeling to build predictive models and support decision-making.; Neural Networks: Design and train large-scale neural networks for time series analysis, anomaly detection, and causal inference.; Transformer Models: Train and deploy transformer-based neural networks such as Temporal Fusion Transformers (TFT) and LSTM for sequence modeling and forecasting.; Model Deployment Pipelines: Deploy models via automated batch pipelines using orchestration tools like Airflow or Astronomer to ensure production readiness.; SQL: Use SQL for data querying and manipulation as part of data processing and feature engineering workflows.; Python: Employ Python programming for data science tasks including model development, data processing, and experimentation.; Scikit-learn: Leverage scikit-learn for implementing classical machine learning algorithms and statistical modeling.; Statsmodels: Use statsmodels for statistical modeling and econometrics analysis relevant to forecasting and causal inference.; Spark: Utilize Apache Spark for distributed data processing and feature engineering on big data.; Ray: Apply Ray for distributed computing to scale data processing and model training tasks.; GPU-based Model Training: Work on GPU-accelerated pipelines to train complex models efficiently at scale.; Multi-GPU and TPU Workflows: Experience with distributed training using multi-GPU setups or TPU-based workflows to optimize model training performance.; Econometrics: Apply econometric methods and domain knowledge in retail, demand forecasting, and e-commerce analytics to improve model relevance.; Optimization Theory: Use optimization theory principles to enhance model training and forecasting accuracy."
bwefpvyZgb9Xr6OzAAAAAA==,"['Large Language Models (LLMs)', 'Prompt Engineering', 'Retrieval-Augmented Generation', 'Conversational AI and Autonomous Agents', 'Trustworthy AI and Responsible ML']","Large Language Models (LLMs): The role focuses on developing and deploying LLM-powered intelligent experiences, including building personalized Q&A systems, fine-tuning LLMs, and creating conversational interfaces that enhance associate productivity and communication.; Prompt Engineering: Experience with prompt engineering is required for customizing and optimizing large language models to generate context-aware responses tailored to user needs.; Retrieval-Augmented Generation: The job involves working with retrieval-augmented generation (RAG) techniques to enhance LLM capabilities by integrating external data retrieval for improved response accuracy and relevance.; Conversational AI and Autonomous Agents: Designing conversational talent recommendation systems and constructing multi-agent intelligent workflows that translate natural language inputs into complex, goal-directed task sequences are key responsibilities, involving autonomous agent architectures.; Trustworthy AI and Responsible ML: The position requires advancing and implementing trustworthy AI and responsible machine learning practices, ensuring safety, fairness, privacy, and regulatory compliance in high-risk NLP applications deployed in production environments.","['Machine Learning', 'Deep Learning Frameworks', 'Python and ML/DS Libraries', 'Recommendation Systems', 'Data Science', 'Machine Learning Infrastructure', 'Text-to-SQL and Text-to-Cypher Applications']","Machine Learning: The role involves strong applied machine learning experience with foundational knowledge in statistics and optimization, designing and deploying scalable machine learning models, and collaborating with data scientists and machine learning engineers to develop AI/ML models and system architectures.; Deep Learning Frameworks: The job requires advanced proficiency in deep learning frameworks such as TensorFlow and PyTorch, including experience in neural network architecture optimization, model distillation, quantization, and on-device inference.; Python and ML/DS Libraries: Advanced proficiency in Python and common machine learning and data science libraries such as NumPy, pandas, and scikit-learn is essential for developing and deploying AI/ML solutions.; Recommendation Systems: Design and enhancement of conversational talent recommendation systems and modern recommender systems are part of the responsibilities, evolving traditional ranked list approaches to multi-topic, interactive experiences reflecting user intent.; Data Science: The position requires expertise in data science, including interpreting and generating insights from tabular and unstructured data, and collaborating with data scientists to prototype and iterate on models.; Machine Learning Infrastructure: Experience with machine learning infrastructure tools such as Kubeflow, MLflow, and Airflow is considered a plus for deploying, monitoring, and optimizing scalable AI/ML solutions in production environments.; Text-to-SQL and Text-to-Cypher Applications: Hands-on experience with Text-to-SQL or Text-to-Cypher based applications is valued, indicating work with translating natural language queries into database queries as part of intelligent systems."
M_ZRUaefrohfGkDxAAAAAA==,"['Large Language Models', 'Generative AI', 'TensorFlow', 'PyTorch']","Large Language Models: Applied to solve logistics challenges and improve routing efficiency as part of the Last Mile Delivery platform.; Generative AI: Integrated into the Last Mile Delivery platform to enhance algorithmic decision-making and operational performance.; TensorFlow: Used as an open-source framework for developing AI models, including neural networks, within the data science team.; PyTorch: Used as an open-source framework for building and training AI models, including neural networks, in the context of advanced AI applications.","['Advanced Machine Learning Techniques', 'Optimization Models', 'Large-Scale Data Analysis', 'Python', 'Spark', 'Scala', 'R', 'Scikit-learn', 'Cloud Platforms (e.g., GCP)']","Advanced Machine Learning Techniques: Used to solve high-impact logistics challenges and improve routing efficiency within the Last Mile Delivery platform.; Optimization Models: Applied to design and optimize algorithms for Last Mile Delivery to enhance operational efficiency and cost structure.; Large-Scale Data Analysis: Conducted to identify opportunities for operational and business improvements in Last Mile Delivery.; Python: One of the programming languages assessed and used for data science and machine learning tasks.; Spark: Used as a framework for handling large-scale data processing and analytics.; Scala: Used as a programming language for data processing and analytics tasks.; R: Used for statistical analysis and data science tasks.; Scikit-learn: An open-source framework used for implementing machine learning models and algorithms.; Cloud Platforms (e.g., GCP): Utilized to deploy and manage machine learning and data science solutions at scale."
WvC-xZO3_loct8pSAAAAAA==,"['Generative AI', 'Large Language Models (LLMs)', 'AI Model Fine-Tuning', 'AI MLOps', 'AI Agents and Orchestration']","Generative AI: Leading projects involving generative AI applications, including model selection, training, fine-tuning, and deployment to create innovative AI solutions.; Large Language Models (LLMs): Hands-on experience with training, fine-tuning, evaluating, and deploying transformer-based LLMs in production, and collaborating on AI algorithms involving LLMs.; AI Model Fine-Tuning: Fine-tuning AI models, particularly transformer-based models, to tailor them for specific business use cases and improve performance.; AI MLOps: Building and managing AI-specific ML operations pipelines that include deployment, monitoring, and retraining of AI models, especially LLMs and generative AI models, using AWS services.; AI Agents and Orchestration: Designing, deploying, and evaluating AI agents and orchestration approaches using frameworks such as LangChain, LangGraph, and LlamaIndex to build AI-native solutions.","['Machine Learning', 'Transformer Models', 'AWS Machine Learning Services', 'MLOps', 'Deep Learning Frameworks', 'AI Agents and Orchestration Frameworks']","Machine Learning: Used to build models for business applications, including training, fine-tuning, evaluating, and deploying models in production to solve real-world problems and optimize business outcomes.; Transformer Models: Experience required in training, fine-tuning, evaluating, and deploying transformer models in production environments as part of AI/ML solutions.; AWS Machine Learning Services: Utilization of AWS cloud services such as Amazon SageMaker for building, training, deploying, and managing machine learning models and pipelines.; MLOps: Building and managing ML systems and operations including data preprocessing, distributed and GPU training, model deployment, monitoring, and retraining, following best practices and using container and CI/CD pipelines.; Deep Learning Frameworks: Experience with PyTorch and TensorFlow for implementing deep learning, computer vision, and robotics algorithms.; AI Agents and Orchestration Frameworks: Design, deployment, and evaluation of AI agents and orchestration approaches using open source frameworks like LangChain, LangGraph, and LlamaIndex."
CBAdXkHc6Pv_MTV_AAAAAA==,['Deep Learning'],Deep Learning: Design and deploy deep learning algorithms specifically to enhance cyber analytic capabilities and predictive modeling.,"['Machine Learning', 'Deep Learning', 'Natural Language Processing', 'Statistical Data Analysis', 'Data Mining', 'Predictive Modeling', 'Feature Engineering', 'Recommendation Systems', 'Knowledge Engineering', 'Experimental Design', 'SQL', 'Graph Theory and Network Analysis']","Machine Learning: Develop machine learning models to create actionable insights and automate scoring in cyber security analytics.; Deep Learning: Design and deploy deep learning algorithms and predictive models to support cyber analytic capabilities.; Natural Language Processing: Apply NLP techniques such as question answering, text mining, and information retrieval to analyze cyber security data.; Statistical Data Analysis: Use mathematical and statistical rigor to analyze large data sets and validate hypotheses.; Data Mining: Employ data mining methods to extract useful information from large and complex data sets.; Predictive Modeling: Develop predictive models to anticipate cyber threats and vulnerabilities.; Feature Engineering: Select and engineer relevant data points from large data sets for analysis and model development.; Recommendation Systems: Build recommendation systems as part of machine learning applications in cyber security.; Knowledge Engineering: Convert knowledge elicited from subject matter experts into derived algorithms for cyber analytic purposes.; Experimental Design: Apply experimental design principles to validate hypotheses and improve algorithmic outcomes.; SQL: Use database querying skills to access and manipulate data for analysis.; Graph Theory and Network Analysis: Utilize graph theory and network analysis techniques to understand relationships and structures in cyber data."
-NpPrxS4_eufwQxnAAAAAA==,['Large Language Models'],"Large Language Models: Implemented and applied in NLP tasks such as document classification, extraction, summarization, and search, indicating use of modern AI techniques involving LLMs.","['Python', 'Natural Language Processing', 'Large Language Models', 'Data Exploration', 'Data Cleaning', 'Data Analysis', 'Data Visualization', 'Data Mining', 'Production-Level Systems', 'Data Lakes', 'Streaming Data', 'Kafka', 'Machine Learning Workflows', 'ML Models for Document Tasks', 'MLOps', 'R', 'SQL and NoSQL', 'Distributed Computing Tools', 'Visualization Packages']","Python: Used for applied data science and machine learning roles, including algorithm development and data analysis.; Natural Language Processing: Applied for document classification, extraction, summarization, and search tasks within machine learning workflows.; Large Language Models: Implemented as part of NLP and machine learning solutions to handle document-related tasks.; Data Exploration: Involves examining and understanding data sets as part of data cleaning, analysis, and mining processes.; Data Cleaning: Performed to prepare data for analysis and machine learning model development.; Data Analysis: Used to extract insights and inform decision-making from complex data sets.; Data Visualization: Utilized to communicate data insights effectively, employing tools such as Plotly, Seaborn, and ggplot2.; Data Mining: Applied to discover patterns and relationships within large data sets.; Production-Level Systems: Experience working with systems that handle real-time or large-scale data processing, including streaming data environments.; Data Lakes: Used as storage environments for large volumes of structured and unstructured data, interfacing with data pipelines.; Streaming Data: Handled data streams, including millions of documents per week, often using Kafka for ingestion and processing.; Kafka: Employed as a streaming platform to manage real-time data flows within production systems.; Machine Learning Workflows: Implemented end-to-end processes from data preparation to model deployment and evaluation.; ML Models for Document Tasks: Designed and iterated on models specifically for document classification, extraction, summarization, and search.; MLOps: Collaborated with engineers to ensure robust deployment, monitoring, and retraining of machine learning models.; R: Used for algorithm development alongside Python and SQL/NoSQL databases.; SQL and NoSQL: Applied for data querying and management in algorithm development and data processing.; Distributed Computing Tools: Experience with MapReduce, Hadoop, Hive, EMR, Spark, and Gurobi to handle large-scale data processing and optimization tasks.; Visualization Packages: Utilized Plotly, Seaborn, and ggplot2 to create visual representations of data insights."
GZLlvlCrp8E8rvQKAAAAAA==,[],,"['Machine Learning', 'Deep Learning', 'Python', 'Distributed Computing with PySpark', 'SQL', 'Data Extraction, Transformation, and Loading (ETL)', 'Data Visualization', 'Data Cleaning and Preprocessing', 'Model Deployment and MLOps', 'Predictive Analytics', 'ROI Analysis', 'Mentorship and Leadership in Data Science']","Machine Learning: Develop, test, and fine-tune machine learning models for enterprise-level supply chain and logistics products; apply advanced machine learning techniques in a business environment.; Deep Learning: Apply advanced deep learning techniques as part of machine learning applications in a business context.; Python: Use Python or other high-level scripting languages for programming and model development.; Distributed Computing with PySpark: Leverage distributed computing frameworks such as PySpark to handle large-scale data processing and analytics.; SQL: Utilize SQL for data querying and management of structured data.; Data Extraction, Transformation, and Loading (ETL): Extract, blend, cleanse, and organize data using automated ETL processes to prepare data for analysis and modeling.; Data Visualization: Visualize data to facilitate understanding and support decision-making processes.; Data Cleaning and Preprocessing: Identify and address outliers, missing, or incomplete records in datasets to ensure data quality for analysis and modeling.; Model Deployment and MLOps: Familiarity with model deployment and MLOps practices, including exposure to containerization technologies like Docker and orchestration systems like Kubernetes, to support production-level model management.; Predictive Analytics: Establish and advance predictive analytics capabilities at the enterprise level to support supply chain and logistics business decisions.; ROI Analysis: Conduct comprehensive return on investment analysis to evaluate the feasibility of potential projects.; Mentorship and Leadership in Data Science: Mentor junior data scientists, guide best practices in modeling activities, and foster professional growth within the team."
mJTOv-IE2zQAxuBZAAAAAA==,"['Vision Transformers', 'Generative AI', 'PyTorch']",Vision Transformers: Utilizing Vision Transformer architectures as part of modern computer vision solutions within the data science projects.; Generative AI: Applying generative AI techniques related to image generation as part of computer vision tasks.; PyTorch: Employing PyTorch deep learning library specifically for neural network and AI model development.,"['Data Wrangling and Processing', 'Statistical Modeling', 'Machine Learning Algorithms', 'Python', 'SQL', 'Spark', 'Data Visualization and Dashboards', 'Model Development Lifecycle', 'Computer Vision', 'Deep Learning Libraries', 'Cloud-Native Technology', 'Large Structured and Unstructured Data']","Data Wrangling and Processing: Ingesting, cleansing, verifying, enriching, and processing data from multiple sources to prepare it for analysis and modeling.; Statistical Modeling: Applying advanced statistical models to extract insights and solve business problems.; Machine Learning Algorithms: Using a wide range of machine learning techniques to build predictive models that address various business challenges.; Python: Utilizing Python programming language and its associated packages commonly used by data scientists for data analysis and model development.; SQL: Querying and managing structured data stored in relational databases.; Spark: Employing Apache Spark and its Python packages for large-scale data processing and analytics.; Data Visualization and Dashboards: Developing visualizations and dashboards to monitor organizational performance, generate insights, and improve user experience.; Model Development Lifecycle: Participating in the end-to-end process of model development including business problem discovery, data exploration, model building, deployment, and monitoring.; Computer Vision: Applying computer vision techniques and libraries such as OpenCV and PIL, including modern architectures like CNNs and Vision Transformers, to analyze image data.; Deep Learning Libraries: Using deep learning frameworks such as PyTorch for building and training neural network models.; Cloud-Native Technology: Working with cloud-native platforms and technologies to support scalable data science and engineering workflows.; Large Structured and Unstructured Data: Handling and analyzing large datasets stored in both relational and NoSQL databases with complex relationships and time scales."
AN7jrONXj9RSGt1JAAAAAA==,"['Deep Learning Frameworks (PyTorch, TensorFlow)']","Deep Learning Frameworks (PyTorch, TensorFlow): Applied specifically for neural network-based machine learning model development and deployment within the data science team.","['Big Data Analytics', 'Machine Learning Models', 'SQL and NoSQL Databases', 'Python and PySpark', 'ML Frameworks (PyTorch, TensorFlow, JAX)', 'CI/CD Frameworks', 'Optimization Models', 'Open Source Frameworks (scikit-learn, TensorFlow, Torch)']","Big Data Analytics: Used to analyze high volumes of data to understand business problems and propose technical solutions, supporting strategic initiatives and business insights.; Machine Learning Models: Developed and iterated from prototypes to production deployment to solve complex business problems and evaluate product features.; SQL and NoSQL Databases: Proficiency required for working with database technologies and distributed datastores to support data solutions.; Python and PySpark: Used for writing production code and handling big data processing within the data science workflows.; ML Frameworks (PyTorch, TensorFlow, JAX): Expertise required in at least one machine learning framework to develop and deploy machine learning models.; CI/CD Frameworks: Experience with continuous integration and continuous deployment frameworks to support production-level code and model deployment.; Optimization Models: Applied to solve business problems and improve decision-making processes as part of data science responsibilities.; Open Source Frameworks (scikit-learn, TensorFlow, Torch): Used for building machine learning models and analytics solutions leveraging popular open source tools."
iDjdplG40ufZFVE9AAAAAA==,['Generative AI'],Generative AI: Understanding and basic usage of generative AI tools like ChatGPT and Claude to identify opportunities for improving team efficiency and product strategy integration.,"['Statistical and Quantitative Modeling', 'Data Mining', 'Clustering and Segmentation', 'SQL', 'R', 'Python', 'Data Storage and ETL Frameworks', 'Data Transformation and Validation', 'BI Dashboards (Looker, Tableau)', 'Data Pipelines', 'Experimentation and A/B Testing', 'dbt']","Statistical and Quantitative Modeling: Used to analyze large healthcare datasets to derive insights and support data-driven decision making in a healthcare analytics context.; Data Mining: Applied to extract patterns and segment data for clustering and segmentation tasks relevant to healthcare analytics.; Clustering and Segmentation: Techniques used to group data points for better understanding of member populations and product usage patterns.; SQL: Used as a data science tool for querying and managing large datasets from the data warehouse.; R: Utilized as a programming language for data analysis and visualization in healthcare analytics.; Python: Employed for data science tasks including data transformation, analysis, and building data pipelines.; Data Storage and ETL Frameworks: Advanced understanding required to manage data storage, perform extraction, transformation, and loading of data to ensure data quality and availability.; Data Transformation and Validation: Processes to convert raw clinical, member, and claims data into clean, usable formats and to ensure data accuracy for analysis and reporting.; BI Dashboards (Looker, Tableau): Building interactive dashboards and reports to visualize KPIs and product metrics that inform business and product decisions.; Data Pipelines: Designing and building scalable data transformation pipelines to support data flows from raw data to actionable insights.; Experimentation and A/B Testing: Leading and enabling frequent, small experiments to speed learning and inform product decisions through data-driven experimentation.; dbt: Used for building data transformation pipelines to support scalable and maintainable data workflows."
KRaSg822gWoBsIzeAAAAAA==,"['Deep Learning Frameworks (PyTorch, TensorFlow)']","Deep Learning Frameworks (PyTorch, TensorFlow): Applied specifically for neural network-based machine learning model development and deployment within the data science team.","['Big Data Analytics', 'Machine Learning Models', 'SQL and NoSQL Databases', 'Python and PySpark', 'ML Frameworks (PyTorch, TensorFlow, JAX)', 'CI/CD Frameworks', 'Data Science and Analytics', 'Optimization Models', 'Spark and Scala', 'Scikit-learn']","Big Data Analytics: Used to analyze high volumes of data to understand business problems and drive strategic decisions within the organization.; Machine Learning Models: Developed and iterated from prototypes to production deployment to solve complex business problems and evaluate product features.; SQL and NoSQL Databases: Proficiency required for working with database technologies and distributed datastores to support data solutions.; Python and PySpark: Used for writing production code and handling big data processing tasks within machine learning and analytics workflows.; ML Frameworks (PyTorch, TensorFlow, JAX): Expertise required to develop and deploy machine learning models using popular open-source frameworks.; CI/CD Frameworks: Experience with continuous integration and continuous deployment frameworks to support production-level machine learning model deployment and automation.; Data Science and Analytics: Applied to deliver insightful analytics, translate business problems into data solutions, and support strategic initiatives.; Optimization Models: Used as part of advanced analytics and machine learning approaches to solve business problems.; Spark and Scala: Mentioned as part of assessments and skills relevant for big data processing and analytics.; Scikit-learn: Used as an open-source framework for machine learning model development."
2gTZt9EtBBLTOfd6AAAAAA==,"['Generative AI', 'Deep Learning']","Generative AI: Explored as part of research efforts to innovate financial product recommendations and personalization at Credit Karma.; Deep Learning: Investigated and applied in the context of advancing AI capabilities for personal finance products, including recommender systems.","['Machine Learning', 'Statistical Modeling', 'Deep Neural Networks', 'Collaborative Filtering', 'Matrix Factorization', 'Time Series Analysis', 'Feature Engineering', 'Python/R', 'SQL', 'Recommender Systems', 'Data Science Techniques', 'Experiment Design and A/B Testing']","Machine Learning: Applied to solve financial challenges and drive monetization, personalization, and value-centric experiences for members at Credit Karma.; Statistical Modeling: Used to develop advanced models including mixed-effect models and to provide solid statistical bases for designing experiments and defining business metrics.; Deep Neural Networks: Employed as an advanced modeling technique to improve financial product recommendations and personalization.; Collaborative Filtering: Utilized as a modeling technique to enhance recommender systems for personal finance products.; Matrix Factorization: Applied as an advanced modeling technique to support recommendation and personalization systems.; Time Series Analysis: Used to analyze temporal data patterns relevant to financial products and user behavior.; Feature Engineering: Involves creating user profiles and behavior features to improve targeting, marketing campaigns, and personalization models.; Python/R: Authoritative programming languages used for data science and machine learning model development.; SQL: Used for querying and managing extensive data to support data-driven decision making and model development.; Recommender Systems: Developed and researched to provide actionable financial product recommendations to members.; Data Science Techniques: Applied broadly to power relevant financial products and actionable recommendations, including driving revenue and engagement improvements.; Experiment Design and A/B Testing: Used to define and quantify business metrics such as revenue, engagement, and user experience through statistically sound experiments."
JVN5an6JjGzZrIZoAAAAAA==,"['Generative AI', 'Large Language Models', 'Deep Learning Frameworks', 'AI Model Deployment and MLOps', 'AI Strategy and Consulting', 'Edge AI and Autonomous Systems', 'AI Hardware Optimization', 'Retrieval-Augmented Generation', 'Cloud AI Services']","Generative AI: Experience with LLM/GenAI use cases and developing Retrieval-Augmented Generation solutions, tools, and services such as LangChain, LangGraph, and MCP.; Large Language Models: Involves working with LLMs in use cases related to generative AI and AI service development.; Deep Learning Frameworks: Use of frameworks like PyTorch specifically for developing and deploying neural network-based AI models.; AI Model Deployment and MLOps: Deploying and optimizing AI/ML models using Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow, with a focus on AI model lifecycle management.; AI Strategy and Consulting: Guiding clients with high autonomy in AI strategy and development, including thought leadership, long-term maintenance, and aligning AI solutions with business objectives.; Edge AI and Autonomous Systems: Working on novel projects involving autonomous systems and edge AI as part of AI service development.; AI Hardware Optimization: Researching and implementing hardware optimization techniques to advance state-of-the-art AI training and solution design.; Retrieval-Augmented Generation: Developing RAG solutions and tools to enhance generative AI capabilities.; Cloud AI Services: Experience with AWS SageMaker and AWS ML Studio for deploying and managing AI/ML workloads.","['Exploratory Data Analysis', 'Machine Learning', 'Deep Learning', 'Time-Series Analysis', 'Natural Language Processing', 'Computer Vision', 'Model Validation and Testing', 'Model Deployment and Optimization', 'Cloud Computing for AI/ML', 'Data Strategy']","Exploratory Data Analysis: Used to understand client data sets and operational requirements to design long-term data science solutions.; Machine Learning: Applied in developing AI/ML solutions, including traditional ML algorithm development, model tuning, performance validation, and deployment in production environments.; Deep Learning: Utilized techniques such as CNNs, RNNs, and GANs across real-world projects, including model tuning and performance validation.; Time-Series Analysis: Applied as part of data analysis methods in AI/ML algorithm development.; Natural Language Processing: Used as a data analysis technique within AI/ML algorithm development.; Computer Vision: Employed as a data analysis method in AI/ML algorithm development.; Model Validation and Testing: Includes validating AI models and algorithms via code reviews, unit tests, and integration tests to ensure quality and performance.; Model Deployment and Optimization: Involves deploying and optimizing ML models using tools like Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow.; Cloud Computing for AI/ML: Leveraging cloud environments such as AWS, Azure, or GCP to deploy AI/ML workloads.; Data Strategy: Defining data strategy to drive technical development and create next-generation tools, products, and AI services."
Ql269g0qeg8rzPDQAAAAAA==,['Artificial Intelligence'],"Artificial Intelligence: Relevant experience includes artificial intelligence, indicating involvement with AI concepts or technologies as part of the Senior Data Scientist role.","['Machine Learning', 'Data Science', 'Statistical Analysis', 'Data Analytics', 'Data Management', 'Data Mining', 'Data Modeling and Assessment']","Machine Learning: Designing and implementing machine learning models and algorithms as part of relevant experience for the Senior Data Scientist role.; Data Science: Applying data science principles including data management, data mining, data modeling, and advanced analytical algorithms to support analysis automation and scaling.; Statistical Analysis: Utilizing statistical methods such as variability analysis, sampling error, inference, hypothesis testing, exploratory data analysis (EDA), and linear models in the context of data science tasks.; Data Analytics: Building and maintaining custom data analytics solutions to automate and scale data analysis processes.; Data Management: Handling and organizing data effectively as part of the data science and analytics responsibilities.; Data Mining: Extracting useful information and patterns from large datasets to support analytical objectives.; Data Modeling and Assessment: Creating and evaluating data models to support decision-making and analysis automation."
9URUKTsZ6I8sQh_wAAAAAA==,[],,"['Big Data Analytics', 'Machine Learning Models', 'SQL and NoSQL Databases', 'Python and PySpark', 'ML Frameworks (PyTorch, TensorFlow, JAX)', 'CI/CD Frameworks', 'Optimization Models', 'Open Source Frameworks (scikit-learn, TensorFlow, Torch)']","Big Data Analytics: Used to analyze high volumes of data to understand business problems and propose technical solutions, supporting strategic initiatives and business insights.; Machine Learning Models: Developed and iterated from prototypes to production deployment to solve complex business problems and evaluate product features.; SQL and NoSQL Databases: Proficiency required for working with database technologies and distributed datastores to support data solutions.; Python and PySpark: Used for writing production code and handling big data processing within the data science workflows.; ML Frameworks (PyTorch, TensorFlow, JAX): Expertise required in at least one machine learning framework to develop and deploy machine learning models.; CI/CD Frameworks: Experience with continuous integration and continuous deployment frameworks to support production code and model deployment.; Optimization Models: Applied to solve business problems and improve decision-making processes as part of data science responsibilities.; Open Source Frameworks (scikit-learn, TensorFlow, Torch): Used for building machine learning models and analytics solutions within the data science team."
0abUPN9AxuM80uqSAAAAAA==,[],,"['SQL', 'Python', 'R', 'Scala', 'Statistical Models', 'Machine Learning', 'Data Visualization Tools', 'Data Pipelines', 'Anomaly Detection', 'Data Analysis', 'Statistical Definitions of Outliers']","SQL: Used for querying and manipulating data as part of data analysis and building data science solutions.; Python: Used for scripting to obtain, manipulate, and analyze data, and to build machine learning and statistical models.; R: Used for statistical analysis, scripting, and building machine learning and statistical models.; Scala: Used as a programming language to write code for data manipulation and analysis.; Statistical Models: Includes models such as multinomial logistic regression used to solve specific business problems and identify outliers.; Machine Learning: Applied to build predictive and decision-making models to address business problems and improve system performance.; Data Visualization Tools: Experience with tools like AWS QuickSight, Tableau, and R Shiny to visualize data and communicate insights.; Data Pipelines: Experience managing data pipelines to support data processing and analysis workflows.; Anomaly Detection: Developing methods and scripts to define, identify, and explain anomalies or outliers in data to support decision making.; Data Analysis: Analyzing historical data to identify trends and support optimal decision making.; Statistical Definitions of Outliers: Formalizing assumptions and creating statistical criteria to systematically identify outliers in data."
GswHAqACfpS8GXwSAAAAAA==,['Generative AI'],Generative AI: Familiarity with usage for productivity improvement and knowledge of open-source and gated/paid model landscapes relevant to enhancing data science workflows.,"['Machine Learning', 'Data Mining', 'Text Mining', 'Regression Models', 'Decision Trees', 'Probability Networks', 'Association Rules', 'Clustering', 'Neural Networks', 'Bayesian Models', 'SQL', 'Python', 'R', 'PySpark', 'Cloud-based Technology Stack']","Machine Learning: Used for developing algorithms and models such as regression, decision trees, probability networks, association rules, clustering, neural networks, and Bayesian models to influence decisions in omnichannel sales, marketing optimization, distribution demands, patient/payer analytics, and commercial strategy.; Data Mining: Applied to extract useful patterns and insights from large healthcare and commercial datasets to support business value and decision-making.; Text Mining: Utilized for analyzing textual data within healthcare and commercial contexts to derive insights and support data science projects.; Regression Models: Employed as part of machine learning techniques to model relationships in data for predictive analytics in commercial and healthcare applications.; Decision Trees: Used as a machine learning method to support classification and regression tasks within commercial strategy and patient analytics.; Probability Networks: Applied as AI-type algorithms to model probabilistic relationships in data for decision-making processes.; Association Rules: Used to discover interesting relations between variables in large datasets relevant to commercial and healthcare analytics.; Clustering: Implemented to group similar data points for segmentation and pattern recognition in sales, marketing, and patient analytics.; Neural Networks: Applied as part of machine learning techniques to model complex patterns in data for commercial and healthcare insights.; Bayesian Models: Used for probabilistic modeling and inference in healthcare and commercial data science projects.; SQL: Proficiency required for querying and managing large healthcare and commercial datasets to support data analysis workflows.; Python: Used as a primary programming language for developing data science solutions, machine learning models, and data analysis workflows.; R: Utilized as a programming language for statistical analysis and data science project development.; PySpark: Experience preferred for handling large-scale data processing and analytics in cloud-based environments.; Cloud-based Technology Stack: Familiarity required for deploying and managing data science projects and workflows in scalable cloud environments."
0taZ7SRDFhgbfrKTAAAAAA==,['Generative AI'],Generative AI: Understanding and basic usage of generative AI tools like ChatGPT and Claude to identify opportunities for improving team efficiency and integrating AI into product strategy.,"['Statistical and Quantitative Modeling', 'SQL', 'R', 'Python', 'ETL Frameworks', 'Data Transformation and Validation', 'BI Dashboards (Looker, Tableau)', 'Data Pipelines', 'Data-Driven Decision Making', 'Experimentation and A/B Testing', 'Data Mining, Clustering, and Segmentation', 'dbt (Data Build Tool)']","Statistical and Quantitative Modeling: Used to analyze large healthcare datasets and conduct data mining, clustering, and segmentation to derive insights relevant to healthcare analytics.; SQL: Employed for querying and managing data within the data warehouse to support data transformation and analysis.; R: Used as a data science tool for statistical analysis, modeling, and visualization in healthcare analytics.; Python: Utilized for programming, data analysis, and building data transformation pipelines in healthcare analytics.; ETL Frameworks: Applied to extract, transform, and load clinical, member, and claims data from the data warehouse into usable formats for analysis and reporting.; Data Transformation and Validation: Involves designing and building data flows and pipelines to ensure data quality and readiness for analysis and reporting.; BI Dashboards (Looker, Tableau): Used to build interactive dashboards and reports that answer real business questions and monitor KPIs and product metrics.; Data Pipelines: Built and maintained to support scalable systems and complex data structures, enabling end-to-end data processing and actionable recommendations.; Data-Driven Decision Making: Driving product and business decisions by prioritizing analyses that inform outcomes, reframing analysis requests, and delivering rapid, impactful insights.; Experimentation and A/B Testing: Leading and enabling frequent, small experiments to speed learning and guide product strategy through data-driven hypotheses and testing.; Data Mining, Clustering, and Segmentation: Techniques used to analyze healthcare data for identifying patterns, user segments, and strategic opportunities.; dbt (Data Build Tool): Used for building data transformation pipelines to support scalable and maintainable data workflows."
4xzCSCAbsA-A_pxzAAAAAA==,[],,"['Machine Learning', 'Descriptive and Inferential Statistics', 'Time Series Data', 'Large-Scale Data Handling', 'Programming Languages', 'Relational SQL Databases']","Machine Learning: The job requires a deep understanding of modern machine learning techniques and algorithms to model and predict financial markets using sophisticated methods.; Descriptive and Inferential Statistics: Exceptional aptitude in descriptive and inferential statistics is necessary to analyze and interpret complex data effectively.; Time Series Data: Experience with time series data is important for modeling and forecasting financial market behaviors over time.; Large-Scale Data Handling: The role involves working daily with complex, large-scale datasets, requiring extensive experience managing and processing big data.; Programming Languages: Competent skills in programming languages such as Python, R, C, C#, and C++ are required to implement data science models and analyses.; Relational SQL Databases: Experience with relational SQL databases is preferred for managing and querying structured data relevant to financial modeling."
BBYOQrTo6n_mgd3XAAAAAA==,"['Deep Learning', 'PyTorch', 'Generative AI', 'Large Language Models', 'Prompt Engineering', 'AI Model Deployment and MLOps', 'Cloud AI Services']","Deep Learning: The role requires applying deep learning techniques such as convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs) across real-world projects, including model tuning and performance validation in production.; PyTorch: PyTorch is used as a core framework for AI/ML algorithm development, particularly for deep learning model training and implementation.; Generative AI: Experience with generative AI use cases, including developing retrieval-augmented generation (RAG) solutions, tools, and services such as LangChain, LangGraph, and MCP, is preferred.; Large Language Models: The job involves working with large language models (LLMs) and generative AI technologies, including fine-tuning and deploying these models for client solutions.; Prompt Engineering: The role includes developing and implementing prompt engineering techniques as part of generative AI and LLM use cases.; AI Model Deployment and MLOps: Deploying and optimizing AI models using Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow, with a focus on managing AI/ML workloads in cloud environments, is a key responsibility.; Cloud AI Services: Experience with cloud AI services such as AWS Sagemaker and AWS ML Studio is preferred for deploying and managing AI/ML workloads.","['Machine Learning', 'Data Analysis', 'Time-Series Analysis', 'Natural Language Processing', 'Computer Vision', 'Model Validation and Testing', 'Model Deployment and Optimization', 'Cloud Computing for AI/ML', 'Data Strategy']","Machine Learning: The role involves developing AI/ML algorithms using core data science languages and frameworks, applying traditional machine learning techniques such as CNNs, RNNs, and GANs, and tuning and validating models in production environments.; Data Analysis: The job requires performing exploratory data analysis on client data sets to understand operational requirements and drive long-term solutions.; Time-Series Analysis: Experience with time-series analysis is required as part of the data analysis and AI/ML algorithm development responsibilities.; Natural Language Processing: The role includes experience with NLP as part of AI/ML algorithm development and data analysis.; Computer Vision: The job involves applying computer vision techniques as part of AI/ML algorithm development and data analysis.; Model Validation and Testing: Responsibilities include validating AI models and algorithms through code reviews, unit tests, and integration tests to ensure model performance and reliability.; Model Deployment and Optimization: The role requires deploying and optimizing machine learning models using tools such as Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow, and leveraging cloud environments like AWS, Azure, or GCP.; Cloud Computing for AI/ML: Experience in leveraging cloud platforms (AWS, Azure, GCP) to deploy AI/ML workloads is essential for delivering scalable solutions.; Data Strategy: The position involves defining data strategy to guide technical development and create next-generation tools, products, and AI services aligned with client needs."
T-qb2uw_Nq1QFMu1AAAAAA==,['Generative AI'],Generative AI: Understanding of generative AI concepts and basic usage of GenAI tools like ChatGPT and Claude to identify opportunities for improving team efficiency and product strategy integration.,"['Statistical and Quantitative Modeling', 'Data Mining', 'Clustering and Segmentation', 'SQL', 'R', 'Python', 'Data Storage and ETL Frameworks', 'Data Transformation and Validation', 'BI Dashboards (Looker, Tableau)', 'Data Pipelines', 'Experimentation and A/B Testing', 'KPI and Product Metrics Definition and Monitoring', 'Data-Driven Reporting and Analysis', 'dbt (Data Build Tool)']","Statistical and Quantitative Modeling: Used to analyze large healthcare datasets to derive insights and support data-driven decision making in a healthcare analytics context.; Data Mining: Applied to extract patterns and segment data for clustering and segmentation tasks relevant to healthcare analytics.; Clustering and Segmentation: Techniques used to group data points for better understanding of member populations and product usage patterns.; SQL: Used as a primary tool for querying and managing data within the company's data warehouse to support analysis and reporting.; R: Utilized for statistical analysis and data science tasks, including modeling and visualization.; Python: Employed for data science programming, including data transformation, analysis, and building data pipelines.; Data Storage and ETL Frameworks: Advanced understanding required to manage data storage, perform data extraction, transformation, and loading to ensure data quality and availability.; Data Transformation and Validation: Critical for preparing clinical, member, and claims data into actionable formats and ensuring data correctness for analysis and reporting.; BI Dashboards (Looker, Tableau): Used to build interactive dashboards and reports that answer real business questions and support product and clinical decision making.; Data Pipelines: Building and maintaining scalable data pipelines to support complex data structures and enable efficient data flow from warehouse to analytics and reporting.; Experimentation and A/B Testing: Leading and enabling self-serve experimentation to validate hypotheses and speed learning through frequent, small experiments.; KPI and Product Metrics Definition and Monitoring: Defining, monitoring, and analyzing key performance indicators and product metrics to identify trends and drivers that inform strategic decisions.; Data-Driven Reporting and Analysis: Creating impactful reports and analyses that strengthen the company's value proposition and inform product decisions.; dbt (Data Build Tool): Experience building data transformation pipelines using dbt to support scalable and maintainable data workflows."
MH0O1Q6IiBqdB79WAAAAAA==,['Machine Learning'],Machine Learning: Collaborated with team members and business/technical partners to use machine learning for automating and developing innovative solutions in the Home Lending business.,"['Descriptive, Diagnostic, and Predictive Analytics', 'SQL', 'MS Excel', 'Tableau', 'Python', 'Data Integration', 'Business Intelligence Dashboards', 'Advanced Analytics', 'Analytical Frameworks']","Descriptive, Diagnostic, and Predictive Analytics: Used to uncover trends, patterns, and actionable business insights supporting Home Lending Sales strategy and optimization.; SQL: Proficiency required for data manipulation and analysis from various platforms to support business decision-making.; MS Excel: Used for data manipulation and analysis to facilitate business insights and reporting.; Tableau: Used to develop and enhance business intelligence dashboards and automated reporting for decision-making.; Python: Proficiency in coding used to support data analysis and development of innovative solutions in the Mortgage business.; Data Integration: Sourcing and integrating data from various platforms to narrate past performance and guide sales strategy optimization.; Business Intelligence Dashboards: Developed and enhanced to facilitate decision-making for business partners by visualizing key metrics and insights.; Advanced Analytics: Applied to derive actionable insights that drive incremental value and improve customer experience in Home Lending.; Analytical Frameworks: Built to support strategic and complex business priorities within the Home Lending Data & Analytics team."
Md1AP1lvzyB3sM65AAAAAA==,['Large Language Models'],"Large Language Models: Fine-tuning LLMs on domain-specific datasets to enhance the performance of the existing NLP model, which is a core product within the business.","['Machine Learning Algorithms', 'Statistical Analysis', 'NLP Model Development', 'Python ML Packages', 'Data Visualization Tools', 'Cloud Services', 'Version Control and CI/CD', 'MLOps Frameworks', 'Geospatial Data Processing']","Machine Learning Algorithms: The role involves developing machine learning models using algorithms such as regression, classification, and clustering to support climate adaptation efforts and build end-to-end ML models.; Statistical Analysis: Performing statistical analysis and techniques for model evaluation to extract meaningful insights and trends from data.; NLP Model Development: Supporting and enhancing an existing NLP model by analyzing text data to extract insights and improve model performance.; Python ML Packages: Using Python libraries such as scikit-learn, spaCy, NumPy, and SciPy for implementing machine learning algorithms and data processing.; Data Visualization Tools: Creating visualizations to communicate findings and facilitate understanding of models using tools like Matplotlib, Seaborn, Tableau, and Power BI.; Cloud Services: Utilizing cloud platforms such as AWS, Google Cloud Platform, and Azure for data storage and processing.; Version Control and CI/CD: Managing code changes and collaboration using Git and knowledge of CI/CD pipelines like GitHub Actions.; MLOps Frameworks: Exposure to MLOps tools such as MLFlow and Weights and Biases to support machine learning lifecycle management.; Geospatial Data Processing: Processing and analyzing geospatial data using Python libraries like geopandas and GDAL, and GIS software such as QGIS."
6ykd2WNVjOHLQ40DAAAAAA==,['Generative AI'],"Generative AI: Identified opportunities to integrate generative AI technologies to improve team efficiency and product strategy, including basic usage of tools like ChatGPT and Claude.","['Statistical and Quantitative Modeling', 'Data Mining, Clustering, and Segmentation', 'SQL', 'R and Python', 'ETL Frameworks and Data Transformation', 'Data Validation', 'BI Dashboards (Looker, Tableau)', 'Data Pipelines', 'Experimentation and A/B Testing', 'dbt (Data Build Tool)']","Statistical and Quantitative Modeling: Used to analyze large healthcare datasets and conduct data-driven analyses to support decision making and identify trends and drivers.; Data Mining, Clustering, and Segmentation: Applied to extract meaningful patterns and groupings from complex healthcare data to inform product strategy and improve outcomes.; SQL: Utilized for querying and managing data within the data warehouse to support data transformation and reporting.; R and Python: Used as primary programming languages for data science tasks including analysis, modeling, and visualization.; ETL Frameworks and Data Transformation: Involved in designing and building data pipelines and workflows to transform raw clinical, member, and claims data into actionable insights.; Data Validation: Ensures data quality and correctness throughout the data processing and analysis lifecycle.; BI Dashboards (Looker, Tableau): Built and maintained dashboards to visualize KPIs and product metrics, enabling stakeholders to monitor trends and make informed decisions.; Data Pipelines: Developed scalable and complex data pipelines to support data flows and reporting needs across teams.; Experimentation and A/B Testing: Led and guided experimentation efforts to validate hypotheses and accelerate learning through frequent, small experiments.; dbt (Data Build Tool): Used for building and managing data transformation pipelines to ensure scalable and maintainable data workflows."
VZCsO5p6akLRIVwMAAAAAA==,"['Large Language Models', 'Natural Language Processing', 'Generative AI', 'LangChain', 'LlamaIndex', 'Retrieval-Augmented Generation', 'Low-Rank Adaptation', 'Parameter-Efficient Fine-Tuning', 'Agentic Frameworks', 'Multi-Modality', 'PyTorch', 'TensorFlow']",Large Language Models: Develop and deploy models based on large language models (LLMs) for natural language processing and general AI tasks.; Natural Language Processing: Utilize NLP techniques specifically in the context of AI and LLMs to process and analyze language data.; Generative AI: Apply generative AI techniques to create or enhance AI-driven language and multimodal applications.; LangChain: Use LangChain framework for building applications that integrate LLMs with external data sources and workflows.; LlamaIndex: Employ LlamaIndex for advanced indexing and retrieval in language model applications.; Retrieval-Augmented Generation: Implement RAG techniques to enhance language model outputs by integrating external knowledge retrieval.; Low-Rank Adaptation: Use LoRa methods for efficient fine-tuning of large language models.; Parameter-Efficient Fine-Tuning: Apply PEFT techniques to fine-tune models with reduced computational resources.; Agentic Frameworks: Work with agentic frameworks to build autonomous AI agents capable of complex decision-making.; Multi-Modality: Engage in research and development involving multiple data modalities to improve AI model performance.; PyTorch: Use PyTorch deep learning framework specifically for neural network and AI model development.; TensorFlow: Utilize TensorFlow deep learning framework for building and training neural network models.,"['Machine Learning', 'Python', 'Vector Databases', 'Jupyter Notebook', 'AWS SageMaker', 'Scikit-learn', 'Docker', 'Cloud Platforms', 'Model Fine-Tuning', 'ML Frameworks', 'Data Streaming Tools', 'Linux']","Machine Learning: Develop and deploy machine learning models using various techniques; collaborate with MLOps teams to ensure smooth deployment and maintenance of models.; Python: Use Python programming language for implementing solutions, managing dependencies with package managers like Anaconda/conda/venv, and working with data analysis tools.; Vector Databases: Utilize vector databases to support advanced language processing and data retrieval tasks.; Jupyter Notebook: Employ Jupyter Notebook as a data analysis and experimentation environment.; AWS SageMaker: Use AWS SageMaker for building, training, and deploying machine learning models at scale.; Scikit-learn: Apply Scikit-learn for traditional machine learning model development and evaluation.; Docker: Containerize applications to ensure consistent deployment environments.; Cloud Platforms: Leverage cloud platforms such as AWS, Azure, and Google Cloud Platform to scale and deploy AI and machine learning solutions.; Model Fine-Tuning: Perform fine-tuning of machine learning models to improve performance and adapt to specific tasks.; ML Frameworks: Use machine learning frameworks such as TensorFlow and PyTorch for model development and experimentation.; Data Streaming Tools: Familiarity with data streaming tools to handle real-time data processing.; Linux: Work within Linux environments for development and deployment tasks."
8tPDaK1LJ_F71gsvAAAAAA==,['Artificial Intelligence and Machine Learning'],"Artificial Intelligence and Machine Learning: Delivering high-impact AI/ML solutions independently that drive significant business results in materials science and chemistry domains, including advanced AI/ML techniques applied to materials discovery and molecular design.","['Advanced AI/ML Techniques', 'Statistical and Computational Methods', 'Chemical and Materials Data Analysis', 'Data Science Tools and Platforms', 'Project Management in Data Science', 'Data-Driven Communication and Visualization', 'Leadership in Data Science Teams', 'Chemical Databases and Molecular Representations']","Advanced AI/ML Techniques: Used to solve complex problems in materials discovery, chemical reaction prediction, molecular design, and related areas within materials science and chemistry.; Statistical and Computational Methods: Applied to solve unusual problems by combining appropriate statistics, machine learning, and computational approaches in materials science and chemistry data analysis.; Chemical and Materials Data Analysis: Involves synthesizing and analyzing chemical and materials data from diverse sources, critically evaluating datasets for quality, completeness, and scientific validity using domain expertise.; Data Science Tools and Platforms: Includes advanced proficiency in SQL, Python, R, Spark, graph databases, and cloud platforms to support data science initiatives in materials science and chemistry.; Project Management in Data Science: Managing multiple high-priority, complex analytical projects simultaneously while maintaining exceptional quality standards and providing strategic input into project ideation and implementation.; Data-Driven Communication and Visualization: Synthesizing analytical findings and presenting strategic recommendations to senior executives and stakeholders through compelling data-driven narratives and visualizations.; Leadership in Data Science Teams: Providing technical direction to junior data scientists, serving as a technical mentor, and leading cross-functional project teams in materials science and chemistry applications.; Chemical Databases and Molecular Representations: Working with chemical databases and molecular representations to support data science projects in materials science and chemistry."
uFzkLiUklwC3z9PoAAAAAA==,[],,"['Supervised and Unsupervised Machine Learning', 'Python Ecosystem', 'SQL', 'Cloud-Based Tools and Technologies', 'Data Visualization and Storytelling', 'Data Analysis and Insight Generation', 'Automated Workflows and Tool Development', 'BI Tools and Dashboards', 'Big Data Technologies']","Supervised and Unsupervised Machine Learning: Expertise in both supervised and unsupervised machine learning methods is required to develop models that improve identity verification and fraud prevention solutions.; Python Ecosystem: Strong programming skills in Python and PySpark are essential for working with large-scale datasets and building data science tools; familiarity with libraries such as Pandas, NumPy, H2O, SHAP, Seaborn, and Jupyter is important for data manipulation, modeling, explainability, visualization, and interactive analysis.; SQL: Proficiency in SQL is necessary for querying relational databases to extract and analyze customer and operational data.; Cloud-Based Tools and Technologies: Experience with cloud platforms like AWS, Azure, GCP, and Databricks is important for handling large-scale data processing and storage in a cloud environment.; Data Visualization and Storytelling: Skills in creating dashboards, presentations, and interactive formats are required to communicate complex model results and data insights effectively to both technical and non-technical stakeholders.; Data Analysis and Insight Generation: Analyzing massive customer datasets to generate actionable insights that inform risk management policies and support business decisions is a core responsibility.; Automated Workflows and Tool Development: Developing tools and automated workflows to increase the speed, accuracy, and reproducibility of client-facing analyses is part of the role.; BI Tools and Dashboards: Familiarity with business intelligence tools such as Looker and Tableau is important for building visualizations and dashboards that support data storytelling and decision-making.; Big Data Technologies: Experience with big data frameworks and tools like Apache Spark, Apache Arrow, Amazon EMR, Redshift, Snowflake, and S3 is relevant for processing and managing large-scale datasets efficiently."
Ki-vKUxWDKEekexeAAAAAA==,[],,"['CRM Analytics', 'Measurement Planning and Strategy', 'Customer Segmentation', 'Testing Frameworks', 'Performance Reporting and Analysis', 'Personalization', 'Forecasting', 'Data Visualization', 'Data Quality Assurance (QA)', 'Statistical Concepts and Coding Languages', 'Data Extraction, Cleansing, and Manipulation', 'Business Intelligence (BI) and Visualization Tools', 'Advanced Excel Skills', 'Data Strategy']","CRM Analytics: Experience with customer relationship management analytics to analyze and optimize client data and marketing strategies.; Measurement Planning and Strategy: Involvement in planning and implementing measurement programs and strategies to evaluate marketing and client performance.; Customer Segmentation: Application of segmentation techniques to categorize customers for targeted marketing and analysis.; Testing Frameworks: Use of frameworks to design and analyze tests for marketing and performance optimization.; Performance Reporting and Analysis: Creation and delivery of reports and analyses to track and communicate client and campaign performance.; Personalization: Development and application of personalized marketing and analytics solutions based on data insights.; Forecasting: Use of forecasting methods to predict future trends and outcomes relevant to client strategies.; Data Visualization: Translating data into charts and graphs to effectively communicate findings to clients and internal teams.; Data Quality Assurance (QA): Ownership of data process elements including syntax, taxonomy management, and ensuring data integrity at collection, extraction, and activation points.; Statistical Concepts and Coding Languages: Utilization of advanced statistical knowledge and coding skills to perform complex data analyses.; Data Extraction, Cleansing, and Manipulation: Ability to extract, cleanse, and manipulate data effectively to prepare it for analysis.; Business Intelligence (BI) and Visualization Tools: Comfortable working with BI and visualization platforms to create reports and dashboards for client insights.; Advanced Excel Skills: Expertise in manipulating and organizing large datasets using advanced Excel functionalities.; Data Strategy: Identifying key issues and objectives based on client and internal needs to guide data-driven decision making."
K-7fcf6yuWkAsvXfAAAAAA==,[],,"['A/B Testing', 'Behavioral and Transactional Data Analysis', 'Data-Driven Decision Making', 'Data Visualization', 'Experimentation Design and Evaluation', 'Hypothesis Testing and Statistical Inference', 'Multi-Dimensional Data Analysis', 'Product Data Science', 'Python and R Scripting', 'Regression Analysis', 'SQL']","A/B Testing: Used to design and evaluate complex experiments to support new product launches and measure their impact on customer experience.; Behavioral and Transactional Data Analysis: Leveraged deep-dive analyses on rich datasets of behavioral and transactional user data to identify new opportunities and generate valuable insights.; Data-Driven Decision Making: Core approach to answering business questions and driving product strategy through analyses and data-centric presentations.; Data Visualization: Experience developing new visualizations using tools such as Tableau to bring clarity to key metrics and flows through dashboards and reports.; Experimentation Design and Evaluation: Responsible for defining and cultivating best practices in analytics instrumentation and experimentation to measure product performance.; Hypothesis Testing and Statistical Inference: Strong understanding and application of statistical methods such as hypothesis testing and inference to analyze data and support decision making.; Multi-Dimensional Data Analysis: Experience analyzing large, complex, and potentially messy multi-dimensional datasets to synthesize insights into actionable solutions.; Product Data Science: Providing data science support for financial services products by generating insights, supporting product launches, and mentoring other data scientists.; Python and R Scripting: Fluent in at least one scripting language (Python or R) to manipulate and analyze large datasets.; Regression Analysis: Applied regression techniques as part of statistical analysis to understand relationships within data and support product decisions.; SQL: Fluent in SQL for querying and working with large, complex datasets."
ZnzoTWtgb1YvbDueAAAAAA==,"['Multimodal Generative AI', 'Generative AI', 'Computer Vision with Deep Learning']","Multimodal Generative AI: Work with multimodal generative AI models, including image-text models, to explore and prototype innovative AI solutions relevant to insurance use cases.; Generative AI: Engage in research, experimentation, and application of generative AI techniques to develop novel AI capabilities and business value.; Computer Vision with Deep Learning: Apply deep learning methods, specifically convolutional neural networks, for advanced computer vision tasks such as video analytics and facial detection.","['Machine Learning', 'Computer Vision', 'Convolutional Neural Networks', 'Python', 'SQL', 'Cloud Platforms', 'Statistical Modeling', 'Data Visualization', 'Rapid Experimentation and Prototyping', 'Vendor and Technology Evaluation']","Machine Learning: Design and prototype machine learning models for image and video data focused on insurance-relevant use cases; involves building domain expertise in machine learning and statistical modeling techniques relevant to the insurance industry.; Computer Vision: Develop expertise in computer vision techniques including video analytics and facial detection to analyze image and video data for insurance applications.; Convolutional Neural Networks: Apply convolutional neural networks (CNNs) as a core method for computer vision tasks such as image and video analysis within insurance-related projects.; Python: Use Python programming language for data science and machine learning model development, prototyping, and experimentation.; SQL: Utilize SQL for data querying and management to support data science workflows and analytics.; Cloud Platforms: Leverage cloud platforms such as AWS, GCP, or Azure to deploy, scale, and manage data science and machine learning solutions.; Statistical Modeling: Employ statistical modeling techniques to support machine learning and data analysis efforts in insurance-related projects.; Data Visualization: Use data visualization techniques to communicate complex data insights and model results effectively to diverse audiences.; Rapid Experimentation and Prototyping: Engage in rapid experimentation and proof-of-concept development to quickly test and validate emerging data science and machine learning approaches.; Vendor and Technology Evaluation: Evaluate emerging technologies and vendors in the AI/ML space to provide technical assessments and recommendations for adoption."
8KqSNGiTErw9w2nBAAAAAA==,"['Generative AI', 'Large Language Models', 'Retrieval-Augmented Generation', 'Deep Learning Frameworks', 'AI Model Deployment and MLOps']","Generative AI: Experience with LLM/GenAI use cases and developing Retrieval-Augmented Generation solutions, tools, and services such as LangChain, LangGraph, and MCP.; Large Language Models: Involves working with LLMs in use cases related to generative AI and AI strategy development.; Retrieval-Augmented Generation: Developing RAG solutions and services to enhance AI capabilities, including integration with tools like LangChain and LangGraph.; Deep Learning Frameworks: Using frameworks such as PyTorch specifically for developing and deploying deep learning models including CNNs, RNNs, and GANs.; AI Model Deployment and MLOps: Deploying and optimizing AI/ML models in production environments using Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, MLflow, and cloud platforms like AWS SageMaker and AWS ML Studio.","['Exploratory Data Analysis', 'Machine Learning', 'Deep Learning', 'Natural Language Processing', 'Time-Series Analysis', 'Computer Vision', 'Model Validation and Testing', 'Model Deployment and Optimization', 'Cloud Computing for AI/ML', 'Data Strategy']","Exploratory Data Analysis: Used to understand client data sets and operational requirements to design long-term solutions and guide AI strategy and development.; Machine Learning: Applied in developing AI/ML solutions, including researching novel approaches, advancing training and solution design, and deploying models into production.; Deep Learning: Utilized techniques such as CNNs, RNNs, and GANs across real-world projects, including model tuning and performance validation in production environments.; Natural Language Processing: Applied as part of data analysis tasks including NLP, time-series analysis, and computer vision to support AI/ML algorithm development.; Time-Series Analysis: Used in data analysis alongside NLP and computer vision to support AI/ML algorithm development.; Computer Vision: Employed as part of data analysis techniques in AI/ML algorithm development.; Model Validation and Testing: Includes validating AI models and algorithms via code reviews, unit tests, and integration tests to ensure model performance and reliability.; Model Deployment and Optimization: Involves deploying and optimizing ML models using tools like Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow to maximize business impact.; Cloud Computing for AI/ML: Leveraging cloud environments such as AWS, Azure, or GCP to deploy AI/ML workloads efficiently.; Data Strategy: Defining data strategy to drive technical development and create next-generation tools, products, and AI services aligned with client needs."
t4yVfsMzzUfK4VS3AAAAAA==,[],,"['Python', 'Pandas', 'Data Analysis', 'Statistical Modeling', 'SQL', 'Time-Series Forecasting', 'Git Version Control']","Python: Used for developing advanced algorithms and models, with emphasis on object-oriented programming and production-level code development.; Pandas: Utilized extensively for data manipulation within the Python ecosystem.; Data Analysis: Conduct complex data analysis on large, messy, and unstructured real-world datasets to derive actionable insights.; Statistical Modeling: Apply foundational statistical methods and experimental design to build models and extract insights from data.; SQL: Handle database querying and manipulation of large datasets, including those involving time series data.; Time-Series Forecasting: Preferred experience in forecasting using time series data to support sustainability challenges.; Git Version Control: Incorporate version control and code refactoring to ensure production-level code quality."
kMUAZ8sBtdhk9a-cAAAAAA==,"['Deep Learning', 'PyTorch', 'Generative AI', 'Retrieval-Augmented Generation', 'Large Language Models', 'Prompt Engineering', 'AI Model Deployment and MLOps', 'Cloud AI Services']","Deep Learning: The position requires applying deep learning techniques including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs) across real-world projects, including model tuning and performance validation in production.; PyTorch: PyTorch is used as a core framework for AI/ML algorithm development and deep learning model implementation in this role.; Generative AI: The job includes experience with generative AI use cases, specifically working with large language models (LLMs) and developing retrieval-augmented generation (RAG) solutions, tools, and services.; Retrieval-Augmented Generation: Experience developing RAG solutions and tools such as LangChain and LangGraph is required, supporting advanced AI applications involving LLMs.; Large Language Models: The role involves working with LLMs for generative AI use cases and AI service development.; Prompt Engineering: The job includes responsibilities related to prompt engineering as part of developing and deploying generative AI and LLM-based solutions.; AI Model Deployment and MLOps: Deploying and optimizing AI models using Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow is part of the role, with a focus on AI/ML workloads and production environments.; Cloud AI Services: Experience with cloud AI services such as AWS Sagemaker and AWS ML Studio is preferred for deploying and managing AI/ML workloads.","['Machine Learning', 'Data Analysis', 'Time-Series Analysis', 'Natural Language Processing', 'Computer Vision', 'Model Validation and Testing', 'Model Deployment and Optimization', 'Cloud Computing for AI/ML']","Machine Learning: The role involves developing AI/ML algorithms using core data science languages and frameworks, applying traditional machine learning techniques such as CNNs, RNNs, and GANs, and tuning and validating models in production environments.; Data Analysis: The job requires performing exploratory data analysis on client data sets to understand operational requirements and drive long-term solutions.; Time-Series Analysis: Experience with time-series analysis is required as part of the data analysis and AI/ML algorithm development responsibilities.; Natural Language Processing: The role includes experience with NLP as part of AI/ML algorithm development and data analysis.; Computer Vision: The job involves applying computer vision techniques as part of AI/ML algorithm development and data analysis.; Model Validation and Testing: Responsibilities include validating AI models and algorithms through code reviews, unit tests, and integration tests to ensure model performance and reliability.; Model Deployment and Optimization: The role requires deploying and optimizing machine learning models using tools such as Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow, and leveraging cloud environments like AWS, Azure, or GCP.; Cloud Computing for AI/ML: Experience in leveraging cloud platforms (AWS, Azure, GCP) to deploy AI/ML workloads is essential for the role."
KowIDlQ5DAMy2r0SAAAAAA==,[],,"['Data Analysis', 'Statistical Modeling', 'Data Mining', 'Predictive Modeling', 'Advanced Analytics', 'Data Communication', 'Data Integrity and Extraction Processes']","Data Analysis: Used to examine and interpret complex data sets to inform pricing and promotions strategies and provide actionable business recommendations.; Statistical Modeling: Applied to develop models that optimize pricing and promotions strategies and forecast sales performance.; Data Mining: Utilized to identify patterns and trends in customer behavior and purchasing habits to support strategic decision-making.; Predictive Modeling: Developed and maintained to forecast sales and evaluate the effectiveness of pricing and promotions strategies.; Advanced Analytics: Employed to create pricing and promotions strategies for new products and services, driving business innovation.; Data Communication: Involves translating complex data and insights into clear, concise information for non-technical stakeholders.; Data Integrity and Extraction Processes: Collaborated with IT teams to ensure data quality and develop processes for data extraction and analysis."
JsXfvWL8uZiXqvP5AAAAAA==,[],,"['Clickstream Tracking', 'Consumer Analytics', 'A/B Testing', 'SQL', 'Python', 'R', 'Web Analytics Tools', 'Data Visualization Tools', 'Statistical Techniques', 'Machine Learning Algorithms', 'KPI Development and Monitoring', 'Segmentation', 'Pre-Post Analysis', 'Product-Level Modeling and Forecasting']","Clickstream Tracking: Used to enhance the quality of tracking user interactions on the website to analyze consumer behavior and improve engagement and conversion rates.; Consumer Analytics: Applied to dissect consumer behavior, optimize consumer funnels, and segment users to drive product and business improvements.; A/B Testing: Designing, executing, and analyzing experiments to evaluate the effectiveness of different strategies and interventions, providing actionable insights and recommendations.; SQL: Used for querying and working with relational databases to extract and manipulate data for analysis.; Python: Utilized for data analysis and modeling, including applying statistical techniques and machine learning algorithms to complex datasets.; R: Used for data analysis and modeling tasks, supporting statistical and advanced analytic disciplines.; Web Analytics Tools: Experience with tools such as Adobe Analytics and Google Analytics to track and analyze web and event data for product and consumer insights.; Data Visualization Tools: Use of Tableau, Looker, or Power BI to create clear and compelling visualizations, reports, and presentations for communicating findings to stakeholders.; Statistical Techniques: Applied to analyze complex datasets, identify patterns, trends, and correlations to uncover actionable insights for business improvement.; Machine Learning Algorithms: Used to understand product performance and support advanced analytics for predictive modeling and insights generation.; KPI Development and Monitoring: Developing and tracking key performance indicators to oversee business initiatives, measure company health, and enable performance management.; Segmentation: Dividing consumer data into meaningful groups to better understand behavior and tailor product strategies.; Pre-Post Analysis: Conducting analyses before and after interventions or changes to measure impact and effectiveness.; Product-Level Modeling and Forecasting: Applying advanced analytic disciplines to model and forecast product performance and marketing outcomes."
X_SgGUG8WLfcAwggAAAAAA==,[],,"['Predictive and Prescriptive Modeling', 'Demand Forecasting', 'Promotion Optimization', 'Retail Execution Analytics', 'Data Visualization Tools', 'Python Programming', 'Big Data Platforms', 'Syndicated Retail Data', 'Model Development and Deployment']","Predictive and Prescriptive Modeling: Design, develop, and scale predictive and prescriptive models that support retail strategy and execution, including demand forecasting and promotion optimization.; Demand Forecasting: Solve complex challenges related to predicting future retail demand to improve business planning and execution.; Promotion Optimization: Develop analytics solutions to optimize retail promotions for better consumer engagement and sales performance.; Retail Execution Analytics: Apply advanced analytics to improve how the company partners with retailers and delivers value to consumers through retail execution strategies.; Data Visualization Tools: Use Tableau and Power BI to translate complex data into actionable insights and compelling visual stories for business stakeholders.; Python Programming: Utilize Python programming skills and IDEs such as PyCharm, Jupyter, and Anaconda to build scalable, production-ready data science solutions.; Big Data Platforms: Work with large-scale data environments including Google BigQuery and Oracle databases to manage and analyze retail and syndicated data.; Syndicated Retail Data: Experience working with syndicated retail data sources like Nielsen and Circona, as well as point-of-sale (POS) data, to inform analytics and modeling.; Model Development and Deployment: Develop and deploy production-grade machine learning models to support retail business needs and strategies."
q-70EhlU6v2jLM4bAAAAAA==,[],,"['Operations Research', 'Supply Chain Optimization', 'Simulation Experiments', 'Optimization Algorithms', 'Analytical Techniques', 'Key Performance Indicators (KPIs)', 'Algorithm Development', 'Statistics']","Operations Research: Used to develop and implement optimization algorithms and simulation experiments to solve complex supply chain problems and improve fleet management practices.; Supply Chain Optimization: Focuses on identifying key performance indicators and metrics to measure supply chain effectiveness and applying analytical techniques to enhance operational efficiency.; Simulation Experiments: Developed from scratch to model and analyze supply chain scenarios, leading to the creation of optimization algorithms.; Optimization Algorithms: Designed and implemented to address complex supply chain challenges and improve operational decision-making.; Analytical Techniques: Applied to create and enhance fleet management practices and to identify high-value opportunities within operations research.; Key Performance Indicators (KPIs): Assisted in identifying and measuring metrics to evaluate the effectiveness of the company's supply chain.; Algorithm Development: Demonstrated understanding of modern algorithm development techniques and methods relevant to operations research and data science.; Statistics: Utilized as a foundational skill in analyzing data and supporting operations research and supply chain optimization efforts."
36sWFGTvdmvx5R7lAAAAAA==,['AI Product Development'],AI Product Development: Lead the development of AI-driven product offerings that enhance messaging strategies and customer communication.,"['Statistical Modeling', 'Econometric Modeling', 'Optimization Models', 'Machine Learning Models', 'Experimental Design and Analysis', 'Exploratory Data Analysis', 'Statistical Analysis and Testing', 'Python Programming', 'SQL', 'Customer Lifetime Value Modeling', 'Personalization Systems', 'Ads Delivery and Optimization Systems', 'Productionizing Algorithms']","Statistical Modeling: Lead the development of statistical models to analyze and predict outcomes relevant to messaging and AI product offerings.; Econometric Modeling: Develop econometric models to understand economic relationships and optimize messaging strategies within AI products.; Optimization Models: Create optimization models to improve messaging delivery and AI product performance.; Machine Learning Models: Design and implement machine learning models to support AI product offerings and enhance messaging strategies.; Experimental Design and Analysis: Design and execute experiments, interpret results, and draw actionable conclusions to improve product performance and inform business decisions.; Exploratory Data Analysis: Perform exploratory data analysis to understand data characteristics and identify opportunities for model development and product improvement.; Statistical Analysis and Testing: Conduct statistical analysis and hypothesis testing to validate models and support data-driven decision-making.; Python Programming: Use Python to efficiently process and analyze large datasets at scale for model development and data insights.; SQL: Utilize advanced SQL skills to query and manipulate large datasets for analysis and model input.; Customer Lifetime Value Modeling: Develop models to estimate and predict customer lifetime value to inform personalization and marketing strategies.; Personalization Systems: Work on personalization systems and cross-sectional cohort modeling to tailor messaging and improve customer engagement.; Ads Delivery and Optimization Systems: Apply modeling techniques to optimize ad delivery and improve marketing campaign effectiveness.; Productionizing Algorithms: Implement and deploy algorithms into real-time systems to support scalable AI product functionalities."
iipKSv4Wf8c_UHL9AAAAAA==,"['Large Language Models', 'Transformers', 'Generative AI']",Large Language Models: Leading the exploration and application of large language models and transformers to develop state-of-the-art NLP systems for medical and legal document processing.; Transformers: Utilizing transformer architectures as part of cutting-edge NLP techniques to solve domain-specific challenges in text and audio data.; Generative AI: Developing and evaluating prompts for generative AI models to enhance NLP solutions and business applications.,"['Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Feature Engineering', 'Python', 'SQL', 'Data Visualization and Analytics', 'Cloud Platforms and Model Deployment', 'MLOps', 'Statistical and Mathematical Fundamentals', 'Optical Character Recognition', 'Reinforcement Learning and Unsupervised Learning', 'Source Control Systems', 'Data Science Frameworks']","Natural Language Processing: Design, development, and deployment of scalable NLP systems to process, analyze, and extract information from medical and legal documents, applying domain-specific NLP techniques such as information retrieval, recommendation, summarization, and personalization.; Machine Learning: Building and deploying machine learning models including data preprocessing, feature engineering, model training, evaluation, deployment, and performance monitoring to deliver impactful data science solutions at scale.; Deep Learning: Expertise in deep learning techniques and model fine-tuning applied to NLP systems, including neural network architectures and linear algebra fundamentals.; Feature Engineering: Data processing and feature engineering for both structured and unstructured data to support analytics and model development.; Python: Proficiency in Python programming language for data science, machine learning, and NLP development.; SQL: Proficiency in SQL for data querying and manipulation relevant to data science workflows.; Data Visualization and Analytics: Analytics and visualization skills to translate complex data insights into clear, actionable business strategies and demonstrate the impact of data products using quantitative metrics.; Cloud Platforms and Model Deployment: Experience with cloud platforms such as AWS, Azure, and Google Cloud for ML model deployment and productionization.; MLOps: Experience with MLOps tools and philosophies to support the deployment and monitoring of machine learning models in production environments.; Statistical and Mathematical Fundamentals: Background in statistics, probability, linear algebra, and optimization methods to underpin data science and machine learning model development.; Optical Character Recognition: Familiarity with OCR technologies to integrate structured and unstructured data sources, particularly in medical and legal document contexts.; Reinforcement Learning and Unsupervised Learning: Background in reinforcement learning, unsupervised learning, outlier detection, and graph-based NLP techniques to enhance NLP system capabilities.; Source Control Systems: Proficiency in using source control systems like GitHub to manage code and collaborate within data science teams.; Data Science Frameworks: Proficiency in AI/ML/NLP frameworks such as scikit-learn, spaCy, and Hugging Face for building and deploying machine learning and NLP models."
_g6t-UajKOyaY_UJAAAAAA==,[],,"['Attribution Modeling', 'Incrementality Testing', 'Advanced Measurement Methodologies', 'SQL', 'Python', 'R', 'Google Cloud Platform', 'BigQuery', 'Looker', 'DataProc', 'Excel', 'BI/Visualization Tools', 'Statistical Concepts', 'Measurement Planning and Strategy', 'Customer Segmentation', 'Testing Frameworks', 'Performance Reporting and Analysis', 'Personalization', 'Forecasting', 'Data QA and Integrity', 'Data Process Management', 'Data Extraction, Cleansing, and Manipulation', 'CRM Key Performance Metrics']","Attribution Modeling: Experience in media performance analytics including attribution modeling to measure the impact of marketing channels on outcomes.; Incrementality Testing: Experience in media performance analytics involving incrementality testing to assess the incremental effect of marketing activities.; Advanced Measurement Methodologies: Use of advanced measurement methodologies for evaluating marketing performance and effectiveness.; SQL: Proficiency in SQL for data extraction, manipulation, and analysis within client projects.; Python: Proficiency in Python for coding and advanced analytics capabilities.; R: Proficiency in R programming language for statistical analysis and advanced analytics.; Google Cloud Platform: Practical experience with Google Cloud Platform services such as BigQuery, Looker, and DataProc for data processing and analytics.; BigQuery: Use of BigQuery as a data warehouse solution within Google Cloud Platform for querying large datasets.; Looker: Use of Looker as a BI and visualization tool to create reports and dashboards for clients.; DataProc: Use of DataProc service on Google Cloud Platform for data processing and analytics workflows.; Excel: Advanced Excel skills for manipulating and organizing large data sets and supporting analytics tasks.; BI/Visualization Tools: Comfortable working with business intelligence and visualization tools to translate data into visuals such as charts and graphs for client presentations.; Statistical Concepts: Deeper knowledge of statistical concepts applied to advanced analytics and measurement planning.; Measurement Planning and Strategy: Application of measurement planning and strategy to support client data-driven projects and analytics.; Customer Segmentation: Use of customer segmentation techniques to analyze and group clients' customer data for insights.; Testing Frameworks: Implementation of testing frameworks to evaluate marketing strategies and performance.; Performance Reporting and Analysis: Creation and delivery of performance reports and analyses to inform client decision-making.; Personalization: Application of personalization techniques to optimize marketing and customer experiences.; Forecasting: Use of forecasting methods to predict future trends and outcomes for clients.; Data QA and Integrity: Quality assurance and maintenance of data integrity at collection, extraction, and activation points.; Data Process Management: Ownership of data process elements such as syntax and taxonomy management and quality assurance.; Data Extraction, Cleansing, and Manipulation: Expertise in extracting, cleansing, manipulating, and analyzing data to generate insights.; CRM Key Performance Metrics: Fluency in standard CRM metrics such as email click-through rate, offer redemption rate, incremental sales, and incremental margin for measurement and reporting."
XJ5j5Ty72RZwbOp-AAAAAA==,[],,"['Fraud Detection Analytics', 'Predictive Modeling', 'Statistical Analysis', 'Advanced Statistical Techniques', 'Machine Learning Techniques', 'SAS', 'R Programming', 'Data Mining and Analysis', 'Multivariate Analysis', 'Model Validation', 'Data Extraction and Cleaning', 'Project Management in Analytics', 'Business Development Support']","Fraud Detection Analytics: Responsible for delivering various fraud analytics statistical projects based on US healthcare data, including model development, validation, governance, and end-to-end delivery to detect fraud, waste, and abuse.; Predictive Modeling: Create, analyze, and maintain explanatory and predictive models of clinical behaviors on healthcare claims data, covering all phases from research design, data extraction and cleaning, to model building and validation.; Statistical Analysis: Evaluate statistical methods to ensure validity, applicability, efficiency, and accuracy; perform detailed statistical analysis to interpret end-user technical requirements and specify economic decision models.; Advanced Statistical Techniques: Lead and develop models using advanced statistical techniques and mentor junior/senior analysts and statisticians to improve their modeling and statistical skills.; Machine Learning Techniques: Develop, refine, and improve predictive models using machine learning techniques alongside advanced SAS and R programming.; SAS: Use advanced SAS programming for statistical analysis, model development, and predictive modeling in fraud analytics and healthcare data projects.; R Programming: Utilize R for statistical analysis, model building, validation, and predictive modeling in healthcare fraud detection and other analytics projects.; Data Mining and Analysis: Apply knowledge of statistical tools and techniques related to data mining and analysis, with the ability to handle and work on large datasets.; Multivariate Analysis: Expertise in multivariate analysis techniques to support model building, validation, and fraud detection analytics.; Model Validation: Responsible for validating models developed for fraud detection and predictive analytics to ensure accuracy and reliability.; Data Extraction and Cleaning: Perform data extraction and cleaning as part of the modeling process to prepare healthcare claims data for analysis and model development.; Project Management in Analytics: Manage analytics projects including solution design, delivery, client management, and team leadership to ensure successful implementation of statistical and predictive models.; Business Development Support: Provide support in business development, client proposals, and building relationships with global analytics teams to drive analytics strategy and service delivery."
boY52InL9yYDLpEOAAAAAA==,['Large Language Models'],Large Language Models: Implement sophisticated Large Language Models to optimize repair routing decisions for efficiency and cost-effectiveness.,"['Predictive Modeling', 'Statistical Analysis and Modeling', 'Feature Engineering', 'Machine Learning Model Development', 'Cross-Validation', 'Data Querying and Scripting', 'Data Analytics and Visualization', 'Computer Vision', 'Analytics Pipeline Creation']","Predictive Modeling: Develop predictive models for future repair and maintenance needs based on historical data to forecast repair requirements.; Statistical Analysis and Modeling: Apply strong statistical analysis and modeling techniques to support data-driven decision making and model development.; Feature Engineering: Perform feature engineering as part of the complete model development lifecycle to improve model performance.; Machine Learning Model Development: End-to-end experience in machine learning model development including problem definition, model tuning, cross-validation, and deployment to create predictive and prescriptive models.; Cross-Validation: Conduct cross-validation of production models to ensure optimal performance and generalizability.; Data Querying and Scripting: Use data querying languages like SQL and scripting languages like Python for data manipulation and analysis.; Data Analytics and Visualization: Generate analytics, reports, visualizations, and dashboards to communicate results effectively to both technical and non-technical stakeholders.; Computer Vision: Utilize advanced computer vision techniques for real-time damage detection to accurately identify and assess damage.; Analytics Pipeline Creation: Support the creation of analytics pipelines to drive business impact and decision-making processes."
81jBdaow2sVgD3jwAAAAAA==,[],,"['Applied Machine Learning', 'Statistical Methods and Packages', 'Data Science Programming Languages and Tools', 'Statistical and Mathematical Modeling', 'Data Science Team Leadership', 'Data-Driven Business Problem Selection', 'Cross-Functional Collaboration']","Applied Machine Learning: Experience applying machine learning techniques to solve real-world business problems and predictive modeling use cases.; Statistical Methods and Packages: Strong statistical background with experience using R or other statistical software packages for data analysis.; Data Science Programming Languages and Tools: Proficiency in coding languages and tools relevant to data science such as Pig, Hive, SQL, and Python.; Statistical and Mathematical Modeling: Expertise in building and applying statistical and mathematical methods to analyze data and develop predictive models.; Data Science Team Leadership: Leading and managing a team of data scientists working on multiple client engagements.; Data-Driven Business Problem Selection: Identifying and selecting high-value business problems to address using data science techniques.; Cross-Functional Collaboration: Working with product management and engineering teams to identify opportunities to leverage data science for product improvement or creation."
dNME9zPUDS8QGAp3AAAAAA==,[],,"['Pricing Models', 'Data Analysis']",Pricing Models: Used to develop and optimize pricing strategies within the insurance or financial sector by leveraging company data assets.; Data Analysis: Involves identifying strategic opportunities and measuring their impact through analysis of company data.
Vi8rKqszbChnjSNrAAAAAA==,[],,"['SQL', 'Python', 'Hadoop', 'Spark', 'Tableau', 'Data Exploration', 'Analytics Dashboards', 'Data Strategy']","SQL: Used for manipulating large data sets and performing complex queries to support analytics and reporting needs in the gaming data environment.; Python: Utilized for data manipulation and analysis, enabling the creation of analytic tools and dashboards for stakeholders.; Hadoop: Applied as a big data technology to handle and process large-scale data sets relevant to gaming analytics.; Spark: Used as a big data processing framework to efficiently manage and analyze large volumes of gaming data.; Tableau: Employed as a visualization tool to build and maintain dashboards and reports for diverse stakeholders across the company.; Data Exploration: Performed proactively to discover insights and identify future testing opportunities within gaming data.; Analytics Dashboards: Developed and maintained to provide key metrics and insights to game stakeholders and support decision-making.; Data Strategy: Involved in shaping the overall games data strategy to scale data access and improve analytics capabilities."
nlyQipkD4AMrHSOaAAAAAA==,"['Large Language Models', 'Generative AI', 'Conversational AI', 'Trustworthy AI Practices']","Large Language Models: Building LLM-powered intelligent experiences such as chatbots or business applications to improve employee productivity and communication.; Generative AI: Developing and deploying generative AI platforms and solutions to support associates globally, including intelligent conversational interfaces.; Conversational AI: Designing and building intelligent conversational interfaces that automate tasks, provide personalized Q&A support, and enhance communication within business applications.; Trustworthy AI Practices: Implementing robust standards and practices to ensure AI/ML solutions are trustworthy and reliable across the enterprise.","['Statistical Analysis', 'Python Programming', 'Machine Learning Frameworks', 'Machine Learning Applications', 'Data Science', 'Optimization Models', 'Spark, Scala, R', 'Scikit-learn']","Statistical Analysis: Used to analyze data and support the development of machine learning applications and AI/ML models in the role.; Python Programming: Programming language used for statistical analysis, data science tasks, and building machine learning applications.; Machine Learning Frameworks: Includes mainstream frameworks such as TensorFlow and PyTorch used to build and deploy machine learning models.; Machine Learning Applications: Experience required in building applications that leverage machine learning models to solve business problems.; Data Science: Involves designing, architecting, and building AI/ML models and systems, as well as coaching junior data scientists and collaborating with multidisciplinary teams.; Optimization Models: Preferred qualification involving mathematical models to improve decision-making and efficiency in analytics or machine learning contexts.; Spark, Scala, R: Technologies mentioned as part of assessments and preferred skills for data analytics and data science tasks.; Scikit-learn: Open source machine learning framework referenced as part of the technology stack for building models."
4M_WMfOdeE3FBSQaAAAAAA==,"['Generative AI', 'Large Language Models (LLMs)', 'Retrieval-Augmented Generation', 'Prompt Engineering', 'Deep Learning Frameworks for AI', 'AI Model Validation and Testing', 'AI Strategy and Development', 'AI/ML Model Deployment and Optimization', 'Cloud AI/ML Workloads', 'AWS SageMaker', 'AWS ML Studio', 'Multi-Modal AI Techniques']","Generative AI: The role involves working on generative AI use cases, including developing solutions and services related to large language models and generative AI technologies.; Large Language Models (LLMs): Experience with LLMs is required, including use cases and development of retrieval-augmented generation (RAG) solutions.; Retrieval-Augmented Generation: Developing RAG solutions, tools, and services such as LangChain and LangGraph is part of the job responsibilities.; Prompt Engineering: The role includes responsibilities related to prompt engineering for AI models.; Deep Learning Frameworks for AI: PyTorch is used specifically for deep learning model development and training in AI projects.; AI Model Validation and Testing: Validating AI models and algorithms through code reviews, unit tests, and integration tests is a key responsibility.; AI Strategy and Development: The role guides clients in AI strategy, including understanding organizational needs, developing AI solutions, and deploying models into production.; AI/ML Model Deployment and Optimization: Deploying and optimizing AI/ML models using tools like Kubernetes, Docker, TensorRT, RAPIDs, Kubeflow, and MLflow is required.; Cloud AI/ML Workloads: Leveraging cloud platforms such as AWS, Azure, or GCP to deploy AI/ML workloads and services is part of the job.; AWS SageMaker: Experience with AWS SageMaker is preferred for deploying and managing AI/ML models in the cloud.; AWS ML Studio: Experience with AWS ML Studio is preferred for building and deploying AI/ML solutions.; Multi-Modal AI Techniques: The role includes working with AI techniques across NLP, computer vision, and time-series analysis within AI projects.","['Machine Learning', 'Deep Learning', 'Exploratory Data Analysis', 'NLP (Natural Language Processing)', 'Time-Series Analysis', 'Computer Vision', 'Python', 'PyTorch', 'Kubernetes', 'Docker', 'TensorRT', 'RAPIDs', 'Kubeflow', 'MLflow', 'Cloud Environments (AWS, Azure, GCP)', 'Valuation Modeling', 'Cost Optimization', 'Restructuring', 'Business Design and Transformation', 'Mergers and Acquisitions (M&A)']","Machine Learning: The role involves developing and deploying machine learning algorithms and models, including traditional ML techniques and model tuning and validation in production environments.; Deep Learning: Experience applying deep learning techniques such as CNNs, RNNs, and GANs across real-world projects is required, including model tuning and performance validation.; Exploratory Data Analysis: Performing exploratory data analysis to understand client data sets and operational requirements is a key responsibility.; NLP (Natural Language Processing): The job requires experience in NLP as part of data analysis tasks.; Time-Series Analysis: Experience with time-series analysis is required for analyzing temporal data in client projects.; Computer Vision: The role includes experience in computer vision techniques as part of data analysis and model development.; Python: Python is a core programming language used for AI/ML algorithm development and data analysis.; PyTorch: PyTorch is used as a framework for AI/ML algorithm development and deep learning model implementation.; Kubernetes: Kubernetes is used for deploying and optimizing machine learning models in production environments.; Docker: Docker is used for containerizing ML models to facilitate deployment and scalability.; TensorRT: TensorRT is used for optimizing ML models for inference performance.; RAPIDs: RAPIDs is used for accelerating ML model deployment and optimization.; Kubeflow: Kubeflow is used to manage ML workflows and pipelines for model deployment and optimization.; MLflow: MLflow is used for managing the machine learning lifecycle including experimentation, reproducibility, and deployment.; Cloud Environments (AWS, Azure, GCP): Experience leveraging cloud platforms such as AWS, Azure, or GCP to deploy AI/ML workloads is required.; Valuation Modeling: The team delivers advisory services including valuation modeling as part of business transformation projects.; Cost Optimization: Cost optimization is part of the integrated support and advisory services provided to clients.; Restructuring: Restructuring is included in the advisory services offered during transformational initiatives.; Business Design and Transformation: The role supports business design and transformation efforts through data science and analytics.; Mergers and Acquisitions (M&A): The team provides support and advisory services related to mergers and acquisitions."
URwTuFHHXHOExfnrAAAAAA==,[],,"['Exploratory Data Analysis', 'Data Engineering Pipelines', 'Machine Learning', 'Statistical Programming Languages', 'Structured Query Languages', 'Data-Driven Recommendations', 'Data Visualization Tools', 'Statistical Concepts and Analytical Methods']","Exploratory Data Analysis: Used to analyze large-scale government data sources to identify trends in payment collections and taxpayer behaviors and to develop briefings and reports communicating key findings.; Data Engineering Pipelines: Collaborated with data scientists to build pipelines that support data processing and machine learning workflows.; Machine Learning: Worked with teams of data scientists to support machine learning initiatives as part of data pipeline development.; Statistical Programming Languages: Familiarity with SAS, R, or Stata is considered beneficial for performing statistical analysis and data manipulation.; Structured Query Languages: Experience with PL/SQL, Postgres, and MySQL is valued for querying and managing structured databases.; Data-Driven Recommendations: Formulated actionable recommendations based on data analysis and research to support government clients' business needs.; Data Visualization Tools: Collaborated with software developers to build tools that facilitate data visualization and analysis for stakeholders.; Statistical Concepts and Analytical Methods: Applied basic statistical concepts and analytical methods to support data analysis and problem-solving."
YgWt9pI83ZqHguR3AAAAAA==,[],,"['Marketing Analytics', 'A/B Testing', 'Multivariate Testing', 'Holdout Testing', 'Statistical Analysis', 'Data Visualization', 'SQL', 'Python', 'R', 'SAS', 'Data Integration', 'Analytic Storytelling']","Marketing Analytics: Used to design and run marketing campaigns, execute A/B and multivariate tests, and optimize marketing efforts through data-driven insights and testing results.; A/B Testing: Developed and executed as a testing strategy to optimize marketing campaigns and measure their effectiveness.; Multivariate Testing: Applied as a testing strategy alongside A/B testing and holdout tests to refine marketing strategies and improve campaign performance.; Holdout Testing: Used as a testing method to evaluate marketing campaign effectiveness by comparing test groups with control groups.; Statistical Analysis: Applied advanced statistical techniques to interpret data, support decision-making, and measure marketing campaign effectiveness.; Data Visualization: Created clear and concise visual representations of data using tools like Tableau and Power BI to support data-driven decision-making and client presentations.; SQL: Used for querying and interpreting complex data sets to extract actionable insights.; Python: Utilized for data analysis and statistical computations in marketing analytics projects.; R: Employed for statistical analysis and data manipulation in marketing analytics.; SAS: Used for advanced statistical analysis and data management in marketing analytics.; Data Integration: Managed and executed large-scale data analytics projects by integrating extensive data sets to provide actionable insights and recommendations.; Analytic Storytelling: Used to effectively communicate analytical findings and insights to business stakeholders through presentations and client-facing materials."
I0EBmZc4z5e7wiLPAAAAAA==,[],,"['Statistical Analysis', 'Python', 'R', 'SQL', 'Data Infrastructure', 'Mathematical Modeling', 'Data Quality Assurance']","Statistical Analysis: Used to analyze and interpret data to solve product or business problems and to evaluate models mathematically expressing defined problems.; Python: Used as a coding tool for data extraction, querying databases, and data manipulation to prepare datasets for analysis.; R: Used as a coding tool for querying databases, statistical analysis, and data manipulation to ensure data quality and readiness for analysis.; SQL: Used to query databases and extract data from multiple sources as part of the data gathering and compilation process.; Data Infrastructure: Involves using custom or existing data models and infrastructure to support data gathering, extraction, and analysis for solving business or product questions.; Mathematical Modeling: Designing and evaluating models to mathematically express and solve defined problems related to YouTube Search.; Data Quality Assurance: Involves formatting, restructuring, validating data, and reviewing datasets to ensure they are accurate and ready for analysis."
e1YXftmzhcWxYPZAAAAAAA==,[],,"['Real-World Data Analytics', 'SAS and SAS Macro SQL', 'Python Programming', 'Advanced Scientific and Analytical Methods', 'Machine Learning', 'Data Integration and Visualization', 'Real-World Evidence (RWE) Dashboards']","Real-World Data Analytics: Involves analyzing large and complex real-world data sets such as medical claims data, electronic medical records, registries, and surveys to derive insights relevant to healthcare and pharmaceutical research.; SAS and SAS Macro SQL: Used as primary programming tools for real-world data analytics, enabling data manipulation, querying, and analysis within healthcare datasets.; Python Programming: Applied for programming and analytical tasks related to real-world data, supporting data integration, visualization, and advanced analytics.; Advanced Scientific and Analytical Methods: Includes the application and training in sophisticated analytical techniques to solve scientific and business problems using complex data.; Machine Learning: Adopted as an advanced methodology to enhance data analysis, automate processes, and develop predictive models within real-world evidence projects.; Data Integration and Visualization: Involves combining disparate data sources and creating visual representations such as dashboards to support decision-making and insight generation.; Real-World Evidence (RWE) Dashboards: Used to visualize and communicate insights derived from real-world data to stakeholders, facilitating faster and better decision-making."
p047tmTGAAKrsnDzAAAAAA==,[],,"['SQL', 'Python', 'Tableau', 'Statistical Modeling', 'Media Mix Modeling (MMM)', 'Marketing Attribution', 'Data Collection and Transformation', 'Marketing Analytics', 'Geo-level Testing', 'Media Mix Testing']","SQL: Used extensively for data querying and manipulation, including advanced analytics functions, window functions, and common table expressions to support marketing analytics and reporting.; Python: Utilized as a scripting language for data analysis and building analytics workflows in marketing data science projects.; Tableau: Employed as a data visualization tool to create dashboards and reporting solutions that provide executive-level and cross-functional visibility into marketing performance.; Statistical Modeling: Applied to implement advanced analytics frameworks such as media mix modeling (MMM) and channel-level marginal customer acquisition cost (CAC) to inform marketing budget recommendations and optimize spend.; Media Mix Modeling (MMM): Developed and implemented as an advanced analytics framework to analyze marketing performance and guide budget allocation decisions.; Marketing Attribution: Analyzed to solve problems related to understanding the impact of various marketing channels on customer acquisition and retention.; Data Collection and Transformation: Designed and maintained processes to support reporting and analytics solutions, ensuring data is prepared and transformed appropriately for analysis.; Marketing Analytics: Focused on acquisition and retention marketing, involving the development of analytical capabilities to drive profitable growth and inform marketing strategies.; Geo-level Testing: Conducted as part of the learning agenda to evaluate channel strategy effectiveness across different geographic regions.; Media Mix Testing: Performed to assess the impact of different marketing channels and optimize media spend."
ZS6TsMNJHJz7BRKwAAAAAA==,[],,"['Data Warehouse (DWH) Architecture', 'Business Intelligence (BI) Tools', 'SQL', 'Data Quality Management', 'Data Governance', 'Core Banking Systems', 'Real-time Data Monitoring', 'Agile/Scrum Methodology', 'Mentoring and Collaboration', 'Task and Project Management Tools']","Data Warehouse (DWH) Architecture: Managing and enhancing the data warehouse environment including administration of DWH layers such as DDS and CDM, ensuring data quality, fixing and restoring data for key entities, and supporting integration of data sources.; Business Intelligence (BI) Tools: Developing BI solutions and assets such as dashboards using tools like Power BI and Grafana to support business units and decision-making.; SQL: Utilizing advanced SQL skills for querying, managing, and manipulating data within the data warehouse and other data sources.; Data Quality Management: Ensuring data quality by controlling, resolving issues, and leading efforts to improve standards and governance across the data lifecycle.; Data Governance: Documenting BI standards and managing data governance strategies to maintain data integrity and compliance.; Core Banking Systems: Working with banking platforms such as ABS and LMS, and CRM systems to understand and support banking data and processes.; Real-time Data Monitoring: Experience with monitoring data in real-time to support risk analytics and operational decision-making.; Agile/Scrum Methodology: Familiarity with Agile and Scrum frameworks to manage task delivery and collaborate effectively within teams.; Mentoring and Collaboration: Mentoring junior analysts and collaborating with stakeholders across business units including Risk, Sales, Data Science, and Operations.; Task and Project Management Tools: Overseeing task delivery and maintaining data monitoring tools using platforms such as Jira."
vBplUZdVBH3ZtfsSAAAAAA==,[],,"['Pricing and Demand Elasticity Models', 'Statistical Modeling Techniques', 'A/B and Multivariate Testing', 'Behavioral, Transactional, and Marketplace Data', 'Scenario Modeling and Revenue Forecasting', 'SQL, Python, and R', 'Causal Inference Methods', 'Pricing Theory and Optimization', 'Experimentation Platforms and Automation', 'Looker, Amplitude, dbt']","Pricing and Demand Elasticity Models: Design, build, and maintain advanced pricing and demand elasticity models using statistical and machine learning methods to optimize price points, discount strategies, and product bundling.; Statistical Modeling Techniques: Apply statistical modeling techniques including regression, matching, and difference-in-differences for causal inference and rigorous analysis of pricing experiments.; A/B and Multivariate Testing: Design and analyze A/B and multivariate experiments to test pricing changes, discount strategies, and offer configurations with statistical rigor including significance testing and confidence intervals.; Behavioral, Transactional, and Marketplace Data: Leverage diverse data sources such as behavioral, transactional, and marketplace data to inform pricing optimization and revenue forecasting.; Scenario Modeling and Revenue Forecasting: Develop tools to support scenario modeling, revenue forecasting, and price testing simulations to guide pricing strategy decisions.; SQL, Python, and R: Utilize advanced proficiency in Python or R and SQL for data manipulation, analysis, and building pricing models.; Causal Inference Methods: Employ causal inference methods to analyze test results and understand the impact of pricing changes on customer behavior and revenue.; Pricing Theory and Optimization: Apply solid understanding of pricing theory, elasticity modeling, and optimization techniques to influence pricing strategies and product design.; Experimentation Platforms and Automation: Partner with experimentation platform owners to automate and scale pricing experiments across channels and customer segments.; Looker, Amplitude, dbt: Familiarity with BI and analytics tools such as Looker, Amplitude, and dbt to support data analysis and experimentation."
gz3I4Cz8D4Gz6yTtAAAAAA==,"['Generative AI', 'Large Language Models', 'Retrieval-Augmented Generation', 'Deep Learning Frameworks', 'AI Model Deployment and MLOps']","Generative AI: Experience with LLM/GenAI use cases and developing Retrieval-Augmented Generation solutions, tools, and services such as LangChain, LangGraph, and MCP.; Large Language Models: Involved in use cases and development related to LLMs, including fine-tuning and deployment within AI solutions.; Retrieval-Augmented Generation: Developing RAG solutions, tools, and services to enhance AI capabilities, including integration with LLMs.; Deep Learning Frameworks: Use of frameworks like PyTorch specifically for developing and deploying deep learning models including CNNs, RNNs, and GANs.; AI Model Deployment and MLOps: Deploying and optimizing AI/ML models using Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow, with a focus on AI model lifecycle management in production environments.","['Exploratory Data Analysis', 'Machine Learning', 'Deep Learning', 'Natural Language Processing', 'Time-Series Analysis', 'Computer Vision', 'Model Validation and Testing', 'Model Deployment and Optimization', 'Cloud Computing for AI/ML']","Exploratory Data Analysis: Used to understand client data sets and operational requirements to design long-term solutions and guide AI strategy and development.; Machine Learning: Applied in developing AI/ML solutions, including research and implementation of novel approaches, model tuning, performance validation, and deployment in production environments.; Deep Learning: Experience required in applying techniques such as CNNs, RNNs, and GANs across real-world projects, including model tuning and performance validation in production.; Natural Language Processing: Used as part of data analysis techniques alongside time-series analysis and computer vision in AI/ML algorithm development.; Time-Series Analysis: Applied as a data analysis method in AI/ML algorithm development.; Computer Vision: Used as a data analysis technique in AI/ML algorithm development.; Model Validation and Testing: Includes validating AI models and algorithms via code reviews, unit tests, and integration tests to ensure quality and performance.; Model Deployment and Optimization: Involves deploying and optimizing ML models using tools like Kubernetes, Docker, TensorRT/Trion, RAPIDs, Kubeflow, and MLflow, and leveraging cloud environments such as AWS, Azure, or GCP.; Cloud Computing for AI/ML: Experience in leveraging cloud platforms (AWS, Azure, GCP) to deploy AI/ML workloads."
6NV_1aLYhYL-zyNRAAAAAA==,"['Deep Learning', 'Natural Language Processing']","Deep Learning: Recognized as a modern AI technique relevant to the role, indicating experience with neural networks and advanced AI model architectures.; Natural Language Processing: Mentioned as an area of expertise involving AI methods, potentially including transformer-based models or other AI-native NLP techniques used in recommendation systems.","['Machine Learning', 'Predictive Analytics', 'Recommendation Systems', 'Natural Language Processing', 'Deep Learning', 'Graph Theory Applications', 'Data Collection and Mining', 'Statistical Modeling']","Machine Learning: Used to design, build, and deploy robust recommendation models that influence company decision-making and strategic direction.; Predictive Analytics: Applied to develop and optimize recommendation models to enhance their predictive accuracy and reliability.; Recommendation Systems: Designed, implemented, and maintained as core products to provide data-driven digital experiences for users.; Natural Language Processing: Experience in NLP is required as part of expertise in machine learning and statistical modeling, potentially applied to recommendation or data analysis tasks.; Deep Learning: Experience in deep learning is required as part of expertise in machine learning and statistical modeling, supporting advanced model development.; Graph Theory Applications: Experience with graph theory is considered relevant for modeling and analytical tasks within recommendation systems or related data science problems.; Data Collection and Mining: Proficiency in collecting and mining data from disparate sources to understand data generation processes and support model development.; Statistical Modeling: Expert knowledge required to build and refine models that support predictive analytics and recommendation systems."
Y2EnjmLLHhIxktFWAAAAAA==,[],,"['Power BI', 'DAX', 'Power Query', 'SQL', 'Data Modeling', 'Data Visualization']","Power BI: Used to design, develop, and optimize interactive dashboards and reports that visualize key business metrics and support business intelligence reporting.; DAX: Applied for advanced data analysis expressions within Power BI to create complex calculations and data models.; Power Query: Utilized for data transformation and preparation to ensure efficient data processing and integration into visualizations.; SQL: Employed to query and manage large datasets, ensuring data integrity, accuracy, and consistency for reporting and visualization purposes.; Data Modeling: Developed to support business intelligence reporting by structuring data effectively for analysis and visualization.; Data Visualization: Implemented best practices in visualization design to create clear, usable, and impactful dashboards aligned with business objectives."
uDm6xPwvq22iQBueAAAAAA==,"['Generative AI', 'Large Language Models (LLMs)', 'Retrieval-Augmented Generation', 'Prompt Engineering']","Generative AI: Experience in embedding generation from training materials, storage and retrieval from vector databases, setup and provisioning of managed large language model gateways, development of retrieval-augmented generation-based LLM agents, model selection, iterative prompt engineering, fine-tuning based on accuracy and user feedback, monitoring, and governance.; Large Language Models (LLMs): Involved in provisioning, managing, and developing LLM-based agents using retrieval-augmented generation techniques and iterative prompt engineering.; Retrieval-Augmented Generation: Developed as part of LLM agents to enhance information retrieval and generation capabilities in AI solutions.; Prompt Engineering: Applied iteratively to improve model accuracy and user feedback in fine-tuning generative AI models.","['Machine Learning', 'Statistical Models', 'Natural Language Processing', 'Feature Engineering', 'Data Pipelines', 'Big Data Platforms', 'Python', 'R', 'Spark (PySpark)', 'SQL and HQL', 'Optimization Models', 'Forecasting Models', 'Graph Machine Learning', 'Causal Inference and Causal Machine Learning', 'Experimentation and A/B Testing', 'Model Evaluation Metrics', 'Data Science Product Development', 'Cloud Platforms (GCP, Azure)', 'Vertex AI', 'Kubeflow', 'GPU and CUDA', 'Open Source Frameworks (scikit-learn, TensorFlow, PyTorch)']","Machine Learning: Used to develop advanced statistical and computational algorithms to solve business problems and optimize operations, including classification, regression, forecasting, unsupervised models, graph ML, causal inference, and experimentation.; Statistical Models: Developed and applied to extract insights and support business decision-making across retail and finance divisions.; Natural Language Processing: Applied as part of machine learning techniques to build data science solutions, including NLP models relevant to business initiatives.; Feature Engineering: Implied in building and training machine learning models and preparing large analytics datasets to support project goals.; Data Pipelines: Built in collaboration with engineers to productize machine learning solutions within a big data ecosystem.; Big Data Platforms: Experience with Hadoop ecosystem components such as Hive, MapReduce, HQL, and Scala to handle petabytes of data for analytics and model deployment.; Python: Used extensively for building, scaling, and deploying data science solutions, including experience with PySpark for big data processing.; R: Used as one of the primary programming languages for data science projects and analytics.; Spark (PySpark): Utilized for big data processing and building scalable data science solutions.; SQL and HQL: Used for data querying and manipulation within big data platforms to synthesize large analytics datasets.; Optimization Models: Applied to improve business practices such as inventory management, price optimization, and shrink and waste reduction.; Forecasting Models: Developed and applied to predict business trends and support decision-making in retail and finance domains.; Graph Machine Learning: Used to solve complex business problems involving relationships and networks within data.; Causal Inference and Causal Machine Learning: Applied to understand cause-effect relationships and improve decision-making accuracy.; Experimentation and A/B Testing: Used to validate models and measure the impact of machine learning products on business outcomes.; Model Evaluation Metrics: Identified and reviewed to ensure analytical requirements are met and to quantify business impact.; Data Science Product Development: Involves building, scaling, and deploying holistic data science products with a product mindset after successful prototyping.; Cloud Platforms (GCP, Azure): Used for deploying and scaling data science and machine learning solutions, including experience with Google Cloud Platform and Azure.; Vertex AI: Utilized as part of Google Cloud Platform services for model deployment and management.; Kubeflow: Used to build and manage machine learning pipelines and operationalize ML workflows.; GPU and CUDA: Experience with GPU acceleration and CUDA for computational efficiency in model training and deployment.; Open Source Frameworks (scikit-learn, TensorFlow, PyTorch): Used for building machine learning models and conducting assessments in Python, Spark, Scala, or R environments."
Ph1NAgFmz4qnW5v0AAAAAA==,[],,"['SQL', 'Data Modeling', 'Data Pipelines Orchestration', 'Business Intelligence (BI) Tools', 'Predictive and Diagnostic Modeling', 'Data Governance and Quality', 'Version Control and Continuous Integration']","SQL: Used extensively for querying and managing high volume, high velocity data sets to build self-service analytics datasets and dashboards in collaboration with business and product teams.; Data Modeling: Designing, implementing, and maintaining clean, reusable, and scalable data models using tools such as dbt or SQLMesh to support various teams including product, GTM, and finance.; Data Pipelines Orchestration: Managing data pipelines using orchestration tools like Dagster or Airbyte to ensure reliable data flow and integration across systems.; Business Intelligence (BI) Tools: Creating dashboards and reports using BI tools such as Superset, Looker, Mode, or Metabase to communicate trends and uncover optimization opportunities.; Predictive and Diagnostic Modeling: Building predictive and diagnostic models to support business planning and user segmentation, enabling data-driven decision making.; Data Governance and Quality: Maintaining data integrity and driving best practices in data governance and quality to ensure reliable and accurate data for analysis.; Version Control and Continuous Integration: Working with engineering teams using version control systems like git and command line tools, as well as continuous integration (CI) to manage code and data workflows."
Ws41jgUNn9uFXim5AAAAAA==,[],,"['Predictive Analytics', 'Prescriptive Analytics', 'Machine Learning', 'Statistical Analysis', 'Unstructured Data Analysis', 'SAS', 'R', 'Python', 'SPSS', 'Excel', 'PowerPoint', 'SQL Programming', 'DBMS (IBM DB2, Oracle, SQL Server, Sybase)', 'Neural Networks', 'Decision Trees', 'Classification Models', 'Time-Series Analysis']","Predictive Analytics: Used to identify insights and patterns from large, disparate data sets to solve complex challenges and find new opportunities.; Prescriptive Analytics: Applied to recommend actions based on data insights derived from integrated large data sets.; Machine Learning: Utilized techniques including building models and analyzing unstructured data to extract business insights and support decision-making.; Statistical Analysis: Conducted intermediate and advanced analyses such as linear regression, ANOVA, time-series analysis, classification models, neural networks, and decision trees to interpret data and support business objectives.; Unstructured Data Analysis: Analyzed data types like social media listening, digital footprints, and speech analytics to extract meaningful information.; SAS: Used as statistical software for analyzing large data sets to discover new business insights.; R: Employed as a statistical programming language for data analysis and modeling.; Python: Used for statistical analysis, data manipulation, and building machine learning models.; SPSS: Applied as statistical software for data analysis and insights generation.; Excel: Used for data manipulation, analysis, and presentation of findings.; PowerPoint: Utilized to prepare and present reports and findings to stakeholders in an understandable business language.; SQL Programming: Required for querying and managing data within databases such as IBM DB2, Oracle, SQL Server, or Sybase.; DBMS (IBM DB2, Oracle, SQL Server, Sybase): Experience with these database management systems is necessary for data storage, retrieval, and management.; Neural Networks: Applied as part of advanced statistical and machine learning techniques to analyze data and build predictive models.; Decision Trees: Used as a classification model within machine learning and statistical analysis to support data-driven decisions.; Classification Models: Implemented to categorize data points and support predictive analytics.; Time-Series Analysis: Conducted to analyze data points collected or recorded at specific time intervals for trend and pattern identification."
WforHCgGe6I1LGvvAAAAAA==,[],,"['SQL', 'Data Extraction and Integration', 'Data Conversion', 'Microsoft Power BI', 'Machine Learning Algorithms', 'Model Optimization', 'Model Explanation']",SQL: Used for writing complex queries to extract and manage data from databases as part of dataset management.; Data Extraction and Integration: Involves extracting data from multiple sources and joining it together to prepare a consolidated datapool for modeling and analysis.; Data Conversion: Ability to convert data from different sources into usable formats for analysis and modeling.; Microsoft Power BI: Used to create business intelligence dashboards and visualizations to support data analysis and reporting.; Machine Learning Algorithms: Applying machine learning techniques to develop predictive models based on business data.; Model Optimization: Improving existing models to make them scalable and easily configurable for different projects.; Model Explanation: Developing explanations to communicate model recommendations effectively to the team.
HHZH0mYYs6nJHLBrAAAAAA==,"['Neural Networks', 'Large Language Models (LLMs)', 'TensorFlow', 'PyTorch', 'SageMaker']","Neural Networks: Experience with neural networks as part of deep learning approaches to enhance AI/ML solutions in ecommerce and marketplace optimization.; Large Language Models (LLMs): Preferred experience with LLMs and AI, indicating involvement with modern AI technologies beyond traditional machine learning.; TensorFlow: Use TensorFlow as a deep learning framework to develop and deploy AI/ML models within Walmart’s Marketplace.; PyTorch: Utilize PyTorch for building and training deep learning models as part of AI/ML product development.; SageMaker: Leverage AWS SageMaker for managing the lifecycle of machine learning and AI models, including development and deployment.","['Machine Learning', 'Causal Learning', 'Anomaly Detection', 'Supervised and Unsupervised Learning', 'Deep Learning', 'Statistical Models', 'Data Science Frameworks', 'Big Data Analytics', 'SQL and Relational Databases', 'Python Programming', 'Optimization Models', 'Data Pipelines and APIs', 'Computational Algorithms']","Machine Learning: Lead the development and deployment of machine learning models across Walmart’s Marketplace with a focus on seller-fulfilled offerings; build and train machine learning algorithms ready for deployment; monitor and tune model performance at scale.; Causal Learning: Apply causal learning techniques specifically in the ecommerce domain to develop innovative products and solutions.; Anomaly Detection: Utilize anomaly detection methods in the ecommerce domain to identify unusual patterns and support business objectives.; Supervised and Unsupervised Learning: Experience in applying both supervised and unsupervised machine learning techniques to solve complex problems.; Deep Learning: Experience with deep learning methods, including neural networks, applied to large-scale data challenges and business problems.; Statistical Models: Develop advanced statistical models to drive data-derived insights across retail challenges and support project goals.; Data Science Frameworks: Leverage data science frameworks such as TensorFlow, PyTorch, and SageMaker to develop novel algorithms and solve complex business problems.; Big Data Analytics: Utilize big data analytics platforms and techniques, including Hive and Spark, to analyze large datasets, identify trends, and support insights.; SQL and Relational Databases: Use SQL and relational database technologies to gather, validate, and synthesize data into large analytics datasets.; Python Programming: Employ Python with strong knowledge of data structures for building data science and machine learning solutions.; Optimization Models: Apply optimization models to improve marketplace operations such as delivery promise, abuse detection, and return optimization.; Data Pipelines and APIs: Build and maintain APIs for machine learning or data-driven applications and develop end-to-end data science pipelines.; Computational Algorithms: Design and implement computational algorithms to address complex retail and ecommerce challenges."
wK-RaE65kRZyZKBrAAAAAA==,['Artificial Intelligence and Machine Learning'],"Artificial Intelligence and Machine Learning: Architecting and implementing AI/ML-based solutions to enhance simulation and optimization products within supply chain operations, supporting analytic innovation and operational efficiency.","['Operations Research', 'Stochastic Process Models', 'Optimization Models', 'Digital Twins', 'Statistical Simulation', 'AI/ML-based Solutions', 'Mathematical Programming Techniques', 'Solver Tools', 'Statistical Programming', 'Fundamental Statistical Knowledge', 'Data Extraction and Wrangling', 'Relational Databases', 'Data Warehousing and ETL', 'Cloud Computing Platforms', 'Data Mining', 'SQL', 'Excel and Financial Modeling']","Operations Research: The role involves applying operations research techniques such as enterprise network design, inventory, and transportation optimization to improve supply chain operations.; Stochastic Process Models: Developing stochastic process models to provide data-driven recommendations to business unit stakeholders in supply chain management.; Optimization Models: Creating optimization models to enhance efficiency and effectiveness in supply chain network design, inventory, and transportation cost management.; Digital Twins: Leading the development and enhancement of enterprise-scale digital twins for network management and control to simulate and optimize supply chain operations.; Statistical Simulation: Developing statistical simulation decision frameworks, including Monte Carlo simulation, to support decision-making in supply chain operations.; AI/ML-based Solutions: Implementing AI and machine learning solutions to support simulation and optimization workstreams within supply chain operations.; Mathematical Programming Techniques: Using advanced mathematical programming techniques such as column generation, decomposition, and local search methods to solve complex optimization problems.; Solver Tools: Experience with optimization solvers like Gurobi, Xpress, CPLEX, and open-source solvers such as CBC and GLPK to implement optimization models.; Statistical Programming: Utilizing statistical programming languages such as Python or R for data analysis, modeling, and simulation tasks.; Fundamental Statistical Knowledge: Applying knowledge of random variables, probability distributions, confidence intervals, and outlier detection to perform statistical modeling and derive business insights.; Data Extraction and Wrangling: Demonstrated ability to extract and wrangle data using SQL to support analytic needs and model development.; Relational Databases: Knowledge of relational database systems including MS SQL Server, Snowflake, and Oracle to manage and query data relevant to supply chain analytics.; Data Warehousing and ETL: Understanding of data warehousing and ETL best practices to support data integration and preparation for analytics (noted as a plus).; Cloud Computing Platforms: Familiarity with cloud platforms such as Azure, AWS, and Databricks to support scalable data processing and analytics (noted as a plus).; Data Mining: Experience with data mining using enterprise systems like SAP or JD Edwards to extract insights from operational data (noted as a plus).; SQL: Using SQL for data extraction and wrangling to support analytics and modeling efforts.; Excel and Financial Modeling: Proficiency with Excel spreadsheets and financial modeling to support reporting and analysis."
3NVf2GeHN199af05AAAAAA==,[],,"['Exploratory Data Analysis', 'Statistical Analysis', 'Machine Learning Algorithms', 'Data Cleaning and Preprocessing', 'Programming Languages for Data Science', 'Data Manipulation Libraries', 'Data Visualization Libraries', 'SQL and Database Concepts', 'Cloud Computing Platforms', 'Big Data Technologies', 'Version Control Systems', 'Model Deployment and Monitoring']","Exploratory Data Analysis: Used to identify patterns, trends, and insights in large and complex datasets as part of the data scientist's responsibilities.; Statistical Analysis: Applied to analyze data and build predictive models, requiring a strong foundation in statistical concepts and probability theory.; Machine Learning Algorithms: Includes supervised and unsupervised learning methods such as regression, classification, clustering, ensemble methods, and time-series models used to develop and validate predictive models.; Data Cleaning and Preprocessing: Involves collecting, cleaning, and preparing large datasets from various sources to ensure data quality for analysis and modeling.; Programming Languages for Data Science: Proficiency in Python or R is required for data manipulation, analysis, and model development.; Data Manipulation Libraries: Experience with libraries such as Pandas, NumPy, SciPy in Python, and dplyr, tidyr in R to handle and process data efficiently.; Data Visualization Libraries: Use of Matplotlib, Seaborn, Plotly in Python, and ggplot2 in R to communicate findings effectively through visualizations.; SQL and Database Concepts: Basic understanding required to query and manage data stored in databases.; Cloud Computing Platforms: Preferred experience with AWS, Azure, or GCP to support data storage, processing, and deployment of models.; Big Data Technologies: Familiarity with Spark and Hadoop is preferred for handling large-scale data processing.; Version Control Systems: Experience with Git is preferred for managing code and collaboration.; Model Deployment and Monitoring: Participation in deploying machine learning models into production and monitoring their performance."
j6OnCVQhiKTpop8TAAAAAA==,[],,"['Data-Driven Decision Making', 'Advanced Analytics', 'Predictive Models', 'Statistical Analysis', 'Data Visualisation', 'Analytics Roadmap Design', 'Quality Controls and Standards for Data Integrity', 'Programming with Python and Java', 'Systems Thinking']","Data-Driven Decision Making: Used to support and implement decisions based on data insights to drive business growth and project success.; Advanced Analytics: Leveraged to extract insights from large datasets and solve complex business problems through statistical analysis and predictive modeling.; Predictive Models: Developed to forecast outcomes and support data-driven decision making in complex business scenarios.; Statistical Analysis: Conducted to analyze data and validate outcomes, enabling informed decision-making and quality results.; Data Visualisation: Created to communicate insights clearly and effectively, supporting storytelling and holistic understanding of data.; Analytics Roadmap Design: Designed collaboratively with leaders to guide analytics strategy and support data-driven deliverables.; Quality Controls and Standards for Data Integrity: Developed and implemented to ensure data accuracy, integrity, and operational excellence in data solutions.; Programming with Python and Java: Used for developing data solutions, implementing analytical techniques, and supporting engineering designs.; Systems Thinking: Applied to identify underlying problems or opportunities within complex data and business environments."
2M3Q5hJ8jDS_BFwMAAAAAA==,['Large Language Models'],"Large Language Models: Implement sophisticated LLMs to make intelligent repair routing decisions, ensuring repairs are conducted efficiently and cost-effectively.","['Machine Vision', 'Large Language Models', 'Predictive Modeling', 'Quality Assurance Modeling', 'Model Development Lifecycle', 'Descriptive, Predictive, and Prescriptive Models', 'Cross Validation', 'Analytics and Visualization', 'SQL and Python', 'Statistical Analysis and Modeling']","Machine Vision: Utilize computer vision techniques to accurately detect and assess damage in real-time as part of damage management technology development.; Large Language Models: Implement sophisticated LLMs to make intelligent repair routing decisions, ensuring repairs are conducted efficiently and cost-effectively.; Predictive Modeling: Develop models to forecast future repair and maintenance needs based on historical data and trends.; Quality Assurance Modeling: Create models to evaluate the quality of repairs and ensure they meet high standards.; Model Development Lifecycle: Formulate strategic and tactical steps for end-to-end model development including problem statement definition, exploratory data analysis, feature engineering, model development, tuning, and implementation.; Descriptive, Predictive, and Prescriptive Models: Build and maintain models to measure the performance of new products and services.; Cross Validation: Apply cross validation techniques to production models to ensure high performance and generalizability.; Analytics and Visualization: Define and implement best practices to generate accurate analytics, reports, visualizations, and dashboards to communicate results effectively to technical and non-technical stakeholders.; SQL and Python: Use data querying languages like SQL and scripting languages such as Python for data manipulation and analysis.; Statistical Analysis and Modeling: Apply statistical analysis and modeling techniques to support machine learning and data science initiatives."
adtj_7QRa5DaMfKcAAAAAA==,"['Transformers', 'Large Language Models (LLMs)', 'Generative AI', 'Fine-tuning of LLMs', 'Agentic Systems']","Transformers: Familiarity with recent deep learning models based on transformer architectures used for advanced NLP tasks within conversational AI systems.; Large Language Models (LLMs): Experience working with large language models such as BERT, LLaMA, GPTs, and Gemini, including safe fine-tuning and serving optimizations for production conversational agents.; Generative AI: Involvement in building generative AI-powered conversational platforms and shopping assistants that leverage natural voice commands, text, and multi-modal interactions.; Fine-tuning of LLMs: Ability to safely fine-tune large language models to adapt them for specific conversational use cases and improve performance.; Agentic Systems: Exposure to real-world, production-grade autonomous agent systems that power conversational AI and multi-modal user experiences.","['Python', 'SQL', 'Classical Machine Learning Models', 'Statistical Measures', 'Natural Language Understanding (NLU) and Natural Language Processing (NLP)', 'Data Pattern Identification and Error Analysis', 'Data Pipelines and Model Lifecycle', 'Open Source Frameworks for Machine Learning', 'Optimization Models', 'Spark, Scala, R']","Python: Required proficiency in Python programming language for data science tasks and model development.; SQL: Solid knowledge of SQL for data querying and manipulation.; Classical Machine Learning Models: Hands-on experience with traditional machine learning models including training, testing, evaluation metrics, and tuning key parameters to optimize model performance.; Statistical Measures: Understanding of statistical concepts such as confidence intervals, significance of error measurements, and development and evaluation of datasets to assess model quality.; Natural Language Understanding (NLU) and Natural Language Processing (NLP): Familiarity with NLU/NLP techniques to improve conversational AI capabilities, including handling multi-modal interactions and contextual understanding.; Data Pattern Identification and Error Analysis: Experience in identifying data patterns, conducting error and deviation analysis, and optimizing data representation to enhance model accuracy and reliability.; Data Pipelines and Model Lifecycle: Hands-on expertise across the full model lifecycle including data extraction, data pipelines, model training, model serving, labeling tools, and ML-ops for production-grade deployment.; Open Source Frameworks for Machine Learning: Experience using open source frameworks such as scikit-learn, TensorFlow, and PyTorch for building and deploying machine learning models.; Optimization Models: Knowledge and application of optimization models to solve complex business problems and improve operational efficiency.; Spark, Scala, R: Experience or successful completion of assessments in big data and analytics tools such as Apache Spark, Scala, and R."
D4HPJ8yoWAUX3-1AAAAAAA==,[],,"['SQL', 'R', 'Python', 'Regression Models', 'Time Series Analysis', 'A/B Testing', 'Statistical Analysis', 'ETL (Extract, Transform, Load)', 'Funnel Optimization', 'User Segmentation', 'Cohort Analysis', 'Statistical Packages (Matlab, R, SAS)', 'Analytics & Visualization Tools (Chartio, Looker, Tableau)']","SQL: Used to build full-cycle analytics experiments, reports, and dashboards, and to perform queries for data analysis and decision-making.; R: Utilized as a statistical tool and scripting language for building analytics experiments, performing statistical analysis, and mentoring junior analysts.; Python: Employed for scripting, statistical analysis, building analytics experiments, and mentoring junior analysts in advanced methods.; Regression Models: Applied to analyze data patterns and support hypothesis testing and experimentation for business insights.; Time Series Analysis: Used to analyze trends over time to understand long-term user behaviors and marketplace dynamics.; A/B Testing: Implemented as a statistical technique to validate findings and optimize business metrics through experimentation.; Statistical Analysis: Includes hypothesis testing, experimentation, and regressions to produce validated recommendations and insights.; ETL (Extract, Transform, Load): Expertise required to manage data pipelines and prepare data for analysis and reporting.; Funnel Optimization: Experience in analyzing and improving user conversion funnels to drive growth and operational efficiency.; User Segmentation: Used to categorize users into meaningful groups for targeted analysis and strategy development.; Cohort Analysis: Applied to study user behavior and trends across defined groups over time to inform business decisions.; Statistical Packages (Matlab, R, SAS): Used for advanced statistical analysis and experimentation to support data-driven decision-making.; Analytics & Visualization Tools (Chartio, Looker, Tableau): Proficiency in these tools is required to create dashboards and reports that track essential business metrics and communicate insights."
J2E6bP-FHYgAVybHAAAAAA==,[],,"['Predictive Modeling', 'Machine Learning', 'Mathematics and Statistics', 'Logical Data Models', 'Data Interface Specifications', 'Online Query and Report Specifications', 'Structured Query Language (SQL)', 'Data Presentation and Visualization', 'Data Identification and Validation', 'Data and Metadata Management', 'Data Warehousing and Integration', 'Help Desk Tools (ServiceNow)', 'Database Performance and Loading Specifications', 'Legacy Data Conversion and Data Structure Design', 'Enterprise Resource Planning (ERP) Systems Knowledge', 'Data Pipelines']","Predictive Modeling: Uses predictive modeling techniques to discover meaningful patterns and knowledge in recorded data.; Machine Learning: Applies machine learning techniques to analyze data and extract insights.; Mathematics and Statistics: Utilizes mathematics and statistics to support data analysis and pattern discovery.; Logical Data Models: Develops logical data models as part of definition and application design to structure data effectively.; Data Interface Specifications: Creates data interface specifications to define how data is exchanged and integrated.; Online Query and Report Specifications: Designs on-line query and report specifications to facilitate data retrieval and presentation.; Structured Query Language (SQL): Uses SQL for querying databases and managing data.; Data Presentation and Visualization: Presents findings and data insights creatively to facilitate understanding across technical and non-technical audiences.; Data Identification and Validation: Identifies, validates, and exploits internal and external data sets from diverse processes.; Data and Metadata Management: Manages data and information in all forms, including analysis of information structure such as taxonomies, data, and metadata.; Data Warehousing and Integration: Experienced in analysis, design, and development related to data warehousing and integration capabilities.; Help Desk Tools (ServiceNow): Experience with help desk tools like ServiceNow to support IT service management.; Database Performance and Loading Specifications: Develops database performance loading specifications and data validation specifications to ensure efficient data processing.; Legacy Data Conversion and Data Structure Design: Designs legacy conversion specifications and data structures/load specifications for data warehousing.; Enterprise Resource Planning (ERP) Systems Knowledge: Knowledge of ERP, financial, and human resources systems methods and strategies for data warehousing.; Data Pipelines: Builds, manages, and operationalizes data pipelines, with advanced proficiency in Cybersecurity Workforce (CWF) context."
Se3xX3nIzCVGQVCxAAAAAA==,[],,"['SQL', 'Data Visualization Tools', 'Statistics', 'Data Pipelines', 'Quantitative Analytics']","SQL: Used extensively for data analysis, building scalable automation solutions, and creating core tables to support business decision-making.; Data Visualization Tools: Experience with tools like Superset and Tableau to create visualizations that translate analytic results for broad understanding across the business.; Statistics: Applied to solve business problems and perform strategic analysis and research to identify new business opportunities.; Data Pipelines: Ability to build simple pipelines to help scale data analysis efforts and automate processes.; Quantitative Analytics: Used to develop strategic problem-solving skills and derive insights from data to impact business decisions."
82c7Q47tRU65uhP7AAAAAA==,[],,"['SQL', 'Relational Databases', 'Entity-Relationship Diagrams', 'Reporting Automation', 'Data Analysis', 'Data Pipelines', 'Data Dictionary', 'Process Improvement', 'Data Visualization and Dashboards', 'Data Stewardship', 'Data Virtualization']","SQL: Used extensively for querying and managing relational databases as part of data analysis and reporting automation efforts.; Relational Databases: Involved in identifying data sources and establishing connectivity with data warehouses and data lakes to support reporting and analytics.; Entity-Relationship Diagrams: Created to model data sources and data elements, supporting detailed data analysis and system integration documentation.; Reporting Automation: Leading requirements gathering and development of automated reports and dashboards to enable data-driven decision making.; Data Analysis: Conducting detailed analysis including metrics calculations and identification of data sources to support portfolio reporting and business initiatives.; Data Pipelines: Supporting architecture and integration activities to connect various data sources with data warehouses and data lakes for scalable data delivery.; Data Dictionary: Developing documentation to define data elements and support data governance and user understanding.; Process Improvement: Designing and implementing internal process improvements such as automating manual processes and optimizing data delivery infrastructure for scalability.; Data Visualization and Dashboards: Supporting creation of UX wireframes and actual reports/dashboards, including quality checks to ensure accuracy for stakeholders.; Data Stewardship: Obtaining approvals for data access and ensuring proper governance in data source connectivity and usage.; Data Virtualization: Experience with data virtualization technologies like Denodo to facilitate data access and integration across multiple sources."
QRaerQP-yQ4HsNzyAAAAAA==,[],,"['Data Pipelines', 'ETL (Extract, Transform, Load)', 'Data Visualization', 'SQL', 'Python', 'Relational Databases', 'Machine Learning Models', 'Version Control', 'Agile Methodology']","Data Pipelines: Building complex data pipelines from different sources to map to data warehouse schema and performing ETL logic as required.; ETL (Extract, Transform, Load): Performing ETL logic as part of building complex data pipelines to process and prepare data for analysis.; Data Visualization: Analyzing data and creating visualizations using BI tools such as Power BI, Tableau, or Si-Sense to support business insights.; SQL: Engaging in SQL programming and writing on-the-fly queries to complete complex data analysis tasks involving relational databases.; Python: Using Python for data processing and analysis tasks as part of the data analyst role.; Relational Databases: Working with relational database systems and AWS infrastructure to manage and query data.; Machine Learning Models: Applying machine learning models as part of data analysis responsibilities.; Version Control: Using Git version control to manage code and collaborate within the team.; Agile Methodology: Following Agile or Scrum methodologies for project management and team collaboration."
DUfnR_OQQwQvdtj_AAAAAA==,[],,"['SQL', 'Python', 'Data Visualization Tools', 'Advanced Exploratory Data Analysis', 'Business Intelligence (BI)', 'Lean, Six Sigma, and Zero-Loss Principles', 'Data Integrity and Quality Management', 'Exploratory Analysis']","SQL: Used for querying and managing large, disparate datasets to measure business KPIs and conduct strategic analyses in transportation optimization.; Python: Utilized for advanced analytics, including exploratory data analysis and developing advanced logic to identify cost savings and process improvements.; Data Visualization Tools: Tools like Tableau and Power BI are employed to create visualizations that monitor and quantify the impact of changes and business challenges, aiding strategic decision making.; Advanced Exploratory Data Analysis: Applied to investigate key drivers of transportation dynamic optimization and data quality, supporting data-driven recommendations and solutions.; Business Intelligence (BI): Used to support operational stakeholders by analyzing data to influence strategic decisions and identify opportunities for cost savings and process improvements.; Lean, Six Sigma, and Zero-Loss Principles: Leveraged to analyze and improve processes, identify opportunities to impact cost and growth plans, and enhance data integrity and quality.; Data Integrity and Quality Management: Involves proactively identifying and addressing data issues to ensure reliable data for analysis and decision making.; Exploratory Analysis: Conducted to inform teams of findings and support collaboration and process optimization across cross-functional partners."
v14PJcF50YHncd3BAAAAAA==,[],,"['Big Data Analytics', 'Data Acquisition', 'Statistical Analysis', 'Predictive Modeling', 'Data Visualization and BI Tools', 'SQL', 'Python and R', 'Hadoop', 'SAS and SPSS', 'Geo-spatial Tools', 'Advanced Analytics', 'Project Management']","Big Data Analytics: Responsible for working on projects that gather and integrate large volumes of data to perform analysis and develop actionable insights and recommendations across the company.; Data Acquisition: Acquires data from multiple data sources to perform analysis and ensure compliance with company standards for data sharing and results.; Statistical Analysis: Interprets data and analyzes results using various statistical techniques and tools to identify trends or patterns in complex data and provide answers to business questions.; Predictive Modeling: Experience in advanced analytics and predictive modeling to support data-driven decision making.; Data Visualization and BI Tools: Uses tools such as Tableau and PowerBI to present data and analysis clearly and concisely, enabling stakeholders to quickly understand results and activate upon recommendations.; SQL: Utilizes SQL and SQL Server Management Studio for data extraction, collection, and organization from various systems.; Python and R: Employs Python and R programming languages for analytics and statistical analysis.; Hadoop: Uses Hadoop for handling and processing large-scale data sets as part of big data analytics projects.; SAS and SPSS: Utilizes SAS and SPSS statistical software for data analysis and interpretation.; Geo-spatial Tools: Applies geo-spatial tools to analyze location-based data as part of the analytics process.; Advanced Analytics: Applies advanced analytics techniques to extract, analyze, and interpret trends or patterns in complex data sets to inform business decisions.; Project Management: Demonstrates project management skills to coordinate analytics projects and collaborate with partners for holistic analysis."
KGwdgfxpOFbJ4d2aAAAAAA==,[],,"['SQL', 'Python', 'R', 'Business Intelligence Tools', 'Data Modeling', 'Statistical Analysis', 'Predictive Modeling', 'ETL Processes', 'Data Pipelines', 'KPI Development', 'Data Literacy Promotion', 'Data Analysis']","SQL: Used extensively to extract data from relational databases such as Postgres, MySQL, and T-SQL to support data analysis and reporting needs.; Python: Utilized for data manipulation, analysis, and visualization, including statistical analysis, machine learning, and data mining tasks.; R: Applied for advanced data manipulation, statistical analysis, and visualization to support complex data analysis and predictive modeling.; Business Intelligence Tools: Experience building dashboards and reports using tools like Tableau, PowerBI, Looker, and Metabase to create intuitive visualizations that communicate data insights effectively.; Data Modeling: Involved in designing and developing data models to support analysis, reporting, and predictive modeling efforts.; Statistical Analysis: Conduct complex statistical analyses to extract meaningful insights from large datasets and support strategic initiatives.; Predictive Modeling: Develop models to predict market trends, customer behavior, and data migrations, thereby improving organizational decision-making.; ETL Processes: Experience working on Extract, Transform, Load (ETL) processes to prepare and manage data for analysis and reporting.; Data Pipelines: Collaborate with data engineering teams to plan, build, and maintain data structures and pipelines that support analytics and reporting.; KPI Development: Consult with stakeholders to develop relevant Key Performance Indicators (KPIs) and translate complex data findings into actionable business insights.; Data Literacy Promotion: Act as a leader in promoting data literacy across the organization by facilitating understanding and effective use of data.; Data Analysis: Perform complex data analysis to support strategic business decisions and initiatives, leveraging advanced analytics tools and statistical methods."
OKOzvEm3BJJAS-RpAAAAAA==,[],,"['SQL', 'Data Visualization Tools', 'A/B Testing', 'Event-Based Analytics Tools', 'Subscription Metrics', 'Data Quality Assurance (Data QA)', 'User Behavior Analysis', 'Customer Segmentation', 'Python or R']","SQL: Used to write clean, efficient queries and handle large datasets for building dashboards and reports tracking key business metrics.; Data Visualization Tools: Tools like Tableau, Looker, and Power BI are used to create dashboards and reports that communicate insights and track business metrics such as subscription conversion, MAU, churn, LTV, and ARPU.; A/B Testing: Used to measure feature performance and understand campaign effectiveness by running controlled experiments.; Event-Based Analytics Tools: Tools such as Mixpanel and Amplitude are used to analyze user behavior and event tracking data to support insights and data quality assurance.; Subscription Metrics: Metrics like subscription conversion, monthly active users, customer churn, lifetime value, and average revenue per user are analyzed to influence product design, user retention, and business growth.; Data Quality Assurance (Data QA): Partnering with engineers to improve tracking and event quality to ensure reliable data for analysis.; User Behavior Analysis: Analyzing user interactions across devices and subscription plans to uncover patterns, identify opportunities, and highlight risks.; Customer Segmentation: Working with marketing and lifecycle teams to understand different customer groups and campaign performance.; Python or R: Used for deeper analysis or prototyping beyond standard SQL and visualization tools."
zR9q3DsgnW1GJNv7AAAAAA==,[],,"['Power BI', 'Python', 'NumPy', 'Pandas', 'ETL Processes', 'SQL', 'Data Visualization', 'Stakeholder Management', 'Business Intelligence']","Power BI: Used to design, develop, and maintain scalable and stable business intelligence solutions and dashboards that support evolving business needs and enable impactful storytelling.; Python: Utilized for creating efficient ETL processes and data manipulation, leveraging essential libraries such as NumPy and Pandas.; NumPy: A Python library employed for numerical computing tasks within data processing and ETL workflows.; Pandas: A Python library used for data manipulation and analysis, supporting the creation of automated data insights and ETL processes.; ETL Processes: Development and automation of data extraction, transformation, and loading workflows to streamline data processes and enhance business efficiency.; SQL: Applied for data extraction and manipulation to create innovative data solutions that support strategic decision-making.; Data Visualization: Mastery of techniques and tools to create dashboards and visual storytelling that communicate data insights effectively to stakeholders.; Stakeholder Management: Engaging and collaborating with business stakeholders to translate strategic needs into actionable data insights and ensure alignment on automation strategies.; Business Intelligence: Designing and maintaining data-driven solutions that provide actionable insights to drive informed decision-making and improve productivity."
2jQUGnt__tj0nxC2AAAAAA==,[],,"['SQL', 'Statistical Techniques', 'Data Visualization', 'Data Quality Management', 'Data Acquisition and Preparation', 'Predictive and Prescriptive Analytics', 'Collaboration with Data Engineers and Data Scientists', 'Business Intelligence Tools', 'Data Governance and Documentation', 'Data Storytelling and Communication']","SQL: Used as a subject matter expert tool for data acquisition, enrichment, and analysis to support business decision-making and data quality improvements.; Statistical Techniques: Applied to analyze data, improve data quality, and troubleshoot data incidents as part of data analysis assignments and projects.; Data Visualization: Performed visualization tasks to communicate data insights and support business decisions.; Data Quality Management: Involves improving data quality, troubleshooting data incidents, and resolving root causes to ensure reliable data for analysis.; Data Acquisition and Preparation: Includes discovering, acquiring, exploring, preparing, assessing, and maintaining datasets from various sources to support analyses and investigative requests.; Predictive and Prescriptive Analytics: Identified opportunities to incorporate these analytics methods into projects and products in collaboration with data scientists.; Collaboration with Data Engineers and Data Scientists: Involves drafting and testing data views with data engineers and identifying analytics and machine learning opportunities with data scientists to meet business needs.; Business Intelligence Tools: Proficiency with Power BI and Microsoft Office tools (Word, Excel, PowerPoint, SharePoint) to support data analysis, reporting, and communication.; Data Governance and Documentation: Reviewing and approving data views, design, and documentation to ensure governance standards and appropriate technical components are utilized.; Data Storytelling and Communication: Using analytical, written, and verbal communication skills to influence, coach leaders, and teach business analysts in interpreting and communicating data insights."
ny-9dTFQI_8p29_YAAAAAA==,[],,"['SQL', 'Tableau', 'Excel', 'Data Modeling', 'Data Analysis', 'Data Visualization', 'Visio', 'SharePoint', 'ERP Systems', 'Reporting']","SQL: Used to analyze complex data sets to identify trends, patterns, and insights relevant to business opportunities and growth.; Tableau: Employed to develop and maintain reports and dashboards that provide actionable insights to stakeholders.; Excel: Utilized for creating reports and dashboards to visualize data and support decision-making processes.; Data Modeling: Developed and maintained to support business analysis and reporting activities.; Data Analysis: Performed on complex data sets to extract insights and inform business strategies.; Data Visualization: Created using tools like Tableau, Excel, and Visio to present data insights and improve business processes.; Visio: Used to create process flow diagrams and visualizations aimed at improving business processes.; SharePoint: Managed and maintained to ensure data accuracy and accessibility across teams.; ERP Systems: Supported to ensure data accuracy and integrity within enterprise resource planning environments.; Reporting: Involves creating insightful reports and presenting findings to key stakeholders to drive business growth."
MY7BQj-ZMRRZ-_o6AAAAAA==,[],,"['Data Analysis', 'Data Cleansing and Preparation', 'Data Pipelines', 'Dashboards and Reporting', 'Data Visualization Tools', 'Advanced Excel', 'SQL and T-SQL', 'Python and PySpark', 'Google BigQuery', 'Business Intelligence (BI) Solutions', 'Data Governance and Data Quality', 'Data Modeling', 'Code Review and Automated Testing']","Data Analysis: Perform in-depth analysis of large datasets to identify trends, patterns, and anomalies relevant to business insights and decision-making.; Data Cleansing and Preparation: Clean and preprocess raw data to ensure accuracy and reliability, develop and implement data quality standards, and collaborate on integrating and automating data pipelines.; Data Pipelines: Work with a team to integrate and automate data pipelines to streamline data processing and ensure data availability for analysis.; Dashboards and Reporting: Create and maintain comprehensive dashboards and reports for key performance indicators to communicate data insights effectively.; Data Visualization Tools: Use tools such as Tableau and Power BI to present complex data in an understandable format and provide training on visualization best practices to junior analysts.; Advanced Excel: Utilize advanced Excel features including pivot tables, VLOOKUPs, Power Pivot, and functions to manipulate and analyze data.; SQL and T-SQL: Use T-SQL language and queries, including stored procedures and functions, for data manipulation and extraction from large data environments.; Python and PySpark: Apply Python and PySpark for data processing, analysis, and development of BI solutions.; Google BigQuery: Leverage Google BigQuery for handling and querying large-scale datasets efficiently.; Business Intelligence (BI) Solutions: Design, develop, and deploy BI solutions using Power BI with capabilities such as DAX and Row-Level Security (RLS).; Data Governance and Data Quality: Implement data governance practices, ensure data quality, and apply master data management principles to maintain data integrity.; Data Modeling: Understand and apply relational and dimensional data modeling techniques to structure data effectively for analysis and reporting.; Code Review and Automated Testing: Recognize the importance of code review and automated testing at various levels and write and implement tests to ensure code quality and reliability."
HGiZwhznwxKGCuoLAAAAAA==,[],,"['SQL', 'Data Visualization', 'Data Warehousing', 'ETL Processes', 'Python', 'KPI Design and Monitoring', 'Anomaly Detection', 'Data Analysis', 'Dashboard Development', 'Behavioral Analytics']","SQL: Used with high proficiency to query and analyze large-scale datasets from multiple sources to uncover actionable insights and drive business impact.; Data Visualization: Experience with BI tools such as Qlik Sense, Sisense, Tableau, and Looker to visualize large datasets and develop dashboards and automated reports for data accessibility and accuracy.; Data Warehousing: Working knowledge of data warehouse and data lake tools including Athena, Redshift, Snowflake, and BigQuery to manage and analyze large datasets.; ETL Processes: Experience building ETL pipelines to transform and prepare data for analysis and reporting.; Python: Knowledge of Python is considered an advantage for data analysis and interpretation.; KPI Design and Monitoring: Designing, monitoring, and maintaining key performance indicators across ad delivery, bidding, and monetization systems to guide performance optimization.; Anomaly Detection: Leading investigative analysis of anomalies or unexpected trends in campaign performance, traffic quality, or platform behavior.; Data Analysis: Analyzing large-scale datasets from multiple sources to uncover actionable insights, support strategic decisions, and drive business impact.; Dashboard Development: Developing and maintaining dashboards and automated reports to ensure data accessibility and accuracy for stakeholders.; Behavioral Analytics: Utilizing patented behavioral analytics technology to predict user app needs and support personalized, contextual recommendations."
SGkqMM25VxWMZYIlAAAAAA==,[],,"['Healthcare Data Analysis', 'Data Integration', 'Data Collection and Aggregation', 'Statistical Software (SAS)', 'Predictive Modeling', 'Data Reporting and Visualization', 'SQL', 'Data Quality and Validation', 'Healthcare Data Formats and Clinical Coding', 'Inferential and Predictive Statistical Analysis', 'Data Documentation', 'Data Mining Techniques', 'Business Intelligence Tools']","Healthcare Data Analysis: Analyzing patient claims, member enrollment, and other healthcare data to identify progress, performance, and opportunities for improvement on programs, quality of care, patient experience, and other metrics.; Data Integration: Working collaboratively with business partners, analysts, and IT to gather and integrate data from disparate sources to support analytic solutions.; Data Collection and Aggregation: Designing and developing data collection strategies, aggregation, analysis, and reporting to ensure data integrity and enhance information value.; Statistical Software (SAS): Using standard statistical software such as SAS for conducting data analysis on large datasets.; Predictive Modeling: Building, managing, and enhancing predictive models to support healthcare analytics and quality improvement activities.; Data Reporting and Visualization: Creating reports and visualizations using Tableau to present data and conclusions effectively to non-technical audiences.; SQL: Using SQL for querying and managing data as part of data analysis and reporting tasks.; Data Quality and Validation: Participating in the design and interpretation of data analyses, providing recommendations for improving data quality and reporting, and validating report results.; Healthcare Data Formats and Clinical Coding: Understanding health data formats including claims, lab, and pharmacy data, as well as clinical coding systems such as ICD9, ICD10, and CPT.; Inferential and Predictive Statistical Analysis: Applying inferential and predictive statistical techniques to analyze healthcare data and support decision-making.; Data Documentation: Creating and maintaining thorough and consistent documentation of programs used to create reports and analyses.; Data Mining Techniques: Utilizing data mining techniques and procedures to extract insights from large healthcare datasets.; Business Intelligence Tools: Using BI tools like Tableau to develop dashboards and reports that highlight key findings and support organizational analytic needs."
0xpyTuMSpUirqR2vAAAAAA==,[],,"['Excel Modeling', 'SQL', 'Forecasting Models', 'Data Visualization and BI Tools', 'Data Analysis', 'Automation with VBA']","Excel Modeling: Develop and maintain complex Excel models for forecasting, planning, and analysis; build user-friendly Excel models for business users; automate data processing and reporting tasks using advanced Excel functions and VBA; create dynamic dashboards and visualizations to support decision-making.; SQL: Write and optimize SQL queries to extract and manipulate data from various databases including Google Cloud Platform; integrate SQL data with Excel models and Tableau to ensure accurate and up-to-date reporting; assist in the maintenance and improvement of SQL databases to support business needs.; Forecasting Models: Develop models to support demand forecasting, supply planning including safety stock and buffer calculations, EOL/NPI planning, and inventory reporting; apply knowledge of statistical analysis and forecasting models to support business planning.; Data Visualization and BI Tools: Create dynamic dashboards and visualizations using Excel and Tableau to support decision-making processes and ensure accurate reporting.; Data Analysis: Consume and analyze large data sets to support business needs; assist in the creation of S&OP meeting materials through data analysis; ensure data accuracy and consistency across various reporting tools and platforms.; Automation with VBA: Automate data processing and reporting tasks using VBA within Excel to improve efficiency and accuracy."
H_0d6MgIKt-jSxCtAAAAAA==,[],,"['Data Pipelines', 'Data Analysis', 'Data Visualization and Reporting', 'Data Quality and Integrity', 'Data Modeling', 'Statistical and Analytical Techniques', 'Experimentation and A/B Testing', 'Business Intelligence (BI)', 'Data Privacy and Governance', 'Data Integration']","Data Pipelines: Recommends opportunities to build new data pipelines or integrations to better meet requirements and initiates collaborative action to source additional data.; Data Analysis: Applies expertise in data, business, and customer needs to evaluate and determine ideal analytical and statistical techniques to address business and/or research questions, guides and establishes partnerships to execute complex analyses, resolve analytical challenges, interpret results, and provide actionable recommendations.; Data Visualization and Reporting: Shares insights through dashboards, reports, data visualizations, and interactive self-service platforms to synthesize and simplify details across analyses and inform business decisions.; Data Quality and Integrity: Determines and leverages optimal methods and tools for integrating data and proactively works to identify and address data integrity, quality, and/or access issues.; Data Modeling: Develops and/or recommends initial or prototype data models and/or tools for others' consumption, evaluates the relationship between analytical models and business objectives, highlights gaps, and presents findings to senior stakeholders.; Statistical and Analytical Techniques: Critically evaluates the choice of tools, techniques, and assumptions to highlight potential gaps, ensures appropriate utilization within context, anticipates risks such as data leakage and methodological limitations, and provides feedback on analytical tools and models.; Experimentation and A/B Testing: Designs and executes formal experiments or prototypes to evaluate the impact of new features or processes, partners cross-functionally to advise on experimental design and evaluation frameworks, and makes data-driven recommendations for strategic business goals.; Business Intelligence (BI): Delivers accessible data insights and tools, builds trust by leveraging knowledge of Microsoft products and solutions, and promotes methods for efficient analytics and reporting including automation of ad-hoc analyses.; Data Privacy and Governance: Maintains expertise in data privacy and security requirements, ensures compliance with regulations, enforces standards related to data usage and handling, and guides others to uphold and apply updated data privacy and governance standards.; Data Integration: Identifies and leverages data across multiple sources, understands data requirements, evaluates sufficiency of data for business questions, and recommends integration methods to enhance data infrastructure and analyses."
XBuBBdrd6I1FWh3EAAAAAA==,[],,"['Pricing Tests', 'Advanced Analytical Techniques', 'Machine Learning']",Pricing Tests: Managing pricing tests across various markets to evaluate and optimize pricing strategies for revenue growth.; Advanced Analytical Techniques: Utilizing advanced analytical methods to develop and refine pricing strategies in revenue management.; Machine Learning: Applying machine learning methods to develop solutions that drive revenue growth through improved pricing and demand modeling.
dzZVz-TjVghNE55KAAAAAA==,[],,"['SQL', 'Business Intelligence Tools', 'Advanced Analytics Methodologies', 'Data Modeling', 'Multi-Cloud Platforms', 'Exploratory Data Analysis', 'Analytics Lifecycle Management', 'Revenue Cycle Management Analytics', 'Performance Metrics and Reporting', 'Version Control and CI/CD Pipelines']","SQL: Used extensively for writing complex queries and stored procedures to extract and manipulate data for analysis and reporting.; Business Intelligence Tools: Utilized tools such as Power BI to design and implement reporting solutions and dashboards that support decision making and performance monitoring.; Advanced Analytics Methodologies: Applied to model, analyze, and clean datasets to identify root causes of problems and recommend solutions that improve business performance.; Data Modeling: Collaborated with Data Engineers to create data models that facilitate analysis, reporting, and standardization of metrics.; Multi-Cloud Platforms: Experience with cloud environments like GCP and Azure to support data platform rationalization and performance improvements.; Exploratory Data Analysis: Performed to investigate data, confirm or reject hypotheses, and identify root causes of business problems.; Analytics Lifecycle Management: Oversaw the end-to-end process of identifying, analyzing, and presenting insights that drive business decisions and competitive advantage.; Revenue Cycle Management Analytics: Analyzed financial and operational data related to revenue cycle processes to support better decision making.; Performance Metrics and Reporting: Developed and maintained metrics and performance reports to provide insights into practice performance and operational effectiveness.; Version Control and CI/CD Pipelines: Used to manage code and automate deployment processes, ensuring reliable and repeatable analytics solutions."
Qu0mEm0SXfGskamgAAAAAA==,[],,"['SQL', 'Salesforce', 'Data Visualization Tools', 'Trend Analysis', 'Data Integrity and Cleaning', 'Financial Forecasting and Budgeting', 'Cost-Benefit and ROI Analysis', 'ERP Solutions and Integrations', 'Dashboard and Reporting']","SQL: Used for working with databases to analyze large datasets, load and change data from multiple areas, and support finance and accounting teams.; Salesforce: Utilized as a data platform requiring proficiency to manage and analyze financial data.; Data Visualization Tools: Experience with tools like Power BI, Tableau, and Looker is used to prepare data visualizations and create dashboards for reporting and insights.; Trend Analysis: Applied to analyze financial data trends, assist with forecasting, risk assessment, and financial projections.; Data Integrity and Cleaning: Responsibilities include creating, verifying, and maintaining data integrity, resolving data disparities, and ensuring data accuracy and reliability.; Financial Forecasting and Budgeting: Involves assisting with budgeting, forecasting, planning, and resource allocation based on analyzed data.; Cost-Benefit and ROI Analysis: Conducted to evaluate financial performance and support strategic initiatives through financial evaluations.; ERP Solutions and Integrations: Supporting the design and integration of ERP systems to maintain data integrity and consistency across platforms.; Dashboard and Reporting: Creating dashboards and reports to provide insights and recommendations for financial strategy and decision-making."
oZYCoKZJuovlcPk8AAAAAA==,[],,"['Data Modeling', 'Business Intelligence', 'Power BI', 'SQL', 'Microsoft Azure Data Services', 'CI/CD Pipelines and Version Control', 'Python', 'Machine Learning Techniques', 'Data Governance']","Data Modeling: Used for advanced data modeling to support data-driven initiatives and strategic decision-making across the organization.; Business Intelligence: Development and delivery of business intelligence solutions including building interactive dashboards to communicate insights and drive action.; Power BI: Advanced proficiency required, including use of DAX, Power Query, and Power BI Report Builder for data visualization and reporting.; SQL: Strong skills needed for database management, performance tuning, and optimization to support data analytics workflows.; Microsoft Azure Data Services: Hands-on experience with Azure Synapse, Azure Data Factory, and Databricks for data integration and processing.; CI/CD Pipelines and Version Control: Familiarity with CI/CD pipelines and version control systems such as Git to manage analytics workflow and ensure reproducibility.; Python: Used for automation and predictive analytics tasks within the data analytics function.; Machine Learning Techniques: Understanding of classification, regression, and clustering methods to support predictive analytics and data modeling.; Data Governance: Maintaining high standards of data quality, consistency, security, and compliance in reporting and analysis."
ZtzBT9B1LQq8HckrAAAAAA==,[],,"['Power BI', 'Data Collection and Analysis', 'Statistical Methods', 'Reporting and Dashboard Development', 'Data Integration and Management', 'Cost Estimation and Validation']","Power BI: Used to develop and refine dashboards incorporating analyses and visuals from multiple data sources (COLS, PBIS, DAI) for organizational use and improved staff review.; Data Collection and Analysis: Involves facilitating data collection efforts for labor costing, manpower costing, and performance management, analyzing cost estimates and performance data, and validating analytic results with subject matter experts to ensure consistency with empirical data.; Statistical Methods: Applied in analyzing cost driver data and DoD cost estimation techniques to support decision-making and ensure validation and consistency of cost estimates.; Reporting and Dashboard Development: Developing reports, splash pages, and dashboards to support staff review and leadership feedback loops, including the Commander’s Organizational Risk Estimate (CORE) report.; Data Integration and Management: Managing detailed requirements for data imports from other systems into the COLS Application and providing administrative support for maintaining COLS and Manpower applications.; Cost Estimation and Validation: Analyzing COLS OTL and labor out-year cost estimate data, comparing cost estimates against multiple data sources (PBIS, DAI, COLS) to validate alignment and improve understanding, adhering to MIL-STD-3022 guidelines for VV&A for decision support."
DHEqVg1KpOlHVDgLAAAAAA==,['Advanced Analytics with AI'],Advanced Analytics with AI: Implementing advanced analytic solutions involving AI to innovate and improve tools and technologies for enhanced data capabilities and insights.,"['Data Storytelling', 'Data Analysis', 'Data Visualization', 'Dashboard Development', 'Workflow Standardization and Automation']","Data Storytelling: Used to translate complex data into actionable insights and influence decision-making by preparing and delivering high-impact insights to stakeholders.; Data Analysis: Conducting thorough analysis to identify key trends, patterns, and insights relevant to product quality, safety, and sustainability.; Data Visualization: Presenting findings in a clear and visually engaging manner using tools like Tableau to support data-driven decision-making.; Dashboard Development: Designing and developing self-service dashboards in collaboration with the Product Quality Analytics team to enable stakeholders to access and interpret data independently.; Workflow Standardization and Automation: Driving continuous improvement by standardizing workflows, assisting in tool enhancements for automation, and standardizing documentation related to defective product analysis and sample disposition."
zKKin4SP6joHQ8cuAAAAAA==,[],,"['Data Analysis', 'Program Evaluation', 'Data-Driven Strategy Development', 'Data Quality Assurance and Compliance', 'Reporting and Dashboarding', 'Documentation and Workflow Management', 'Quantitative and Qualitative Data Analysis', 'Team Leadership and Mentorship', 'Stakeholder and Relationship Management']","Data Analysis: Perform and review complex senior-level data analysis and data research work to provide advanced analysis of existing data and satisfy ad-hoc reporting and analysis requests, serving as a first point of escalation for leadership.; Program Evaluation: Evaluate the effectiveness, impact, and use of clinical programs by analyzing program performance metrics, key performance indicators (KPIs), and outcome measures to assess program effectiveness and identify areas for improvement.; Data-Driven Strategy Development: Collaborate with senior management and stakeholders to understand business needs, define strategies, identify opportunities, and develop innovative data-driven solutions for strategic decision-making and business growth.; Data Quality Assurance and Compliance: Oversee regular reviews and audits to identify and address data integrity issues, define and implement data and reporting standards, best practices, and quality assurance processes to ensure accuracy, consistency, and reliability of analytical findings and deliverables, and implement necessary changes to improve data quality.; Reporting and Dashboarding: Deliver clear, concise, and actionable reports, presentations, and dashboards to communicate analytical findings, key insights, and recommendations to stakeholders at all organizational levels.; Documentation and Workflow Management: Ensure comprehensive documentation of data analysis methodologies, specifications, tools, processes, and outcomes, and oversee development of technical workflows, project plans, and timelines to ensure scalable data management processes and increase efficiencies across platforms, departments, teams, and institutions.; Quantitative and Qualitative Data Analysis: Analyze quantitative and qualitative data from satisfaction surveys, focus groups, and other sources to monitor quality, assess fidelity of program implementation, and drive continuous improvement.; Team Leadership and Mentorship: Provide leadership, guidance, and mentorship to a team of data analysts and data management technicians, assign tasks, set goals, monitor progress, and lead recruitment, onboarding, training, and professional development efforts.; Stakeholder and Relationship Management: Build and maintain strong relationships with internal and external stakeholders, act as a liaison between the data analytics team and other units, clinics, and programs, facilitate communication, resolve conflicts, and represent the organization in inter-institutional data governance committees."
WBpigE1EQGpRhE3aAAAAAA==,[],,"['Statistical Programming', 'Bayesian Estimation', 'Machine Learning', 'Natural Language Processing', 'Data Architecture', 'Data Processing Pipelines', 'Data Visualization', 'Reproducible Research', 'Version Control', 'R Programming', 'Stata']","Statistical Programming: Used to develop advanced models and perform complex statistical analyses to support research projects, including mentoring analysts and ensuring reproducibility of analyses.; Bayesian Estimation: Applied as an advanced statistical method preferred for supporting social science research projects and solving analytic challenges.; Machine Learning: Preferred experience involving the application of machine learning techniques to support research and data analysis.; Natural Language Processing: Experience preferred in applying NLP techniques, likely for analyzing educational or social science data.; Data Architecture: Developing and managing data architectures to support large datasets and scalable data processing pipelines.; Data Processing Pipelines: Designing and implementing pipelines to process and manage data efficiently for research purposes.; Data Visualization: Creating visualizations to communicate key findings and complex analyses clearly to non-technical audiences including policy and practitioner stakeholders.; Reproducible Research: Ensuring that research methods, analyses, and algorithms are documented and reproducible, maintaining high standards of research reliability.; Version Control: Using version control software such as Git to manage code, conduct code reviews, and support collaborative software development.; R Programming: Utilized for advanced statistical programming, developing R packages, and supporting data analysis workflows.; Stata: Used for advanced statistical programming and supporting social science research projects."
XrGXmWQT_mRCZOTmAAAAAA==,[],,"['SQL', 'Relational Databases', 'Python', 'R', 'Java', 'Data Visualization Tools', 'Statistical Analysis', 'Machine Learning', 'Data Pipelines', 'Exploratory Data Analysis', 'Cloud Platforms', 'Big Data Technologies', 'Predictive Models', 'Data Infrastructure Optimization']","SQL: Used for querying and managing relational databases such as MySQL and PostgreSQL to support data analysis and engineering tasks.; Relational Databases: Experience with databases like MySQL and PostgreSQL is required to store, retrieve, and manage structured data efficiently.; Python: Programming language used for data analysis, building data pipelines, and developing predictive models.; R: Programming language utilized for statistical analysis and data visualization.; Java: Programming language employed for data engineering and building scalable data solutions.; Data Visualization Tools: Tools such as Tableau and Power BI are used to create dashboards and visual representations of data insights for business intelligence.; Statistical Analysis: Applied to analyze complex datasets, identify trends and patterns, and support data-driven decision-making.; Machine Learning: Techniques used to build predictive models and algorithms that enhance data-driven decision-making processes.; Data Pipelines: Designing and developing ETL processes to extract, transform, and load data from various sources to support analytics and reporting.; Exploratory Data Analysis: Performed to uncover insights from data and inform strategic initiatives.; Cloud Platforms: Familiarity with AWS, Azure, and Google Cloud is required to leverage cloud infrastructure for data storage, processing, and analytics.; Big Data Technologies: Experience with Hadoop and Spark to handle large-scale data processing and analytics.; Predictive Models: Built to support data-driven decision-making by forecasting outcomes based on historical data.; Data Infrastructure Optimization: Maintaining and optimizing data systems to ensure scalability, reliability, and performance."
RFznnWszOvS3d7z7AAAAAA==,[],,"['SQL', 'Data Modeling', 'Data Pipelines Orchestration', 'Business Intelligence (BI) Tools', 'Data Governance and Quality', 'Version Control and CI']","SQL: Used for querying and managing data to build self-service analytics datasets and dashboards in collaboration with business and product teams.; Data Modeling: Involves building data models to represent complex, real-world systems and defining key metrics to guide product innovation and go-to-market strategies, using tools such as dbt or SQLMesh.; Data Pipelines Orchestration: Managing and automating data workflows using orchestration tools like Dagster or Airbyte to handle high volume, high velocity data sets.; Business Intelligence (BI) Tools: Utilizing BI platforms such as Superset, Looker, Mode, or Metabase to create dashboards and enable data-driven decision making across teams.; Data Governance and Quality: Maintaining data integrity and driving best practices to ensure reliable and accurate data for analysis and reporting.; Version Control and CI: Collaborating with engineering teams using version control systems like git and command line tools, as well as continuous integration practices to support data workflows and analytics development."
TJy5OnpWfXBgyiJWAAAAAA==,[],,"['Data Analysis', 'Data Visualization', 'Data Modeling', 'Optimization', 'Data Mining', 'Machine Learning', 'Business Intelligence', 'Ad-hoc Reporting', 'Data Strategy', 'Data Analytics', 'Training and Mentorship', 'Data Asset Management']","Data Analysis: Used to lead the company's data and information strategy to enhance organizational decision-making and data utilization across the company.; Data Visualization: Employed to facilitate and enhance organizational decision-making and data utilization across the company.; Data Modeling: Applied in developing optimization and business strategies, with a minimum of 3 years demonstrated experience required.; Optimization: Used in conjunction with data modeling to apply to business strategies, requiring demonstrated experience.; Data Mining: Requires a thorough understanding of the latest data mining techniques as part of the role's qualifications.; Machine Learning: Requires a thorough understanding of the latest machine learning techniques as part of the role's qualifications.; Business Intelligence: Provided enterprise-wide as services and recommendations to decentralized, embedded data scientists and analysts throughout the business.; Ad-hoc Reporting: Oversees and presents robust ad-hoc reports and deep dive analysis to provide clear observations for informed managerial decisions.; Data Strategy: Supports leadership in the development, assessment, refinement, and implementation of corporate strategy through effective use of data analytics.; Data Analytics: Used to cultivate data-driven insights supporting strategic and tactical business opportunities and to champion a data-driven decision-making culture.; Training and Mentorship: Provides training and mentorship to junior data analysts and supports senior data analysts with advanced analytics.; Data Asset Management: Responsible for documenting, classifying, and making data assets accessible and discoverable across the organization."
7sn8sBGbUiK7rVLwAAAAAA==,[],,"['Program Evaluation', 'Data Analysis', 'Data Visualization', 'Data Management', 'Statistical Methods', 'Reporting and Communication', 'Business Process Modeling', 'Algorithm Development', 'Survey Design and Management']","Program Evaluation: Conduct routine analysis on process and outcome data to assess program impact and support continuous improvement of programs and initiatives.; Data Analysis: Perform data analysis including big data analysis to generate insights, identify trends, and support decision-making for health-related programs.; Data Visualization: Develop meaningful data dashboards using tools such as Tableau or similar software to enable frontline staff and stakeholders to access, understand, and utilize data effectively.; Data Management: Maintain internal data systems ensuring high quality data processes, including data imports, exports, and management through tools like MS Excel and Google Suite.; Statistical Methods: Apply knowledge of statistics and research methodologies to analyze data, evaluate pilot programs, and support evidence-based practices.; Reporting and Communication: Prepare reports and materials that demonstrate program impact for internal and external stakeholders, communicating findings effectively to executive leadership and clients.; Business Process Modeling: Engage in business process modeling at enterprise to department level to support program evaluation and strategic initiatives.; Algorithm Development: Develop software and data models and algorithms to support data analysis and program evaluation.; Survey Design and Management: Create and manage online surveys to collect data relevant to program evaluation and research."
hqBDLRuo4rlY4DXHAAAAAA==,[],,"['Data Ingestion and Normalization', 'Data Cleansing and Preparation', 'Exploratory Data Analysis', 'Reporting and Data Reconciliation', 'Process Improvement and Automation', 'SQL and SSRS', 'Excel and VBA', 'R and Python', 'Business Intelligence and Visualization Tools']","Data Ingestion and Normalization: Responsible for ingesting, normalizing, and enhancing accounting, financial, and regulatory data from multiple sources to support reporting needs for organizational stakeholders.; Data Cleansing and Preparation: Supports cleansing, preparation, and enhancement of large and complex datasets for clients and internal teams to ensure data quality and usability.; Exploratory Data Analysis: Performs exploratory data analysis to understand data characteristics and identify insights relevant to reporting and decision-making.; Reporting and Data Reconciliation: Maintains and updates reports and records as needed, and reconciles reporting outputs to source documents to ensure accuracy and consistency.; Process Improvement and Automation: Identifies areas for process improvement or automation to enhance efficiency and accuracy in data handling and reporting workflows.; SQL and SSRS: Utilizes SQL and SQL Server Reporting Services (SSRS) for querying databases and generating reports to support data analysis and reporting requirements.; Excel and VBA: Uses Excel and VBA for data manipulation, analysis, and automation of repetitive tasks within reporting and data preparation processes.; R and Python: Employs R and Python programming languages for data analysis, statistical modeling, and scripting to support data science tasks.; Business Intelligence and Visualization Tools: Leverages business intelligence and visualization software to create dashboards and reports that communicate data insights effectively."
phqd2bATrSnCJjrHAAAAAA==,"['Artificial Intelligence', 'Natural Language Processing', 'Computer Vision', 'AIOps']","Artificial Intelligence: Assess AI tools and methods for data analysis to enhance business impact and decision-making, and contribute to technical task forces solving domain-specific problems using AI/ML techniques.; Natural Language Processing: Hands-on experience in NLP projects within the industry, contributing to data science and AI initiatives.; Computer Vision: Experience in computer vision projects as part of AI and machine learning applications in the industry.; AIOps: Expertise in AIOps frameworks for operationalizing AI models and automating IT and business processes.","['Machine Learning', 'MLOps', 'DevOps', 'Predictive Modeling', 'Data Collection and Preprocessing', 'SQL', 'Python', 'R', 'SAS', 'Scala', 'Cloud Platforms', 'Data Visualization', 'CI/CD Pipelines', 'Containerization']","Machine Learning: Develop and optimize machine learning models and pipelines, ensuring their efficient deployment, monitoring, and scaling to solve strategic and tactical analytic business problems and enhance operational efficiency.; MLOps: Drive ideas from conception to production using best-in-class Machine Learning Operations practices to support model deployment, monitoring, and automation.; DevOps: Utilize Development Operations practices alongside MLOps to drive ideas from conception to production and support continuous integration and deployment pipelines.; Predictive Modeling: Implement predictive modeling techniques to optimize production facilities, revenue streams, and operational efficiencies by exploring diverse data sources to improve business strategies.; Data Collection and Preprocessing: Experience in data collection, cleaning, preprocessing, and wrangling for industry-related problems based on domain knowledge to support analytics and modeling efforts.; SQL: Proficiency in SQL for data querying and manipulation as part of data analytics and engineering tasks.; Python: Use of Python as a primary platform for data science, machine learning, and analytics development.; R: Utilize R programming language for statistical analysis and data science tasks.; SAS: Experience with SAS for advanced analytics and statistical modeling.; Scala: Use of Scala programming language for data processing and analytics.; Cloud Platforms: Proficiency with cloud platforms such as Azure and Google Cloud (including Vertex AI) to deploy and manage data and machine learning solutions.; Data Visualization: Expertise in visualization tools and packages, including Power BI or similar tools, to create dashboards and support data-driven decision-making.; CI/CD Pipelines: Strong understanding and implementation of continuous integration and continuous deployment pipelines to automate and streamline model and software delivery.; Containerization: Experience with containerization technologies such as Docker and Kubernetes to support deployment and scaling of analytics and machine learning models."
aIf525Bn8ViIxXsSAAAAAA==,[],,"['Data Standardization', 'Data Normalization', 'Data Flows', 'Data Quality Management', 'Data Risk Management', 'Root Cause Analysis']","Data Standardization: Developing and applying standards to product data supplied by manufacturers to ensure consistency and quality across data attributes.; Data Normalization: Using techniques to transform and conform product data attributes to established standards to improve data quality and usability.; Data Flows: Understanding and managing how data moves through various business processes, particularly in a supply chain environment, to support data standardization and quality.; Data Quality Management: Assessing gaps in existing data, performing data analysis and manipulation to improve data content, and implementing controls to maintain high data quality standards.; Data Risk Management: Applying standards, methods, processes, tools, and controls to identify and mitigate risks associated with data quality and integrity.; Root Cause Analysis: Conducting investigations to identify underlying issues in data quality or processes and communicating recommendations to resolve these issues."
pMuqDWepe6FzERL2AAAAAA==,[],,"['Customer Feedback Analysis', 'SQL', 'Data Visualization', 'Text Analytics', 'Survey Tools']","Customer Feedback Analysis: Analyzing customer feedback data from multiple sources including surveys, support tickets, call transcripts, and customer satisfaction metrics to identify insights, opportunities, and pain points that inform strategic decision-making and improve customer experience.; SQL: Using SQL to extract and transform raw data from data warehouses to support analysis and reporting.; Data Visualization: Building dashboards and reports using Tableau, Google Slides, and PowerPoint to effectively communicate insights and influence organizational strategies with clear narratives and visual storytelling.; Text Analytics: Applying text analytics techniques to unstructured data such as call transcripts and support tickets to derive meaningful customer insights.; Survey Tools: Utilizing survey platforms like Qualtrics and Alchemer to collect and analyze customer feedback data."
yoPki0mc0ZKhQjdQAAAAAA==,[],,"['Data Handling and Management Plan (DHMP)', 'Data Collection and Recovery', 'Data Analysis', 'Sensor Data and Instrumentation', 'Data Uncertainty Management', 'Data Documentation and Storage Requirements']","Data Handling and Management Plan (DHMP): Used to review and coordinate data collection and management processes for flight tests, ensuring relevant data is captured and gaps or uncertainties are identified and addressed.; Data Collection and Recovery: Involves gathering hypersonic testing data from sensor packages and flight instrumentation, and ensuring data integrity and completeness for analysis.; Data Analysis: Applied to analyze hypersonic testing data, identify parameters affecting modeling and simulation (M&S) models, and support decision-making related to flight test instrumentation and data quality.; Sensor Data and Instrumentation: Knowledge and awareness of flight-testing sensors, both on-board and off-board, to understand data sources, measurement parameters, and uncertainties introduced by instrumentation.; Data Uncertainty Management: Accounting for uncertainties introduced by flight-testing instrumentation to improve the reliability and accuracy of data used in modeling and analysis.; Data Documentation and Storage Requirements: Documenting the data discovery process and defining storage needs to ensure proper data management and accessibility for hypersonic testing programs."
9ZfVslBUqr53kvS6AAAAAA==,[],,"['Data Visualization Tools', 'SQL', 'Data Analytics Workflow', 'Quantitative and Qualitative Analysis', 'Forecasting', 'Data Collection Management', 'Team Leadership in Data Analytics', 'Scalable Data Products Development']","Data Visualization Tools: The job requires practical experience with data visualization software such as Tableau and Looker to support data analysis and communication of insights.; SQL: Strong to advanced knowledge of SQL is required, including familiarity with statistical, aggregate, and windowing functions to query and analyze game and business data.; Data Analytics Workflow: The role involves establishing guidelines and documentation for the data analytics workflow to ensure alignment between technical and non-technical stakeholders and support product vision and goals.; Quantitative and Qualitative Analysis: The position requires identifying trends in game and business data through quantitative and qualitative analysis to advise development teams and senior leadership on product strategy and opportunities.; Forecasting: The job includes using forecasting techniques to analyze data trends and provide actionable insights for game development and business strategy.; Data Collection Management: Managing programs for data collection is a key responsibility, including gathering requirements and defining business needs for data collection and analysis within the game development field.; Team Leadership in Data Analytics: The role involves leading and mentoring a small team of data analysts, providing feedback, and ensuring smooth integration and growth of team members focused on game engagement, performance, and stability.; Scalable Data Products Development: Collaborating with data and gameplay programmers to design, develop, launch, and maintain scalable data products that meet high standards of performance, usability, and accuracy."
zQ9-rN43aeJNuHiIAAAAAA==,[],,"['Predictive Modeling', 'Statistical Analysis', 'Machine Learning', 'A/B Testing', 'Data Pipelines', 'SQL', 'Python', 'R', 'Java', 'REGEX', 'ETL Tools', 'Adobe Analytics SDK', 'Adobe Target', 'Adobe Tags', 'Digital Analytics Tools', 'Data Governance', 'Reporting and Dashboards', 'Personalization Strategy']","Predictive Modeling: Develop predictive models and segmentation strategies to support business goals and enhance customer experiences.; Statistical Analysis: Apply statistical analysis techniques as part of data science efforts to derive insights and support personalization and optimization strategies.; Machine Learning: Utilize machine learning methods to advance e-commerce strategy and personalization initiatives.; A/B Testing: Execute A/B testing to define audience segments, set conversion goals, and optimize targeted experiences.; Data Pipelines: Build and optimize data pipelines that support reporting, personalization, and marketing strategies.; SQL: Use SQL programming skills to manage and query data relevant to analytics and reporting.; Python: Apply Python programming skills for data science, analytics, and pipeline development tasks.; R: Use R programming skills for statistical analysis and data science activities.; Java: Employ Java programming skills as part of technical implementation and analytics work.; REGEX: Utilize REGEX for data parsing, tagging audits, and ensuring data quality.; ETL Tools: Use ETL tools like Alteryx and API data connections to extract, transform, and load data for analytics and personalization.; Adobe Analytics SDK: Work hands-on with Adobe Analytics SDK to implement and maintain digital analytics tools and infrastructure.; Adobe Target: Leverage Adobe Target Premium for personalization strategy, including audience segmentation and targeted experiences.; Adobe Tags: Manage Adobe Tags for tag management, tagging audits, and ensuring data accuracy and governance.; Digital Analytics Tools: Lead the implementation and maintenance of digital analytics tools and infrastructure to ensure scalable and reliable data systems.; Data Governance: Ensure data accuracy and governance through tag management, tagging audits, and data quality initiatives.; Reporting and Dashboards: Deliver ongoing and ad hoc reporting, translating complex data into actionable insights for stakeholders.; Personalization Strategy: Shape and evolve personalization strategy by defining audience segments, setting conversion goals, and executing targeted experiences."
cmKgYTV8VSvb052yAAAAAA==,[],,"['SQL', 'Python', 'AWS', 'Tableau', 'Data Quality', 'Data Management', 'Data Storytelling', 'Root Cause Analysis', 'Stakeholder Management']","SQL: Used as a technical skill for querying and managing data within the organization's data management processes.; Python: Utilized as a technical skill to develop innovative data solutions and perform data analysis tasks.; AWS: Applied as a cloud platform to support data storage, management, and analytics capabilities.; Tableau: Employed as a BI tool to create dashboards and support data storytelling and visualization for stakeholders and senior management.; Data Quality: Expertise required to ensure accuracy, consistency, and reliability of data used in analytics and decision-making processes.; Data Management: Involves managing critical data sets, improving data management processes, and enabling AI-ready data to support analytics and business intelligence solutions.; Data Storytelling: Skill used to effectively communicate data insights and recommendations to senior management and stakeholders.; Root Cause Analysis: Performed to identify and resolve complex data errors and improve data processes.; Stakeholder Management: Engaged to consult with internal and external partners, including external data vendors, to define data requirements and ensure project success."
yc4y5bmxG1zHCC8fAAAAAA==,[],,"['Data Analytics', 'Tableau', 'Power BI', 'Microsoft Excel', 'KPI Measurement and Reporting', 'Data Integration', 'Commercial Analytics', 'Cross-Functional Collaboration', 'Lean and Six Sigma']","Data Analytics: Used to deliver data driven insights and develop analytics resources supporting Medtech Surgery transformation, including KPI measurement and reporting for transformation initiatives.; Tableau: A technical tool required for creating metrics and reporting tools linked to transformation execution and commercial leadership insights.; Power BI: A technical tool required for building data visualizations and dashboards to support reporting and analytics across multiple organizational levels.; Microsoft Excel: Strong knowledge required, including advanced functions like vLookups and pivot tables, to manage and analyze data for reporting and insights.; KPI Measurement and Reporting: Responsible for defining, measuring, and reporting key performance indicators related to transformation activities at various organizational levels.; Data Integration: Ensures data analytics are integrated across multiple organizational levels (National, GPO, IDN, Area, Region, Territory, Account) to monitor and improve execution and accountability.; Commercial Analytics: Delivers analytics and insights related to commercial transformation activities, including financial indicators, conversion progress, and performance versus plan.; Cross-Functional Collaboration: Partners with Supply Chain, Marketing, FSO, KAM, and Customer Solutions teams to drive transformation and analytics initiatives.; Lean and Six Sigma: Process excellence methodologies appreciated for improving data-driven processes and project execution within the organization."
FLrA7FAa0FoXbqFaAAAAAA==,[],,"['Business Intelligence', 'Tableau', 'Power BI', 'Google Sheets', 'Microsoft Excel', 'Google Scripts', 'SQL Developer', 'Data Management', 'Data Analysis', 'Macros and Queries', 'Standard Operating Procedures (SOPs)']","Business Intelligence: The role involves developing, maintaining, and improving BI tools and dashboards to support data consumption and decision-making within the organization.; Tableau: Responsible for leading Tableau projects including developing new custom dashboards, maintaining existing tools, and promoting dashboard adoption across the organization.; Power BI: Experience with Power BI is preferred for developing custom dashboards as part of data visualization efforts.; Google Sheets: Used extensively for managing and manipulating large data sets and workbooks, including use of macros, imports, and query functions.; Microsoft Excel: Utilized for data manipulation and management, including use of macros and other advanced spreadsheet functions.; Google Scripts: Proficiency required for automating tasks and managing data workflows within Google Sheets and other Google Suite tools.; SQL Developer: Experience with SQL Developer is a plus, indicating involvement in querying and managing structured data.; Data Management: The role includes managing various data sets, developing long-term data management tools, processes, and solutions based on organizational needs.; Data Analysis: Responsibilities include analyzing financial health information, supporting data calls, risk management, audits, and project management related to data.; Macros and Queries: Used to manipulate and manage program-level information within spreadsheets and workbooks to support data analysis and reporting.; Standard Operating Procedures (SOPs): Involved in building and enhancing SOPs to standardize data processes and improve operational efficiency."
NUZLYaLrF9ZTpUygAAAAAA==,[],,"['Data Visualization', 'Power BI', 'SQL', 'Advanced Excel', 'Tableau', 'R', 'SAS', 'Python', 'Data Exploration and Trending', 'Data Cleansing and Transformations', 'Forecasting', 'DAX', 'Data Architecture and Engineering Structures', 'Business Intelligence (BI) Tools and Dashboards']","Data Visualization: Designs, implements, and develops data visualizations using tools such as Power BI, Custom Visuals, and R Visualizations to deliver engaging and informative data stories and support reporting and analytics solutions.; Power BI: Used to create dashboards, automated reports, report templates, and presentations, providing an easy-to-understand interface for end users to quickly identify key themes within data.; SQL: Utilized for querying and reporting, including working with databases such as Oracle, SQL Server, and AS400, to retrieve and manipulate data from large data sets.; Advanced Excel: Used for data aggregation, analysis, and reporting, supporting the handling of large data sets and complex business intelligence projects.; Tableau: Employed as a data visualization tool similar to Power BI to create reports and dashboards that communicate data insights effectively.; R: Used for scripting visualizations in Power BI and performing statistical analysis, data exploration, trending, and modeling on large data sets.; SAS: Applied for aggregating large data sets and conducting data analysis as part of business intelligence and reporting tasks.; Python: Utilized as a statistical tool for data analysis and scripting within the analytics and reporting processes.; Data Exploration and Trending: Performs research and analysis on large data sets to identify patterns, trends, and insights that inform business decisions.; Data Cleansing and Transformations: Involves preparing and transforming data to ensure accuracy and usability for analysis and reporting.; Forecasting: Understanding and applying forecasting techniques to support business trend analysis and decision-making.; DAX: Used alongside SQL and R for scripting and creating calculations within Power BI visualizations.; Data Architecture and Engineering Structures: Recommends and designs data structures necessary to support reports and dashboards, ensuring efficient data retrieval and analysis.; Business Intelligence (BI) Tools and Dashboards: Creates and manages dashboards and reports to provide actionable, data-driven insights that influence business decisions and communicate results to stakeholders."
_RSyd7-vmOinX-TgAAAAAA==,['Large Language Models'],Large Language Models: The job includes experience with large language models (LLMs) as part of implementing NLP concepts to enhance analytic solutions.,"['Machine Learning', 'Data Mining', 'Data Visualization', 'Predictive Modeling', 'SQL', 'Python and PySpark', 'Graph Technologies', 'Anomaly Detection, Clustering, and Time-Series Analysis', 'NLP Concepts (Preprocessing, TF-IDF, Named Entity Recognition)', 'Big Data Analytics', 'Data-Driven Solutions and Data Models']","Machine Learning: The role requires at least three years of direct experience in machine learning and involves creating and deploying predictive models using the latest mathematical and statistical methods.; Data Mining: The candidate must demonstrate knowledge of data mining methods to extract insights and build analytic solutions for clients.; Data Visualization: The job involves developing data visualizations to communicate analysis techniques, concepts, and products effectively to clients and stakeholders.; Predictive Modeling: The position includes creating and deploying predictive models from various data sources to solve client problems and integrate analytics into daily operations.; SQL: The candidate should be comfortable developing complex SQL queries to extract, transform, and load data as part of the analytic solutions.; Python and PySpark: Experience with Python and PySpark is desired for developing analytic solutions and handling large-scale data processing.; Graph Technologies: Experience with graph technologies and tools is preferred to support analysis and visualization of complex data relationships.; Anomaly Detection, Clustering, and Time-Series Analysis: The job requires experience with analytic techniques such as anomaly detection, clustering, and time-series modeling (e.g., ARIMA) to analyze data patterns and trends.; NLP Concepts (Preprocessing, TF-IDF, Named Entity Recognition): The role involves implementing natural language processing concepts including text preprocessing, TF-IDF, and named entity recognition to support analytic solutions.; Big Data Analytics: The candidate should have experience in big data analytics, including working with large JSON data stores and distributed database technologies like Databricks.; Data-Driven Solutions and Data Models: The position focuses on developing data-driven solutions and data models tailored to client needs to support decision-making and operational integration."
RJMAzVpuaAdM1qcVAAAAAA==,"['Natural Language Processing (NLP)', 'Deep Learning']",Natural Language Processing (NLP): Explicitly mentioned as a cutting-edge AI technology researched and applied to discover potential business opportunities and enhance quantitative research in financial contexts.; Deep Learning: Included as part of AI-related techniques researched and applied for advanced modeling and predictive analytics in the Treasury and financial engineering domain.,"['Python', 'SQL', 'Typescript/Javascript', 'Linux', 'Statistical and Machine Learning Techniques', 'Time Series Analysis', 'Natural Language Processing (NLP)', 'Deep Learning', 'REST APIs', 'BI Tools', 'Predictive Modeling', 'Data Visualization', 'Model Validation and Testing', 'Financial Mathematics', 'Alternative Data']","Python: Used for hands-on development and implementation of data analysis, statistical and machine learning techniques in the Treasury and financial engineering context.; SQL: Utilized for querying and managing traditional and alternative datasets to support quantitative research and predictive modeling.; Typescript/Javascript: Applied in development tasks related to building analysis and data visualization tools and creating information reports reflecting business requirements.; Linux: Used as the operating system environment for development and deployment of data science and financial engineering solutions.; Statistical and Machine Learning Techniques: Includes a wide range of methods such as time series analysis, NLP, and deep learning applied to quantitative research, predictive modeling, and solving complex business problems in finance.; Time Series Analysis: Employed for analyzing market-related data and financial time-dependent variables to support predictive modeling and strategy development.; Natural Language Processing (NLP): Used as part of statistical and machine learning techniques to analyze textual data relevant to financial markets and business insights.; Deep Learning: Applied within machine learning techniques to enhance predictive modeling and quantitative research in financial contexts.; REST APIs: Experience developing or working with REST APIs to integrate and access data or services necessary for financial modeling and analysis.; BI Tools: Experience with business intelligence tools such as PowerBI and Tableau to build data visualization tools and create reports that reflect business requirements.; Predictive Modeling: Conducted for market-related businesses by sourcing, integrating, and analyzing traditional and alternative datasets to discover new insights and strategies.; Data Visualization: Building tools and reports to visually represent data and analysis results to support business decision-making in Treasury and corporate-wide applications.; Model Validation and Testing: Involves creation, methodology, documentation, validation, remediation, signoff, maintenance, and production deployment of financial and predictive models.; Financial Mathematics: Knowledge of stochastic calculus, interest rate models, and derivative products used to support quantitative modeling and risk management in Treasury.; Alternative Data: Sourcing and integrating non-traditional datasets to enhance quantitative research and predictive modeling for market-related business insights."
0vCgYbJlilJjeSuIAAAAAA==,[],,"['SQL', 'Apache Hadoop', 'Hive', 'Presto', 'Python', 'Excel', 'Tableau', 'Power BI', 'Adobe Analytics', 'A/B Testing', 'Data Pipelines', 'Statistical Modeling', 'Machine Learning']","SQL: Used for querying and managing data to build scalable datasets and support data pipelines.; Apache Hadoop: Employed as a big data framework to handle large-scale data processing and storage.; Hive: Utilized for querying and managing large datasets stored in Hadoop, supporting data pipeline construction.; Presto: Used as a distributed SQL query engine to analyze large datasets efficiently.; Python: Applied for data analysis, scripting, and building reporting solutions.; Excel: Used for data manipulation, analysis, and visualization tasks.; Tableau: A data visualization tool used to create dashboards and visual representations of data insights.; Power BI: A business intelligence tool used to develop interactive reports and dashboards.; Adobe Analytics: Used to analyze clickstream data and build reporting solutions focused on customer behavior.; A/B Testing: Co-designed and analyzed experiments to measure impact and optimize business strategies.; Data Pipelines: Built robust data pipelines to integrate and process data from multiple sources for reporting and analysis.; Statistical Modeling: Understanding of statistical techniques to analyze data and derive insights.; Machine Learning: Knowledge of machine learning methods as a bonus for enhancing data analysis and predictive capabilities."
jEzrv_10Ou-QJBYaAAAAAA==,[],,"['Dimensional Modeling', 'SQL', 'Data Warehousing', 'ETL Processes', 'Entity-Relationship (ER) Modeling', 'Data Masking', 'Source-to-Target (S2T) Mappings', 'Data Element Dictionaries (DED)', 'Functional Requirement Specifications (FRS)', 'Data Modeling Tools', 'Performance Optimization', 'DB2', 'ETL Tools', 'Reporting and BI Tools', 'Data Governance', 'Metadata Management']","Dimensional Modeling: Used to design and model data solutions within the clinical data ecosystem, supporting data warehousing and analytics in healthcare payer systems.; SQL: Applied for writing efficient queries to optimize performance and support data analysis and process design in the clinical data environment.; Data Warehousing: Central to the role, involving design, implementation, and maintenance of data solutions and architecture within healthcare payer clinical data ecosystems.; ETL Processes: Understanding and interpreting ETL terminologies and workflows such as jobs, Psets, graphs, schedules, and job series to support data integration and transformation.; Entity-Relationship (ER) Modeling: Following data model standards and using ER modeling techniques with tools like Erwin to design and maintain data models.; Data Masking: Applied to ensure compliance with data privacy standards by preventing exposure of PHI/PII in healthcare data environments.; Source-to-Target (S2T) Mappings: Creating and maintaining mappings that define data flow from source systems to target data stores as part of data architecture deliverables.; Data Element Dictionaries (DED): Maintaining comprehensive dictionaries that document data elements to support data governance and clarity in the clinical data ecosystem.; Functional Requirement Specifications (FRS): Developing detailed specifications that outline functional requirements for data solutions and system design.; Data Modeling Tools: Using tools like Erwin to support data modeling, design, and documentation within the healthcare payer data environment.; Performance Optimization: Improving system and query performance through index creation, efficient SQL writing, and process design.; DB2: Experience with the DB2 database system as part of the data warehousing and management technology stack.; ETL Tools: Utilizing ETL tools such as Ab Initio to support data extraction, transformation, and loading processes.; Reporting and BI Tools: Using tools like Cognos and Tableau to create reports and dashboards for data analysis and visualization.; Data Governance: Ensuring data quality, compliance, and management practices within the healthcare payer clinical data ecosystem.; Metadata Management: Managing metadata to support data governance, lineage, and understanding of data assets."
W-poE1bKK2KGnW48AAAAAA==,[],,"['Regression models', 'Segmentation', 'Time-series analysis', 'Bayesian methods', 'SQL', 'Python', 'R', 'Tableau', 'Data modeling', 'Predictive analytics', 'Scenario analysis', 'Machine learning', 'Data visualization', 'Dashboard creation', 'Advanced statistical methods', 'Morningstar and Bloomberg data']","Regression models: Used as an advanced statistical method to analyze historical data and deliver insights that influence decision-making.; Segmentation: Applied as an advanced analytical method to surface trends and insights from structured and unstructured data.; Time-series analysis: Employed for scenario planning, forecasting, and analyzing historical data trends to support marketing or distribution strategies.; Bayesian methods: Utilized as part of advanced statistical techniques to validate analytical approaches and deliver insights from data.; SQL: Used as a data analysis tool to acquire, compile, and verify structured and unstructured data quality and accuracy.; Python: Applied for data analysis, building analytics products, and delivering insights through advanced statistical and machine learning methods.; R: Used for data analysis, statistical modeling, and creating predictive analytics and scenario analysis.; Tableau: Employed for data visualization, dashboard creation, and storytelling with data to translate analytic insights into actionable business solutions.; Data modeling: Involves building predictive analytics and scenario analysis models to support business strategy and decision-making.; Predictive analytics: Used to forecast outcomes and support scenario planning and ROI analysis for marketing or distribution strategies.; Scenario analysis: Applied to evaluate potential business outcomes and support strategic planning and forecasting efforts.; Machine learning: Used as an advanced method to deliver insights from structured and unstructured data, including validation of analytical techniques.; Data visualization: Involves preparing expert-level visualizations and presentations to communicate analytic insights effectively to business partners.; Dashboard creation: Developed and managed recurring analytic or reporting processes to provide ongoing insights and support decision-making.; Advanced statistical methods: Employed to analyze historical data, surface trends, and validate analytical techniques used by other analysts.; Morningstar and Bloomberg data: Used for competitive and market trend analysis to inform strategic initiatives and industry insights."
TD41iLrpCk_xL4BOAAAAAA==,[],,"['SQL', 'NoSQL', 'MongoDB', 'Psychometric Data Analysis', 'Statistical Analysis Techniques', 'Data Mining', 'SAS', 'R', 'Python', 'Assessment Quality Metrics', 'Data Visualization']","SQL: Used for extracting assessment data from structured databases such as Snowflake to support various analyses.; NoSQL: Used for extracting assessment data from unstructured databases such as MongoDB to support various analyses.; MongoDB: A NoSQL database specifically mentioned as a source for assessment data extraction and management.; Psychometric Data Analysis: Involves analyzing item and test content, quality, and performance data from multiple sources to support educational assessment products.; Statistical Analysis Techniques: Applied to support advanced psychometric work and to maintain up-to-date knowledge for accurate data interpretation and research.; Data Mining: Used to design and implement projects including scenarios and simulations for research and testing purposes.; SAS: A tool used to manipulate and analyze psychometric and assessment data.; R: A programming language used to manipulate and analyze psychometric and assessment data.; Python: A programming language used to manipulate and analyze psychometric and assessment data.; Assessment Quality Metrics: Defined and tracked to evaluate the quality and performance of educational assessments.; Data Visualization: Used to communicate results of analyses through visual displays to support understanding of psychometric data."
d2U_5v716K_r4I5AAAAAAA==,[],,"['Statistical Software (Stata, R)', 'Data Dashboards (Tableau, Power BI)', 'Data Collection Tools (Qualtrics)', 'Data Management', 'Program Assessment', 'Data Visualization', 'Data Analysis', 'Data Equity Principles']","Statistical Software (Stata, R): Used to validate, structure, clean, and analyze public sector data as part of data analysis tasks.; Data Dashboards (Tableau, Power BI): Developed and updated to create effective visual storytelling through interactive figures and communication materials for internal and external stakeholders.; Data Collection Tools (Qualtrics): Used to support primary data collection by developing and programming surveys and interview guides.; Data Management: Involves assessing and providing recommendations on best practices for organizing, structuring, storing, and accessing data to clients.; Program Assessment: Supports assessment of program performance by identifying key research questions and analyzing performance using quantitative and qualitative data.; Data Visualization: Creating visual storytelling through dashboards and interactive figures to communicate data insights effectively.; Data Analysis: Involves cleaning, validating, structuring, and analyzing data using statistical software to support decision-making and program evaluation.; Data Equity Principles: Ensuring data analysis and visualization follow best practices in design, structure, and format to promote equity in data work."
H0I1lCNDyz3fz5iFAAAAAA==,[],,"['Statistics', 'Python', 'Data Visualization Tools', 'Text Mining', 'Snowflake', 'Databricks', 'Java', 'Core Java', 'JavaScript', 'C++', 'Spring Boot', 'AWS Microservices', 'Docker', 'Jenkins', 'REST APIs', 'Computer Vision', 'TensorFlow']","Statistics: Knowledge of statistics is required for data science and machine learning positions to analyze and interpret data effectively.; Python: Python programming language is required for data science and machine learning roles, supporting data analysis, scripting, and development.; Data Visualization Tools: Experience with data visualization tools such as Tableau and PowerBI is preferred to create dashboards and visual reports.; Text Mining: Text mining skills are preferred to extract meaningful information from textual data in data science projects.; Snowflake: Snowflake is a preferred data warehousing tool for managing and querying large datasets.; Databricks: Databricks is preferred for big data processing and collaborative data science workflows.; Java: Experience in Java programming and understanding of the software development life cycle is required for data science and software development roles.; Core Java: Knowledge of Core Java is required for software programming and full stack development.; JavaScript: JavaScript knowledge is required for software programming and full stack development.; C++: C++ programming knowledge is required for software programming roles.; Spring Boot: Spring Boot experience is required for building Java-based microservices in full stack development.; AWS Microservices: Experience with AWS microservices is required for cloud-based software development.; Docker: Docker experience is required for containerization in software development.; Jenkins: Jenkins experience is required for continuous integration and deployment in software projects.; REST APIs: Experience with REST APIs is required for building and integrating web services.; Computer Vision: Knowledge of computer vision is required for data science and machine learning roles involving image data.; TensorFlow: TensorFlow is a preferred framework for machine learning projects."
jWLfCiK47FmY1HOVAAAAAA==,[],,"['SQL', 'Python', 'R', 'Data Visualization Tools', 'Data Platforms', 'Data Pipelines', 'Data Integrity', 'Machine Learning', 'Large Customer Databases']","SQL: Used to query and extract data from various data sources to support marketing data analysis and reporting.; Python: Utilized for data manipulation, analysis, and possibly building data pipelines in the marketing analytics context.; R: Employed for statistical analysis and data science tasks related to marketing data.; Data Visualization Tools: Power BI, Tableau, and Data Studio are used to create dashboards and reports that visualize marketing data insights.; Data Platforms: Google Analytics, Oracle, and Azure platforms are leveraged to collect, store, and analyze marketing and customer data.; Data Pipelines: Setting up automated data pipelines and API connections to integrate data from multiple vendor platforms and sources for analysis and reporting.; Data Integrity: Ensuring the accuracy and consistency of marketing data collected from various sources before analysis and reporting.; Machine Learning: Experience preferred for applying predictive modeling and analytical techniques to marketing data.; Large Customer Databases: Experience working with extensive customer data sets to derive marketing insights and support data-driven decision making."
kE8oPxNiUE-RX-IvAAAAAA==,[],,"['Data Analysis', 'Data Validation and Quality Assurance', 'SQL', 'Excel', 'Python', 'Statistics', 'Data Visualization', 'Reporting and Dashboarding']","Data Analysis: Analyze revenue trends and patterns to identify discrepancies or anomalies and provide actionable insights to optimize revenue streams and enhance operational efficiency.; Data Validation and Quality Assurance: Scrutinize and validate revenue data collected from third-party delivery platforms and implement quality assurance measures to guarantee the reliability of data sources.; SQL: Use SQL to query and manage data relevant to revenue and financial information.; Excel: Utilize intermediate-level Excel skills for data manipulation, analysis, and reporting.; Python: Apply Python programming for data analysis tasks.; Statistics: Employ statistical methods to analyze revenue data and support data-driven decision making.; Data Visualization: Create visual representations of revenue metrics and key performance indicators to communicate findings effectively.; Reporting and Dashboarding: Generate regular reports detailing revenue metrics and key performance indicators for stakeholders."
hJeVOiPwVMxk7jSZAAAAAA==,[],,"['Power BI', 'Data Pipelines', 'Data Analysis', 'Business Intelligence (BI) Tools', 'Data Quality and Auditing', 'Data Modeling', 'Data Visualization']","Power BI: Used to develop and maintain data analytics solutions, including dashboards and reports, to support business decision-making processes and visualize data models that meet business requirements.; Data Pipelines: Built and optimized to facilitate the flow and transformation of data for analytics and reporting purposes.; Data Analysis: Conducted to identify trends, patterns, and insights that inform business strategies and support decision-making.; Business Intelligence (BI) Tools: Utilized to create complex BI solutions, including dashboards and reports, and to continuously improve data analytics capabilities.; Data Quality and Auditing: Ensured through regular audits and troubleshooting to maintain accuracy and reliability of data used in analytics solutions.; Data Modeling: Designed to meet business needs by structuring data appropriately for analysis and visualization in BI tools.; Data Visualization: Created to communicate complex data and analytical findings clearly and concisely to non-technical stakeholders."
CUnGbokAezvJeakTAAAAAA==,[],,"['Cloud Data Architecture', 'Data Integration Patterns', 'Infrastructure as Code', 'Data Warehousing Concepts', 'Big Data Engineering', 'ETL Pipeline Development', 'File and Object Storage Solutions', 'SQL and Data Visualization', 'Programming with Python and Spark', 'Enterprise Data Concepts', 'CI/CD and Cloud DevOps', 'Insurance Data and KPIs']","Cloud Data Architecture: Implementing and migrating client data strategies to modern cloud data environments using cloud providers such as AWS, Azure, Google Cloud Platform, Snowflake, Databricks, and Teradata.; Data Integration Patterns: Applying cloud data integration patterns using tools like AWS Glue, Azure Data Factory, Event Hub, Databricks, and Snowflake to support data migration and processing.; Infrastructure as Code: Using tools such as CloudFormation and Terraform to manage cloud infrastructure deployments as part of data architecture implementation.; Data Warehousing Concepts: Applying knowledge of data warehousing including normalization, OLAP, OLTP, Vault data model, graph databases, star and snowflake schemas to design scalable data warehouse solutions.; Big Data Engineering: Utilizing big data technologies such as Hadoop, Spark, Scala, and Kafka for data processing and engineering, particularly in insurance data warehouse contexts.; ETL Pipeline Development: Developing ETL pipelines using tools like IICS, AWS Glue, Matillion, Abinitio, SSIS, and SnapLogic to support data ingestion and transformation.; File and Object Storage Solutions: Developing storage solutions using Azure ADLS 2.0 and AWS S3 for scalable data storage in cloud environments.; SQL and Data Visualization: Applying SQL for data querying and generating reports using visualization tools such as Tableau, Power BI, and Cognos to extract insights and support decision-making.; Programming with Python and Spark: Using Python and Spark programming for data processing, analytics, and pipeline development within cloud data environments.; Enterprise Data Concepts: Understanding and applying enterprise data management principles including Master Data Management, Data Governance, and Enterprise Data Warehouse design.; CI/CD and Cloud DevOps: Familiarity with continuous integration/continuous deployment practices and cloud DevOps tools including containerization technologies like Kubernetes and Docker to support data platform automation and scalability.; Insurance Data and KPIs: Applying domain knowledge of insurance data, key performance indicators, and their usage to tailor data solutions for P&C and L&A insurance sectors."
h3gffiseL3cHej7oAAAAAA==,[],,"['SQL', 'Python', 'Power BI', 'Tableau', 'Looker', 'Sigma Computing', 'Snowflake', 'DBT (Data Build Tool)', 'Data Governance', 'Data Pipelines', 'ERP and CRM Systems', 'Dashboards and Reporting', 'Ad Hoc Reporting and Analysis', 'Process Optimization', 'Business Intelligence (BI) Tools']","SQL: Used to write complex queries across large datasets to support revenue and accounting analytics and reporting.; Python: Utilized as a programming language to enhance data analytics capabilities, considered a strong plus for the role.; Power BI: Employed to design, build, and maintain user-friendly dashboards and reports that support revenue and accounting teams.; Tableau: Mentioned as an alternative dashboarding tool for creating visual analytics and reports.; Looker: Listed as another dashboarding tool option for developing business intelligence reports and visualizations.; Sigma Computing: Included as a dashboarding tool to support data visualization and reporting needs.; Snowflake: Used as an enterprise data warehouse platform to support cloud-based data storage and analytics.; DBT (Data Build Tool): Applied as a modern ELT tool to support data transformation and pipeline development in analytics workflows.; Data Governance: Implemented to uphold standards across key revenue data assets, ensuring data accuracy, consistency, and quality.; Data Pipelines: Defined and optimized in collaboration with Data Engineering to ensure seamless integration between source systems and efficient end-to-end data workflows.; ERP and CRM Systems: Understanding and integration of systems such as Microsoft Dynamics 365, RevStream, and Salesforce to support revenue data analytics and reporting.; Dashboards and Reporting: Developed and maintained to provide actionable, scalable insights aligned with strategic goals for revenue and accounting teams.; Ad Hoc Reporting and Analysis: Performed to deliver timely and accurate reports for internal audits, executive reviews, and specific business needs.; Process Optimization: Engaged in identifying bottlenecks and proposing enhancements to improve revenue-related processes and analytics maturity.; Business Intelligence (BI) Tools: Utilized to advance analytics platforms and support data-driven decision-making within the revenue and finance functions."
6ORuhMAMyhhWFGJ3AAAAAA==,[],,"['Key Performance Indicators (KPIs)', 'Data Collection Processes', 'Data Integrity', 'Trend Analysis', 'Reporting Templates', 'Quality Assurance Audits', 'Business Requirements Documentation', 'Data Visualization']","Key Performance Indicators (KPIs): Used to measure and monitor performance metrics critical to business success, requiring management and accuracy maintenance.; Data Collection Processes: Managing the gathering of data for recurring metrics and ensuring completeness and correctness.; Data Integrity: Ensuring accuracy, consistency, and quality of data through quality assurance checks and resolving discrepancies.; Trend Analysis: Analyzing data trends to identify insights that inform decision-making across organizational levels.; Reporting Templates: Creating and updating standardized templates to ensure consistent and accurate data reporting.; Quality Assurance Audits: Performing quality control audits and peer reviews to maintain data accuracy and reliability.; Business Requirements Documentation: Drafting and documenting business requirements to improve data gathering and reporting processes.; Data Visualization: Using visualization techniques to convey analytical findings effectively to stakeholders."
WpAHQdHE1-8aYJgAAAAAAA==,[],,"['Business Intelligence', 'Data-Driven Decision Making', 'Key Performance Indicators (KPIs)', 'Data Validation', 'Data Reporting and Dashboards', 'Medical Cost and Leading Indicator Data Analysis', 'Database Organization and Maintenance', 'Power BI', 'Tableau', 'Microsoft Excel (Pivot Tables, Look-Ups)']","Business Intelligence: Used to create data-driven insights and guide strategic execution for senior leadership in healthcare performance reliability.; Data-Driven Decision Making: Supports the design, development, and implementation of reporting and dashboards to enable informed decisions by finance, commercial, and business leaders.; Key Performance Indicators (KPIs): Assists in the design, evaluation, and tracking of organizational KPIs to monitor performance and support organizational growth.; Data Validation: Performs validation to ensure completeness and accuracy of queries and reports, reconciling discrepancies to maintain data integrity.; Data Reporting and Dashboards: Develops and maintains reports and dashboards with high utility for executive and senior leadership to communicate insights and performance metrics.; Medical Cost and Leading Indicator Data Analysis: Prepares and analyzes healthcare-related cost and indicator data to develop presentations and actionable recommendations for leadership.; Database Organization and Maintenance: Involves skills in database design, organization, maintenance, and troubleshooting to support data collection and storage.; Power BI: Required tool knowledge for creating dashboards and reports to support data-driven insights and decision making.; Tableau: Preferred tool knowledge for data visualization and reporting to support business intelligence efforts.; Microsoft Excel (Pivot Tables, Look-Ups): Used for data analysis, manipulation, and reporting within the Microsoft Office Suite."
M0w2cQAtRxfPuuXsAAAAAA==,[],,"['Data Analysis', 'Data-Driven Solutions', 'Data Visualization and Dashboards', 'Data Collection and Quality Assurance', 'Business Analytics Strategy']","Data Analysis: Analyze data to identify actionable insights that support clients' business objectives and improve decision-making processes.; Data-Driven Solutions: Develop and implement solutions based on data analysis to enhance operational efficiency and business processes.; Data Visualization and Dashboards: Create and present insights through dashboards, reports, and visualizations to communicate findings and drive action.; Data Collection and Quality Assurance: Evaluate and refine data collection methods and tools to ensure high-quality data inputs for analysis.; Business Analytics Strategy: Collaborate with cross-functional teams and clients to gather business requirements and develop tailored analytics strategies."
sfm1iZKL7DgOl0eoAAAAAA==,[],,"['Statistical Analysis', 'Business Intelligence Reporting', 'Data Visualization Tools', 'SQL and Databases', 'ETL Frameworks and Programming', 'Trend and Pattern Analysis', 'Data Collaboration and Process Improvement']","Statistical Analysis: Use of statistical packages such as Excel, R, SPSS, and SAS to analyze large datasets and identify trends or patterns relevant to business and product performance.; Business Intelligence Reporting: Designing and developing management reports and ad hoc analyses to answer key business questions and communicate product value and performance.; Data Visualization Tools: Creating client-facing visualizations using software like Data Studio, Tableau, and Power BI to clearly articulate value propositions and identify business opportunities.; SQL and Databases: Awareness and use of databases and SQL for data querying and supporting data-driven decision making.; ETL Frameworks and Programming: Familiarity with ETL frameworks and programming languages such as XML and JavaScript to support data transformation and enable repeatable data processes.; Trend and Pattern Analysis: Rigorous identification, analysis, and interpretation of trends or patterns in complex data sets to inform business insights and product improvements.; Data Collaboration and Process Improvement: Collaborating with data transformation, engineering, QA, and production support teams to develop preventive measures and lead product improvement initiatives."
0RPj7-j0a7dKpU6NAAAAAA==,"['AI-Driven Scientific Solutions', 'Scientific AI Use Cases', 'AI Model Evaluation', 'AI Adoption Enablement']","AI-Driven Scientific Solutions: Delivering AI and machine learning-driven solutions in operational or productized environments to improve efficiency, reduce costs, and enhance scientific data utilization.; Scientific AI Use Cases: Developing and implementing AI and machine learning use cases specifically tailored to scientific domains like drug discovery, preclinical development, and quality assurance.; AI Model Evaluation: Providing scientific input and feedback on AI model outputs to improve real-world performance and applicability in scientific contexts.; AI Adoption Enablement: Engaging with customers through onsite demonstrations and working sessions to facilitate adoption and trust in AI technologies within scientific workflows.","['Scientific Data Workflows', 'Exploratory Data Analysis', 'Data Enrichment', 'Data Transformation Workflows', 'Feature Engineering', 'Machine Learning Applications', 'Scientific Use Case Development', 'Workflow Documentation', 'Scientific Data Architecture', 'High Throughput Screening Data', 'Lab Automation Data', 'Python Programming', 'SDKs and Cloud Tools']","Scientific Data Workflows: Involved in managing and analyzing complex scientific datasets to enable AI and machine learning applications in life sciences.; Exploratory Data Analysis: Performing initial investigations on diverse customer datasets to identify enrichment and transformation opportunities that support scientific AI use cases.; Data Enrichment: Enhancing raw scientific data through integration and transformation to maximize its value for AI and machine learning applications.; Data Transformation Workflows: Defining and implementing workflows that convert and prepare scientific data to be AI-ready and support downstream AI/ML models.; Feature Engineering: Deriving meaningful features from scientific data to improve AI and machine learning model performance in drug discovery and quality domains.; Machine Learning Applications: Driving AI and machine learning use cases in scientific domains such as drug discovery, preclinical development, and quality control.; Scientific Use Case Development: Collaborating with customers to define, iterate, and implement innovative AI/ML-driven scientific use cases that improve operational efficiency and data utilization.; Workflow Documentation: Creating visual documentation such as workflow diagrams, entity-relationship diagrams (ERDs), and ontology definitions to support scientific data management and AI integration.; Scientific Data Architecture: Applying modern data architecture principles, including FAIR data principles, to organize and manage scientific data for AI readiness and interoperability.; High Throughput Screening Data: Experience with datasets generated from high throughput screening processes relevant to drug discovery and preclinical research.; Lab Automation Data: Handling data generated from automated laboratory processes to support scientific AI and machine learning workflows.; Python Programming: Using Python scripting and coding skills to manipulate scientific data, develop data workflows, and support AI/ML solution delivery.; SDKs and Cloud Tools: Utilizing software development kits (SDKs) and cloud platforms such as AWS to build and deploy scientific data and AI solutions."
X_ayPIoPy9Kzho7-AAAAAA==,[],,"['Data Analysis', 'Data Management', 'Ad Hoc Data Projects', 'Reporting', 'Statistical and Analytical Knowledge']","Data Analysis: Perform analysis on relevant information from various sources to prepare accurate documents, reports, summaries, and replies to inquiries.; Data Management: Manage the compilation, cataloging, caching, distribution, and retrieval of data to support organizational needs.; Ad Hoc Data Projects: Participate in ad hoc data projects and analyses that produce actionable recommendations and build relevant insights for internal and external stakeholders.; Reporting: Collect data and run basic reports in response to client inquiries to support decision-making and client requirements.; Statistical and Analytical Knowledge: Apply strong statistical and analytical knowledge to analyze data effectively and support problem-solving and critical thinking."
_rU6sFGYh2fmCHkCAAAAAA==,[],,"['Data Management', 'Data Analysis', 'Data Modeling', 'ETL (Extract, Transform, Load)', 'Data Visualization', 'SQL', 'Python', 'Big Data Technologies', 'Data Governance', 'Data Integration', 'Data Preparation', 'Data Roadmap and Strategy', 'Business Intelligence (BI) Tools', 'Agile Methodologies', 'Data Risk Management', 'Data Lifecycle Management', 'Data Profiling', 'Metadata Management', 'Process Flows and Data Mapping', 'Cloud Platforms', 'Programming Languages', 'Data Repositories and Warehouses']","Data Management: Supports and leads business data management initiatives including data governance, data quality, data risk management, data lineage, metadata management, and data lifecycle management to meet regulatory requirements and business needs.; Data Analysis: Performs data profiling, data analysis, and interpretation using various tools and methods to validate data quality, draw conclusions, and support business projects and initiatives.; Data Modeling: Develops and supports data models such as entity relationship diagrams and dimensional data models to structure business data for consumption and reporting.; ETL (Extract, Transform, Load): Designs, develops, and supports data movement, data wrangling, data mapping, and transformation processes to integrate and prepare data for business use.; Data Visualization: Utilizes visualization tools like Tableau and Power BI to develop dashboards and reporting solutions that support business insights and data understanding.; SQL: Reads and writes SQL queries to extract, manipulate, and analyze data from various data sources as part of data profiling and reporting activities.; Python: Uses Python programming language for data analysis, scripting, and supporting data preparation frameworks for data scientists.; Big Data Technologies: Has familiarity with big data technologies and works with very large datasets to support data management and analytics solutions.; Data Governance: Implements and enforces data governance policies, standards, and controls to ensure data quality, compliance, and risk mitigation.; Data Integration: Drives data-centric solution development focusing on complex data integration across multiple systems and platforms.; Data Preparation: Develops sophisticated data preparation frameworks and architectures to create or modify data features for consumption by data scientists.; Data Roadmap and Strategy: Develops data roadmaps and information management strategies aligned with enterprise data policies and standards.; Business Intelligence (BI) Tools: Supports business teams in the use and understanding of BI tools and reporting solutions to enable data-driven decision making.; Agile Methodologies: Familiar with Agile project delivery methodologies to support data and analytics projects.; Data Risk Management: Provides expert guidance on data risk identification, mitigation, and controls within data management frameworks.; Data Lifecycle Management: Ensures business data and information are retained and disposed of in compliance with enterprise data standards and policies.; Data Profiling: Performs data profiling using proprietary tooling and ad hoc query languages to validate and assess data quality.; Metadata Management: Manages metadata to support data governance, lineage, and reporting requirements.; Process Flows and Data Mapping: Understands and documents data flow from source to target systems, including process flows and data mapping for lineage and transformation.; Cloud Platforms: Experience with cloud platforms such as Databricks and Azure to support data management and analytics solutions.; Programming Languages: Fluent in one or two programming languages, including Python and SQL, to support data analysis and solution development.; Data Repositories and Warehouses: Analyzes, designs, and develops data repositories, warehouses, and marts to support business data storage and access."
c65hTLkZsP0-LLNjAAAAAA==,[],,"['Statistical Techniques', 'Data Engineering', 'Business Analytics', 'Python', 'R', 'SQL']","Statistical Techniques: Used to produce customized analyses and advanced models tailored to client needs involving large and complex datasets.; Data Engineering: Involves working with data pipelines and managing data infrastructure as part of the role's exposure and responsibilities.; Business Analytics: Applied to analyze organizational and business issues, supporting client-facing project-based analytics and research innovation.; Python: Preferred programming language for advanced analytics and data manipulation tasks within the role.; R: Preferred programming language for advanced analytics and data manipulation tasks within the role.; SQL: Preferred programming language for advanced analytics and data manipulation tasks within the role."
ZDb9DQ2dBOAn6CYzAAAAAA==,[],,"['SQL', 'dbt', 'Sigma Computing', 'Data Modeling', 'Data Analysis', 'Business Intelligence Dashboards', 'Experiment Design and Evaluation']","SQL: Used for complex data exploration and transformation, including writing advanced queries to analyze real-time shipment data, driver behavior, and port performance.; dbt: Used to design, build, and maintain modular, version-controlled data pipelines and robust data models that enable scalable and trusted analytics.; Sigma Computing: Used to build and manage dynamic dashboards and visualizations that communicate findings and monitor KPIs across the organization.; Data Modeling: Involves designing and maintaining robust data models using dbt to support scalable and trusted analytics.; Data Analysis: Includes analyzing logistics and operational data to evaluate performance, identify inefficiencies, and recommend improvements that inform product direction, operations, and business strategy.; Business Intelligence Dashboards: Creation and management of dashboards in Sigma Computing to visualize data insights and monitor key performance indicators.; Experiment Design and Evaluation: Partnering with product managers to design experiments, evaluate feature performance, and support roadmap planning using data-driven insights."
