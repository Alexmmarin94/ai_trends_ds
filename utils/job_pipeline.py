import os
import time
import pandas as pd
from pathlib import Path

from utils.job_fetcher import fetch_us_jobs, job_queries
from utils.job_classifier import classify_job
from utils.ai_extraction import extract_ai_mentions

# Paths
OUTPUT_PATH = Path("data/job_postings_cleaned/data_scientist_us.csv")
CLASSIFIED_PATH = Path("data/job_postings_cleaned/data_scientist_us_classified.csv")
AI_ENRICHED_PATH = Path("data/job_postings_cleaned/data_scientist_us_ai_enriched.csv")

EXPECTED_BASE_COLS = [
    "job_id",
    "job_title",
    "job_description",
    "job_posted_at_datetime_utc",
    "job_highlights.Qualifications",
    "job_highlights.Responsibilities",
    "extraction_date",
    "is_valid_job",
    "ai_mentions",
    "ai_details",
    "data_mentions",
    "data_details",
]

def _ensure_columns(df: pd.DataFrame, cols):
    """Ensure columns exist; create as None if missing."""
    for c in cols:
        if c not in df.columns:
            df[c] = None
    return df

def run_job_pipeline():
    print("üöÄ Starting job pipeline...")

    # --- Load or initialize base dataset
    if OUTPUT_PATH.exists():
        print("üìÇ Loading existing data from CSV...")
        df_old = pd.read_csv(OUTPUT_PATH, low_memory=False)
    else:
        print("üìÇ No existing file found. Creating empty dataset...")
        df_old = pd.DataFrame(columns=EXPECTED_BASE_COLS)

    df_old = _ensure_columns(df_old, EXPECTED_BASE_COLS)

    # --- Fetch
    print("üåê Fetching new jobs from API...")
    try:
        df_new_raw = fetch_us_jobs(job_queries)
        # Defensive: normalize expected columns and index
        if not df_new_raw.empty:
            df_new_raw = df_new_raw.reset_index(drop=True)
    except Exception as e:
        print(f"‚ùå Failed to fetch jobs from API: {e}")
        df_new_raw = pd.DataFrame()

    # --- Deduplicate vs historical by job_id and near-duplicate text
    if not df_new_raw.empty:
        # Strict de-dup by job_id against df_old
        already_ids = set(df_old["job_id"].dropna()) if not df_old.empty else set()
        mask_unseen = ~df_new_raw["job_id"].isin(already_ids)
        df_new = df_new_raw.loc[mask_unseen].copy().reset_index(drop=True)

        if not df_new.empty:
            # Build helper text
            df_new["job_title_description"] = (
                df_new["job_title"].fillna("") + " " + df_new["job_description"].fillna("")
            )

            # Reset index so cosine matrix positions match DataFrame positions
            df_new = df_new.reset_index(drop=True)

            # Near-duplicate dedupe by cosine similarity (TF-IDF)
            try:
                from sklearn.feature_extraction.text import TfidfVectorizer
                from sklearn.metrics.pairwise import cosine_similarity

                tfidf = TfidfVectorizer(stop_words="english")
                tfidf_matrix = tfidf.fit_transform(df_new["job_title_description"])
                cos_sim_matrix = cosine_similarity(tfidf_matrix)

                to_drop_pos = set()
                n = len(df_new)
                # Triangular loop to avoid double work; threshold 0.90 as in your version
                for i in range(n):
                    if i in to_drop_pos:
                        continue
                    sims = cos_sim_matrix[i, i + 1 :]
                    for offset, s in enumerate(sims, start=1):
                        j = i + offset
                        if s >= 0.90:
                            to_drop_pos.add(j)

                if to_drop_pos:
                    # Drop by positions safely
                    idx_to_drop = sorted(to_drop_pos)
                    df_new = df_new.drop(index=idx_to_drop, errors="ignore").reset_index(drop=True)

            except Exception as e:
                print(f"‚ö†Ô∏è Text-dedupe step skipped due to error: {e}")

            # Keep extraction_date coming from fetcher if present; otherwise set now
            if "extraction_date" not in df_new.columns or df_new["extraction_date"].isnull().all():
                df_new["extraction_date"] = pd.Timestamp.now().strftime("%Y-%m-%d")

            # Append to base (drop helper col)
            df_new = df_new.drop(columns=["job_title_description"], errors="ignore")
            before = len(df_old)
            df_old = pd.concat([df_old, df_new], ignore_index=True)
            added = len(df_old) - before
            print(f"‚úÖ Appended {added} new jobs to base.")
        else:
            print("‚ö†Ô∏è No unseen jobs to process after job_id de-duplication.")
    else:
        print("‚ö†Ô∏è No new jobs retrieved. Continuing with enrichment...")

    # --- Classification
    if CLASSIFIED_PATH.exists():
        df_classified = pd.read_csv(CLASSIFIED_PATH, low_memory=False)
    else:
        df_classified = pd.DataFrame(columns=["job_id", "is_valid_job"])

    # Choose jobs missing classification (not present in file or null in base)
    already_classified_ids = set(df_classified["job_id"]) if not df_classified.empty else set()
    need_cls_mask = (~df_old["job_id"].isin(already_classified_ids)) | (df_old["is_valid_job"].isnull())
    to_classify = df_old.loc[need_cls_mask].copy()

    if not to_classify.empty:
        print(f"üîç Classifying {len(to_classify)} jobs missing classification...")
        new_classified = []
        for _, row in to_classify.iterrows():
            try:
                is_valid = classify_job(row.to_dict())
                new_classified.append({"job_id": row["job_id"], "is_valid_job": is_valid})
            except Exception as e:
                print(f"‚ùå Error classifying job {row['job_id']}: {e}")

        if new_classified:
            df_classified = pd.concat(
                [df_classified, pd.DataFrame(new_classified)],
                ignore_index=True
            ).drop_duplicates("job_id", keep="last")
            df_classified.to_csv(CLASSIFIED_PATH, index=False)
            print("‚úÖ Classification completed and saved.")
        else:
            print("‚ö†Ô∏è Classification produced no rows (all failed or none needed).")
    else:
        print("‚úÖ No jobs missing classification.")

    # Merge classification back
    df_old = df_old.drop(columns=["is_valid_job"], errors="ignore").merge(
        df_classified, on="job_id", how="left"
    )

    # --- AI/Data enrichment
    if AI_ENRICHED_PATH.exists():
        df_ai = pd.read_csv(AI_ENRICHED_PATH, low_memory=False)
    else:
        df_ai = pd.DataFrame(columns=["job_id", "ai_mentions", "ai_details", "data_mentions", "data_details"])

    already_enriched_ids = set(df_ai["job_id"]) if not df_ai.empty else set()
    enrich_mask = (df_old["is_valid_job"] == True) & (~df_old["job_id"].isin(already_enriched_ids))
    to_enrich = df_old.loc[enrich_mask].copy()

    if not to_enrich.empty:
        print(f"üß† Extracting AI and Data mentions for {len(to_enrich)} jobs...")
        new_enriched = []
        for idx, row in to_enrich.iterrows():
            try:
                enriched = extract_ai_mentions(row.to_dict())
                new_enriched.append({
                    "job_id": row["job_id"],
                    "ai_mentions": enriched.get("ai_mentions"),
                    "ai_details": enriched.get("ai_details"),
                    "data_mentions": enriched.get("data_mentions"),
                    "data_details": enriched.get("data_details"),
                })
                if (idx + 1) % 10 == 0 or idx == len(to_enrich) - 1:
                    print(f"  ‚è≥ {idx + 1}/{len(to_enrich)} completed...")
            except Exception as e:
                print(f"‚ùå Error enriching job {row['job_id']}: {e}")

        if new_enriched:
            df_ai = pd.concat([df_ai, pd.DataFrame(new_enriched)], ignore_index=True)\
                     .drop_duplicates("job_id", keep="last")
            df_ai.to_csv(AI_ENRICHED_PATH, index=False)
            print("‚úÖ AI & Data enrichment completed and saved.")
        else:
            print("‚ö†Ô∏è Enrichment produced no rows (all failed or none needed).")
    else:
        print("‚úÖ No jobs missing AI enrichment.")

    # Merge enrichment back
    df_old = df_old.drop(
        columns=["ai_mentions", "ai_details", "data_mentions", "data_details"],
        errors="ignore"
    ).merge(df_ai, on="job_id", how="left")

    # --- Save final
    df_old.to_csv(OUTPUT_PATH, index=False)
    print(f"üì¶ Final dataset saved: {len(df_old)} jobs")
    return df_old
